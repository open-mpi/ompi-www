#
# Copyright (c) 2006-2009 Cisco Systems, Inc.  All rights reserved.
# Copyright (c) 2006-2007 Sun Microystems, Inc.  All rights reserved.
# Copyright (c) 2008      High Performance Computing Center Stuttgart,
#                         University of Stuttgart.  All rights reserved.
#

# Template MTT configuration file for Open MPI core testers.  Note
# that there are many items in this file that, while they are good for
# examples, may not work for random MTT users.  For example, the
# "ompi-tests" SVN repository that is used in many of the examples
# below is *not* a public repository (there's nothing really secret in
# this repository; it contains many publicly-available MPI tests and
# benchmarks, but we have never looked into the redistribution rights
# of these codes, so we keep the SVN repository "closed" to the
# general public and only use it internally in the Open MPI project).

#-------------------------------------------------------------------------

# The intent for this template file is to establish at least some
# loose guidelines for what Open MPI core testers should be running on
# a nightly basis.  This file is not intended to be an exhaustive
# sample of all possible fields and values that MTT offers.  Each site
# will undoubtedly have to edit this template for their local needs
# (e.g., pick compilers to use, etc.), but this file provides a
# baseline set of configurations that we intend you to run.

# OMPI core members will need to edit some values in this file based
# on your local testing environment.  Look for comments with "OMPI
# Core:" for instructions on what to change.

# Note that this file is artificially longer than it really needs to
# be -- a bunch of values are explicitly set here that are exactly
# equivalent to their defaults.  This is mainly because there is no
# reliable form of documentation for this ini file yet, so the values
# here comprise a good set of what options are settable (although it
# is not a comprehensive set).

# Also keep in mind that at the time of this writing, MTT is still
# under active development and therefore the baselines established in
# this file may change on a relatively frequent basis.

# The guidelines are as follows:
#
# 1. Download and test nightly snapshot tarballs of at least one of
#    the following:
#    - the trunk (highest preference)
#    - release branches (highest preference is the most recent release
#      branch; lowest preference is the oldest release branch)
# 2. Run all 4 correctness test suites from the ompi-tests SVN
#    - trivial, as many processes as possible
#    - intel tests with all_tests_no_perf, up to 64 processes
#    - IBM, as many processes as possible
#    - IMB, as many processes as possible
# 3. Run with as many different components as possible
#    - PMLs (ob1, dr)
#    - BTLs (iterate through sm, tcp, whatever high speed network(s) you
#      have, etc. -- as relevant)

#======================================================================
# Overall configuration
#======================================================================

[MTT]

# OMPI Core: if you are not running in a scheduled environment and you
# have a fixed hostfile for what nodes you'll be running on, fill in
# the absolute pathname to it here.  If you do not have a hostfile,
# leave it empty.  Example:
#     hostfile = /home/me/mtt-runs/mtt-hostfile
# This file will be parsed and will automatically set a valid value
# for &env_max_np() (it'll count the number of lines in the hostfile,
# adding slots/cpu counts if it finds them).  The "hostfile" value is
# ignored if you are running in a recognized scheduled environment.
hostfile =

# OMPI Core: if you would rather list the hosts individually on the
# mpirun command line, list hosts here delimited by whitespace (if you
# have a hostfile listed above, this value will be ignored!).  Hosts
# can optionally be suffixed with ":num", where "num" is an integer
# indicating how many processes may be started on that machine (if not
# specified, ":1" is assumed).  The sum of all of these values is used
# for &env_max_np() at run time.  Example (4 uniprocessors):
#    hostlist = node1 node2 node3 node4
# Another example (4 2-way SMPs):
#    hostlist = node1:2 node2:2 node3:2 node4:2
# The "hostlist" value is ignored if you are running in a scheduled
# environment or if you have specified a hostfile.
hostlist =

# OMPI Core: if you are running in a scheduled environment and want to
# override the scheduler and set the maximum number of processes
# returned by &env_max_procs(), you can fill in an integer here.
max_np = 

# OMPI Core: Output display preference; the default width at which MTT
# output will wrap.
textwrap = 76

# OMPI Core: After the timeout for a command has passed, wait this
# many additional seconds to drain all output, and then kill it with
# extreme prejiduce.
drain_timeout = 5

# OMPI Core: Whether this invocation of the client is a test of the
# client setup itself.  Specifically, this value should be set to true
# (1) if you are testing your MTT client and/or INI file and do not
# want the results included in normal reporting in the MTT central
# results database.  Results submitted in "trial" mode are not
# viewable (by default) on the central database, and are automatically
# deleted from the database after a short time period (e.g., a week).
# Setting this value to 1 is exactly equivalent to passing "--trial"
# on the MTT client command line.  However, any value specified here
# in this INI file will override the "--trial" setting on the command
# line (i.e., if you set "trial = 0" here in the INI file, that will
# override and cancel the effect of "--trial" on the command line).
trial = 0 

# OMPI Core: Set the scratch parameter here (if you do not want it to
# be automatically set to your current working directory). Setting
# this parameter accomplishes the same thing that the --scratch option
# does.
scratch = &getenv("HOME")/mtt/mtt-scratch

# OMPI Core: Set local_username here if you would prefer to not have
# your local user ID in the MTT database
# local_username =

# OMPI Core: --force can be set here, instead of at the command line.
# Useful for a developer workspace in which it makes no sense to not
# use --force
# force = 1

# OMPI Core: Specify a list of sentinel files that MTT will regularly
# check for.  If these files exist, MTT will exit more-or-less
# immediately (i.e., after the current test completes) and report all
# of its results.  This is a graceful mechanism to make MTT stop right
# where it is but not lose any results.
# terminate_files = &getenv("HOME")/mtt-stop,&scratch_root()/mtt-stop

# OMPI Core: Specify a default description string that is used in the
# absence of description strings in the MPI install, Test build, and
# Test run sections.  The intent of this field is to record variable
# data that is outside the scope, but has effect on the software under
# test (e.g., firmware version of a NIC).  If no description string is
# specified here and no description strings are specified below, the
# description data field is left empty when reported.  
# description = NIC firmware: &system("get_nic_firmware_rev")

# OMPI Core: Specify a logfile where you want all MTT output to be
# sent in addition to stdout / stderr.
# logfile = /tmp/my-logfile.txt

# OMPI Core: If you have additional .pm files for your own funclets,
# you can have a comma-delimited list of them here.  Note that each
# .pm file *must* be a package within the MTT::Values::Functions
# namespace.  For example, having a Cisco.pm file must include the
# line:
#
#     package MTT::Values::Functions::Cisco;
#
# If this file contains a perl function named foo, you can invoke this
# functlet as &Cisco::foo().  Note that funclet files are loaded
# almost immediately, so you can use them even for other field values
# in the MTT section.
# funclet_files = /path/to/file1.pm, /path/to/file2.pm

# OMPI Core: To ensure that MTT doesn't fill up your disk, you can
# tell MTT to stop when disk space gets too low.  You can specify a
# raw number of bytes or a percentage of overall disk space.  For
# example (default value is "5%"):
#
# min_disk_free = 5% # stop when there's less than 5% disk free
# min_disk_free = 500000 # stop when there's less than 500,000 bytes free

# OMPI Core: When MTT detects a low-disk situation, it can wait a
# little while before reporting whatever results it has accumulated
# and exiting.  The min_disk_free_wait field specifies a number of
# minutes to wait for there to be enough disk space to be free.  If
# there is still not enough disk space at the end of that time, MTT
# will report accumulated results and quit.
#
# min_disk_free_wait = 60

#
# Submit results on per section basis as alterative to "submit_results_after_n_results"
#
submit_group_results = 1

# code to run on mtt start
# on_start=&shell("modprobe ummunot")

# code to run on mtt stop
# on_stop=&shell("modprobe -r ummunot")

#======================================================================
# Reporter phase
#======================================================================

[Reporter: IU database]
module = MTTDatabase

mttdatabase_realm = OMPI
mttdatabase_url = https://mtt.open-mpi.org/submit/
# OMPI Core: Change this to be the username and password for your
# submit user.  Get this from the OMPI MTT administrator.
mttdatabase_username = ****
mttdatabase_password = ****
# OMPI Core: Change this to be some short string identifying your
# cluster.
mttdatabase_platform = lakhi_HLRS_nehalem_nodes

# This is a backup for while debugging MTT; it also writes results to
# a local text file

[Reporter: text file backup]
module = TextFile

textfile_filename = $phase-$section-$mpi_name-$mpi_version.txt

textfile_summary_header = <<EOT
hostname: &shell("hostname")
uname: &shell("uname -a")
who am i: &shell("who am i")
EOT

textfile_summary_footer =
textfile_detail_header =
textfile_detail_footer =

textfile_textwrap = 78


