<html><body>
<p>Gilbert,<br>
<br>
      I did not know the MCA parameters that can monitor the message passing.  I have tried a few MCA verbose parameters and did not identify anyone helpful.<br>
<br>
     One way to check if the message goes via IB or SM maybe  to check the counters in /sys/class/infiniband.<br>
<br>
Regards,<br>
Mi<br>
<img width="16" height="16" src="cid:1__=0ABBFE60DFFB72CE8f9e8a93df938@us.ibm.com" border="0" alt="Inactive hide details for Gilbert Grosdidier &lt;grodid@mail.cern.ch&gt;">Gilbert Grosdidier &lt;grodid@mail.cern.ch&gt;<br>
<br>
<br>

<table width="100%" border="0" cellspacing="0" cellpadding="0">
<tr valign="top"><td style="background-image:url(cid:2__=0ABBFE60DFFB72CE8f9e8a93df938@us.ibm.com); background-repeat: no-repeat; " width="40%">
<ul>
<ul>
<ul>
<ul><b><font size="2">Gilbert Grosdidier &lt;grodid@mail.cern.ch&gt;</font></b><font size="2"> </font><br>
<font size="2">Sent by: users-bounces@open-mpi.org</font>
<p><font size="2">10/29/2008 12:36 PM</font>
<table border="1">
<tr valign="top"><td width="168" bgcolor="#FFFFFF"><div align="center"><font size="2">Please respond to<br>
Open MPI Users &lt;users@open-mpi.org&gt;</font></div></td></tr>
</table>
</ul>
</ul>
</ul>
</ul>
</td><td width="60%">
<table width="100%" border="0" cellspacing="0" cellpadding="0">
<tr valign="top"><td width="1%"><img width="58" height="1" src="cid:3__=0ABBFE60DFFB72CE8f9e8a93df938@us.ibm.com" border="0" alt=""><br>
<div align="right"><font size="2">To</font></div></td><td width="100%"><img width="1" height="1" src="cid:3__=0ABBFE60DFFB72CE8f9e8a93df938@us.ibm.com" border="0" alt=""><br>
<font size="2">Open MPI Users &lt;users@open-mpi.org&gt;</font></td></tr>

<tr valign="top"><td width="1%"><img width="58" height="1" src="cid:3__=0ABBFE60DFFB72CE8f9e8a93df938@us.ibm.com" border="0" alt=""><br>
<div align="right"><font size="2">cc</font></div></td><td width="100%"><img width="1" height="1" src="cid:3__=0ABBFE60DFFB72CE8f9e8a93df938@us.ibm.com" border="0" alt=""><br>
</td></tr>

<tr valign="top"><td width="1%"><img width="58" height="1" src="cid:3__=0ABBFE60DFFB72CE8f9e8a93df938@us.ibm.com" border="0" alt=""><br>
<div align="right"><font size="2">Subject</font></div></td><td width="100%"><img width="1" height="1" src="cid:3__=0ABBFE60DFFB72CE8f9e8a93df938@us.ibm.com" border="0" alt=""><br>
<font size="2">Re: [OMPI users] Working with a CellBlade cluster</font></td></tr>
</table>

<table border="0" cellspacing="0" cellpadding="0">
<tr valign="top"><td width="58"><img width="1" height="1" src="cid:3__=0ABBFE60DFFB72CE8f9e8a93df938@us.ibm.com" border="0" alt=""></td><td width="336"><img width="1" height="1" src="cid:3__=0ABBFE60DFFB72CE8f9e8a93df938@us.ibm.com" border="0" alt=""></td></tr>
</table>
</td></tr>
</table>
<br>
<tt>Thank you very much Mi and Lenny for your detailed replies.<br>
<br>
 I believe I can summarize the infos to allow for <br>
'Working with a QS22 CellBlade cluster' like this:<br>
- Yes, messages are efficiently handled with &quot;-mca btl openib,sm,self&quot;<br>
- Better to go to the OMPI-1.3 version ASAP<br>
- It is currently more efficient/easy to use numactl to control<br>
processor affinity on a QS22.<br>
<br>
 So far so good.<br>
<br>
 One question remains: how could I monitor in details message passing<br>
thru IB (on one side) and thru SM (on the other side) thru the use of mca <br>
parameters, please ? Additionnal info about the verbosity level<br>
of this monitoring will be highly appreciated ... A lengthy travel<br>
inside the list of such parameters provided by ompi_info did not<br>
enlighten me (there are so many xxx_sm_yyy type params that I don't know which <br>
could be the right one ;-)<br>
<br>
 Thanks in advance for your hints, &nbsp; &nbsp; &nbsp;Best Regards, &nbsp; &nbsp; Gilbert.<br>
<br>
<br>
On Thu, 23 Oct 2008, Mi Yan wrote:<br>
<br>
&gt; <br>
&gt; 1. &nbsp;MCA BTL parameters<br>
&gt; With &quot;-mca btl openib,self&quot;, both message between two Cell processors on<br>
&gt; one QS22 and &nbsp; messages between two QS22s go through IB.<br>
&gt; <br>
&gt; With &quot;-mca btl openib,sm,slef&quot;, &nbsp;message on one QS22 go through shared<br>
&gt; memory, &nbsp;message between QS22 go through IB,<br>
&gt; <br>
&gt; Depending on the message size and other MCA parameters, &nbsp;it does not<br>
&gt; guarantee message passing on shared memory is faster than on IB. &nbsp; E.g.<br>
&gt; the bandwidth for 64KB message is 959MB/s on shared-memory and is 694MB/s<br>
&gt; on IB; &nbsp;the bandwidth for 4MB message is 539 MB/s and 1092 MB/s on &nbsp;IB.<br>
&gt; The bandwidth of 4MB message on shared memory may be higher if you tune<br>
&gt; some MCA parameter.<br>
&gt; <br>
&gt; 2. &nbsp;mpi_paffinity_alone<br>
&gt; &nbsp; &quot;mpi_paffinity_alone =1&quot; &nbsp;is not a good choice for QS22. &nbsp;There are two<br>
&gt; sockets with two physical &nbsp;Cell/B.E. on one QS22. &nbsp;Each Cell/B.E. has two<br>
&gt; SMT threads. &nbsp; So there are four logical CPUs on one QS22. &nbsp;CBE Linux<br>
&gt; kernel maps logical cpu 0 and 1 to socket1 and maps logical cpu 1 and 2 to<br>
&gt; socket 2. &nbsp; &nbsp;If mpi_paffinity_alone is set to 1, &nbsp; the two MPI instances<br>
&gt; will be assigned to logical cpu 0 and cpu 1 on socket 1. &nbsp;I believe this is<br>
&gt; not what you want.<br>
&gt; <br>
&gt; &nbsp; &nbsp; A temporaily solution to &nbsp;force the affinity on &nbsp;QS22 is to use<br>
&gt; &quot;numactl&quot;, &nbsp; E.g. &nbsp;assuming the hostname is &quot;qs22&quot; and the executable is<br>
&gt; &quot;foo&quot;. &nbsp;the following command can be used<br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; mpirun -np 1 -H qs22 numactl -c0 -m0 &nbsp;foo : &nbsp; -np 1 -H qs22<br>
&gt; numactl -c1 -m1 foo<br>
&gt; <br>
&gt; &nbsp; &nbsp;In the long run, &nbsp;I wish CBE kernel export &nbsp;CPU topology &nbsp;in /sys &nbsp;and<br>
&gt; use &nbsp;PLPA to force the processor affinity.<br>
&gt; <br>
&gt; Best Regards,<br>
&gt; Mi<br>
&gt; <br>
&gt; <br>
&gt; <br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&quot;Lenny &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Verkhovsky&quot; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&lt;lenny.verkhovsky &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;To<br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;@gmail.com&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;Open MPI Users&quot; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Sent by: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&lt;users@open-mpi.org&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;users-bounces@ope &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;cc<br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;n-mpi.org &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Subject<br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Re: [OMPI users] Working with a <br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;10/23/2008 05:48 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;CellBlade cluster &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;AM &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Please respond to &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Open MPI Users &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&lt;users@open-mpi.o &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; rg&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<br>
&gt; <br>
&gt; <br>
&gt; <br>
&gt; <br>
&gt; Hi,<br>
&gt; <br>
&gt; <br>
&gt; If I understand you correctly the most suitable way to do it is by<br>
&gt; paffinity that we have in Open MPI 1.3 and the trank.<br>
&gt; how ever usually OS is distributing processes evenly between sockets by it<br>
&gt; self.<br>
&gt; <br>
&gt; There still no formal FAQ due to a multiple reasons but you can read how to<br>
&gt; use it in the attached scratch ( there were few name changings of the<br>
&gt; params, so check with ompi_info )<br>
&gt; <br>
&gt; shared memory is used between processes that share same machine, and openib<br>
&gt; is used between different machines ( hostnames ), no special mca params are<br>
&gt; needed.<br>
&gt; <br>
&gt; Best Regards<br>
&gt; Lenny,<br>
&gt; <br>
&gt; <br>
&gt; <br>
&gt; <br>
&gt; <br>
&gt; <br>
&gt; <br>
&gt; On Sun, Oct 19, 2008 at 10:32 AM, Gilbert Grosdidier &lt;grodid@mail.cern.ch&gt;<br>
&gt; wrote:<br>
&gt; &nbsp; &nbsp;Working with a CellBlade cluster (QS22), the requirement is to have one<br>
&gt; &nbsp; instance of the executable running on each socket of the blade (there are<br>
&gt; &nbsp; 2<br>
&gt; &nbsp; sockets). The application is of the 'domain decomposition' type, and each<br>
&gt; &nbsp; instance is required to often send/receive data with both the remote<br>
&gt; &nbsp; blades and<br>
&gt; &nbsp; the neighbor socket.<br>
&gt; <br>
&gt; &nbsp; &nbsp;Question is : which specification must be used for the mca btl component<br>
&gt; &nbsp; to force 1) shmem type messages when communicating with this neighbor<br>
&gt; &nbsp; socket,<br>
&gt; &nbsp; while 2) using openib to communicate with the remote blades ?<br>
&gt; &nbsp; Is '-mca btl sm,openib,self' suitable for this ?<br>
&gt; <br>
&gt; &nbsp; &nbsp;Also, which debug flags could be used to crosscheck that the messages<br>
&gt; &nbsp; are<br>
&gt; &nbsp; _actually_ going thru the right channel for a given channel, please ?<br>
&gt; <br>
&gt; &nbsp; &nbsp;We are currently using OpenMPI 1.2.5 shipped with RHEL5.2 (ppc64).<br>
&gt; &nbsp; Which version do you think is currently the most optimised for these<br>
&gt; &nbsp; processors and problem type ? Should we go towards OpenMPI 1.2.8<br>
&gt; &nbsp; instead ?<br>
&gt; &nbsp; Or even try some OpenMPI 1.3 nightly build ?<br>
&gt; <br>
&gt; &nbsp; &nbsp;Thanks in advance for your help, &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Gilbert.<br>
&gt; <br>
&gt; &nbsp; _______________________________________________<br>
&gt; &nbsp; users mailing list<br>
&gt; &nbsp; users@open-mpi.org<br>
&gt; &nbsp; </tt><tt><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></tt><tt><br>
&gt; (See attached file: RANKS_FAQ.doc)<br>
&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; users@open-mpi.org<br>
&gt; </tt><tt><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></tt><tt><br>
<br>
-- <br>
*---------------------------------------------------------------------*<br>
 &nbsp;Gilbert Grosdidier &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Gilbert.Grosdidier@in2p3.fr<br>
 &nbsp;LAL / IN2P3 / CNRS &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Phone : +33 1 6446 8909<br>
 &nbsp;Faculté des Sciences, Bat. 200 &nbsp; &nbsp; Fax &nbsp; : +33 1 6446 8546<br>
 &nbsp;B.P. 34, F-91898 Orsay Cedex (FRANCE)<br>
 ---------------------------------------------------------------------<br>
_______________________________________________<br>
users mailing list<br>
users@open-mpi.org<br>
</tt><tt><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></tt><tt><br>
</tt><br>
</body></html>

