<html><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space; "><font class="Apple-style-span" face="'Andale Mono'"><div><font class="Apple-style-span" face="'Andale Mono'"><div>Thanks! &nbsp;I ran the command:&nbsp;</div><div><br></div><div>mpirun --slot-list 0-3 -np 4 --report-bindings $EXECUTABLE:</div><div><br></div><div>and this is the output of standard error:</div><div><br></div><div>[node50.cl.corp.com:15473] [[45030,0],0] odls:default:fork binding child [[45030,1],0] to slot_list 0-3</div><div>[node50.cl.corp.com:15473] [[45030,0],0] odls:default:fork binding child [[45030,1],1] to slot_list 0-3</div><div>[node50.cl.corp.com:15473] [[45030,0],0] odls:default:fork binding child [[45030,1],2] to slot_list 0-3</div><div>[node50.cl.corp.com:15473] [[45030,0],0] odls:default:fork binding child [[45030,1],3] to slot_list 0-3</div><div><br></div></font></div><div>top shows the first 3 cores are bound:</div><div><font class="Apple-style-span" face="'Andale Mono'"><br></font></div>top - 11:17:06 up 35 days, &nbsp;1:03, &nbsp;2 users, &nbsp;load average: 3.15, 1.15, 0.41<br>Tasks: 453 total, &nbsp; 6 running, 446 sleeping, &nbsp; 1 stopped, &nbsp; 0 zombie<br>Cpu0 &nbsp;:100.0%us, &nbsp;0.0%sy, &nbsp;0.0%ni, &nbsp;0.0%id, &nbsp;0.0%wa, &nbsp;0.0%hi, &nbsp;0.0%si, &nbsp;0.0%st<br>Cpu1 &nbsp;:100.0%us, &nbsp;0.0%sy, &nbsp;0.0%ni, &nbsp;0.0%id, &nbsp;0.0%wa, &nbsp;0.0%hi, &nbsp;0.0%si, &nbsp;0.0%st<br>Cpu2 &nbsp;:100.0%us, &nbsp;0.0%sy, &nbsp;0.0%ni, &nbsp;0.0%id, &nbsp;0.0%wa, &nbsp;0.0%hi, &nbsp;0.0%si, &nbsp;0.0%st<br>Cpu3 &nbsp;:100.0%us, &nbsp;0.0%sy, &nbsp;0.0%ni, &nbsp;0.0%id, &nbsp;0.0%wa, &nbsp;0.0%hi, &nbsp;0.0%si, &nbsp;0.0%st<br>Cpu4 &nbsp;: &nbsp;0.0%us, &nbsp;0.0%sy, &nbsp;0.0%ni,100.0%id, &nbsp;0.0%wa, &nbsp;0.0%hi, &nbsp;0.0%si, &nbsp;0.0%st<br>Cpu5 &nbsp;: &nbsp;0.0%us, &nbsp;0.0%sy, &nbsp;0.0%ni,100.0%id, &nbsp;0.0%wa, &nbsp;0.0%hi, &nbsp;0.0%si, &nbsp;0.0%st<br>Cpu6 &nbsp;: &nbsp;0.0%us, &nbsp;0.3%sy, &nbsp;0.0%ni, 99.7%id, &nbsp;0.0%wa, &nbsp;0.0%hi, &nbsp;0.0%si, &nbsp;0.0%st<br>Cpu7 &nbsp;: &nbsp;0.0%us, &nbsp;0.0%sy, &nbsp;0.0%ni,100.0%id, &nbsp;0.0%wa, &nbsp;0.0%hi, &nbsp;0.0%si, &nbsp;0.0%st<br>Mem: &nbsp; 8059116k total, &nbsp;1577220k used, &nbsp;6481896k free, &nbsp; &nbsp;62020k buffers<br>Swap: 16787916k total, &nbsp; &nbsp;61108k used, 16726808k free, &nbsp; 718036k cached</font><div><font class="Apple-style-span" face="'Andale Mono'"><br></font></div><div><font class="Apple-style-span" face="'Andale Mono'"><br></font></div><div><font class="Apple-style-span" face="'Andale Mono'">For a multinode job, rankfile is needed:</font></div><div><font class="Apple-style-span" face="'Andale Mono'"><br></font></div><div><font class="Apple-style-span" face="'Andale Mono'"><a href="http://www.open-mpi.org/faq/?category=tuning#using-paffinity-v1.3">http://www.open-mpi.org/faq/?category=tuning#using-paffinity-v1.3</a></font></div><div><font class="Apple-style-span" face="'Andale Mono'"><br></font></div><div><font class="Apple-style-span" face="'Andale Mono'">Appreciate the suggestions and solution.</font></div><div><font class="Apple-style-span" face="'Andale Mono'"><br></font></div><div><font class="Apple-style-span" face="'Andale Mono'"><br><br></font>On Jul 16, 2012, at 5:08 PM, Ralph Castain wrote:<br><br><blockquote type="cite">Or you could just do:<br><br>mpirun --slot-list 0-3 -np 4 hostname<br><br>That will put the four procs on the cpu numbers 0-3, which should all be on the first socket<br><br><br>On Jul 16, 2012, at 3:23 PM, Dominik Goeddeke wrote:<br><br><blockquote type="cite">in the "old" 1.4.x and 1.5.x, I&nbsp;achieved this by using rankfiles (see FAQ), and it worked very&nbsp;well. With these versions, --byslot etc. didn't work for me, I&nbsp;always needed the rankfiles. I haven't tried the overhauled&nbsp;"convenience wrappers" in 1.6 that you are using for this feature&nbsp;yet, but I see no reason&nbsp;why the "old" way should not work,&nbsp;although it requires some shell magic if rankfiles are to be&nbsp;generated automatically from e.g. PBS or SLURM node&nbsp;lists.<br><br>Dominik<br><br>On 07/17/2012 12:13 AM, Anne M. Hammond wrote:<br><blockquote cite="mid:EDFA87CB-EEC8-46C4-BFCB-5D2CFBCA048B@txcorp.com" type="cite">There are 2 physical processors, each with 4 cores (no&nbsp;hyperthreading).<br><br>I want to instruct openmpi to run only on the first&nbsp;processor, using 4 cores.<br><br><br>[hammond@node48&nbsp;~]$ cat /proc/cpuinfo<br>processor&nbsp;: 0<br>vendor_id&nbsp;: AuthenticAMD<br>cpu family&nbsp;: 16<br>model&nbsp;: 4<br>model name&nbsp;: Quad-Core AMD&nbsp;Opteron(tm) Processor 2376<br>stepping&nbsp;: 2<br>cpu MHz&nbsp;: 2311.694<br>cache size&nbsp;: 512 KB<br>physical id&nbsp;: 0<br>siblings&nbsp;: 4<br>core id&nbsp;: 0<br>cpu cores&nbsp;: 4<br>apicid&nbsp;: 0<br>initial apicid&nbsp;: 0<br>fpu&nbsp;: yes<br>fpu_exception&nbsp;: yes<br>cpuid level&nbsp;: 5<br>wp&nbsp;: yes<br>flags&nbsp;: fpu vme de pse&nbsp;tsc msr pae mce cx8 apic mtrr pge mca cmov pat pse36 clflush mmx&nbsp;fxsr sse sse2 ht syscall&nbsp;nx mmxext fxsr_opt pdpe1gb rdtscp lm&nbsp;3dnowext 3dnow constant_tsc rep_good nonstop_tsc extd_apicid pni&nbsp;monitor&nbsp;cx16 popcnt lahf_lm cmp_legacy svm extapic cr8_legacy&nbsp;abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt npt&nbsp;lbrv&nbsp;svm_lock nrip_save<br>bogomips&nbsp;: 4623.38<br>TLB size&nbsp;: 1024 4K pages<br>clflush size&nbsp;: 64<br>cache_alignment&nbsp;: 64<br>address sizes&nbsp;: 48 bits&nbsp;physical, 48 bits virtual<br>power management: ts ttp tm stc 100mhzsteps hwpstate<br><br>processor&nbsp;: 1<br>vendor_id&nbsp;: AuthenticAMD<br>cpu family&nbsp;: 16<br>model&nbsp;: 4<br>model name&nbsp;: Quad-Core AMD&nbsp;Opteron(tm) Processor 2376<br>stepping&nbsp;: 2<br>cpu MHz&nbsp;: 2311.694<br>cache size&nbsp;: 512 KB<br>physical id&nbsp;: 0<br>siblings&nbsp;: 4<br>core id&nbsp;: 1<br>cpu cores&nbsp;: 4<br>apicid&nbsp;: 1<br>initial apicid&nbsp;: 1<br>fpu&nbsp;: yes<br>fpu_exception&nbsp;: yes<br>cpuid level&nbsp;: 5<br>wp&nbsp;: yes<br>flags&nbsp;: fpu vme de pse&nbsp;tsc msr pae mce cx8 apic mtrr pge mca cmov pat pse36 clflush mmx&nbsp;fxsr sse sse2 ht syscall&nbsp;nx mmxext fxsr_opt pdpe1gb rdtscp lm&nbsp;3dnowext 3dnow constant_tsc rep_good nonstop_tsc extd_apicid pni&nbsp;monitor&nbsp;cx16 popcnt lahf_lm cmp_legacy svm extapic cr8_legacy&nbsp;abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt npt&nbsp;lbrv&nbsp;svm_lock nrip_save<br>bogomips&nbsp;: 4623.17<br>TLB size&nbsp;: 1024 4K pages<br>clflush size&nbsp;: 64<br>cache_alignment&nbsp;: 64<br>address sizes&nbsp;: 48 bits&nbsp;physical, 48 bits virtual<br>power management: ts ttp tm stc 100mhzsteps hwpstate<br><br>processor&nbsp;: 2<br>vendor_id&nbsp;: AuthenticAMD<br>cpu family&nbsp;: 16<br>model&nbsp;: 4<br>model name&nbsp;: Quad-Core AMD&nbsp;Opteron(tm) Processor 2376<br>stepping&nbsp;: 2<br>cpu MHz&nbsp;: 2311.694<br>cache size&nbsp;: 512 KB<br>physical id&nbsp;: 0<br>siblings&nbsp;: 4<br>core id&nbsp;: 2<br>cpu cores&nbsp;: 4<br>apicid&nbsp;: 2<br>initial apicid&nbsp;: 2<br>fpu&nbsp;: yes<br>fpu_exception&nbsp;: yes<br>cpuid level&nbsp;: 5<br>wp&nbsp;: yes<br>flags&nbsp;: fpu vme de pse&nbsp;tsc msr pae mce cx8 apic mtrr pge mca cmov pat pse36 clflush mmx&nbsp;fxsr sse sse2 ht syscall&nbsp;nx mmxext fxsr_opt pdpe1gb rdtscp lm&nbsp;3dnowext 3dnow constant_tsc rep_good nonstop_tsc extd_apicid pni&nbsp;monitor&nbsp;cx16 popcnt lahf_lm cmp_legacy svm extapic cr8_legacy&nbsp;abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt npt&nbsp;lbrv&nbsp;svm_lock nrip_save<br>bogomips&nbsp;: 4623.19<br>TLB size&nbsp;: 1024 4K pages<br>clflush size&nbsp;: 64<br>cache_alignment&nbsp;: 64<br>address sizes&nbsp;: 48 bits&nbsp;physical, 48 bits virtual<br>power management: ts ttp tm stc 100mhzsteps hwpstate<br><br>processor&nbsp;: 3<br>vendor_id&nbsp;: AuthenticAMD<br>cpu family&nbsp;: 16<br>model&nbsp;: 4<br>model name&nbsp;: Quad-Core AMD&nbsp;Opteron(tm) Processor 2376<br>stepping&nbsp;: 2<br>cpu MHz&nbsp;: 2311.694<br>cache size&nbsp;: 512 KB<br>physical id&nbsp;: 0<br>siblings&nbsp;: 4<br>core id&nbsp;: 3<br>cpu cores&nbsp;: 4<br>apicid&nbsp;: 3<br>initial apicid&nbsp;: 3<br>fpu&nbsp;: yes<br>fpu_exception&nbsp;: yes<br>cpuid level&nbsp;: 5<br>wp&nbsp;: yes<br>flags&nbsp;: fpu vme de pse&nbsp;tsc msr pae mce cx8 apic mtrr pge mca cmov pat pse36 clflush mmx&nbsp;fxsr sse sse2 ht syscall&nbsp;nx mmxext fxsr_opt pdpe1gb rdtscp lm&nbsp;3dnowext 3dnow constant_tsc rep_good nonstop_tsc extd_apicid pni&nbsp;monitor&nbsp;cx16 popcnt lahf_lm cmp_legacy svm extapic cr8_legacy&nbsp;abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt npt&nbsp;lbrv&nbsp;svm_lock nrip_save<br>bogomips&nbsp;: 4623.16<br>TLB size&nbsp;: 1024 4K pages<br>clflush size&nbsp;: 64<br>cache_alignment&nbsp;: 64<br>address sizes&nbsp;: 48 bits&nbsp;physical, 48 bits virtual<br>power management: ts ttp tm stc 100mhzsteps hwpstate<br><br>processor&nbsp;: 4<br>vendor_id&nbsp;: AuthenticAMD<br>cpu family&nbsp;: 16<br>model&nbsp;: 4<br>model name&nbsp;: Quad-Core AMD&nbsp;Opteron(tm) Processor 2376<br>stepping&nbsp;: 2<br>cpu MHz&nbsp;: 2311.694<br>cache size&nbsp;: 512 KB<br>physical id&nbsp;: 1<br>siblings&nbsp;: 4<br>core id&nbsp;: 0<br>cpu cores&nbsp;: 4<br>apicid&nbsp;: 4<br>initial apicid&nbsp;: 4<br>fpu&nbsp;: yes<br>fpu_exception&nbsp;: yes<br>cpuid level&nbsp;: 5<br>wp&nbsp;: yes<br>flags&nbsp;: fpu vme de pse&nbsp;tsc msr pae mce cx8 apic mtrr pge mca cmov pat pse36 clflush mmx&nbsp;fxsr sse sse2 ht syscall&nbsp;nx mmxext fxsr_opt pdpe1gb rdtscp lm&nbsp;3dnowext 3dnow constant_tsc rep_good nonstop_tsc extd_apicid pni&nbsp;monitor&nbsp;cx16 popcnt lahf_lm cmp_legacy svm extapic cr8_legacy&nbsp;abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt npt&nbsp;lbrv&nbsp;svm_lock nrip_save<br>bogomips&nbsp;: 4623.16<br>TLB size&nbsp;: 1024 4K pages<br>clflush size&nbsp;: 64<br>cache_alignment&nbsp;: 64<br>address sizes&nbsp;: 48 bits&nbsp;physical, 48 bits virtual<br>power management: ts ttp tm stc 100mhzsteps hwpstate<br><br>processor&nbsp;: 5<br>vendor_id&nbsp;: AuthenticAMD<br>cpu family&nbsp;: 16<br>model&nbsp;: 4<br>model name&nbsp;: Quad-Core AMD&nbsp;Opteron(tm) Processor 2376<br>stepping&nbsp;: 2<br>cpu MHz&nbsp;: 2311.694<br>cache size&nbsp;: 512 KB<br>physical id&nbsp;: 1<br>siblings&nbsp;: 4<br>core id&nbsp;: 1<br>cpu cores&nbsp;: 4<br>apicid&nbsp;: 5<br>initial apicid&nbsp;: 5<br>fpu&nbsp;: yes<br>fpu_exception&nbsp;: yes<br>cpuid level&nbsp;: 5<br>wp&nbsp;: yes<br>flags&nbsp;: fpu vme de pse&nbsp;tsc msr pae mce cx8 apic mtrr pge mca cmov pat pse36 clflush mmx&nbsp;fxsr sse sse2 ht syscall&nbsp;nx mmxext fxsr_opt pdpe1gb rdtscp lm&nbsp;3dnowext 3dnow constant_tsc rep_good nonstop_tsc extd_apicid pni&nbsp;monitor&nbsp;cx16 popcnt lahf_lm cmp_legacy svm extapic cr8_legacy&nbsp;abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt npt&nbsp;lbrv&nbsp;svm_lock nrip_save<br>bogomips&nbsp;: 4623.16<br>TLB size&nbsp;: 1024 4K pages<br>clflush size&nbsp;: 64<br>cache_alignment&nbsp;: 64<br>address sizes&nbsp;: 48 bits&nbsp;physical, 48 bits virtual<br>power management: ts ttp tm stc 100mhzsteps hwpstate<br><br>processor&nbsp;: 6<br>vendor_id&nbsp;: AuthenticAMD<br>cpu family&nbsp;: 16<br>model&nbsp;: 4<br>model name&nbsp;: Quad-Core AMD&nbsp;Opteron(tm) Processor 2376<br>stepping&nbsp;: 2<br>cpu MHz&nbsp;: 2311.694<br>cache size&nbsp;: 512 KB<br>physical id&nbsp;: 1<br>siblings&nbsp;: 4<br>core id&nbsp;: 2<br>cpu cores&nbsp;: 4<br>apicid&nbsp;: 6<br>initial apicid&nbsp;: 6<br>fpu&nbsp;: yes<br>fpu_exception&nbsp;: yes<br>cpuid level&nbsp;: 5<br>wp&nbsp;: yes<br>flags&nbsp;: fpu vme de pse&nbsp;tsc msr pae mce cx8 apic mtrr pge mca cmov pat pse36 clflush mmx&nbsp;fxsr sse sse2 ht syscall&nbsp;nx mmxext fxsr_opt pdpe1gb rdtscp lm&nbsp;3dnowext 3dnow constant_tsc rep_good nonstop_tsc extd_apicid pni&nbsp;monitor&nbsp;cx16 popcnt lahf_lm cmp_legacy svm extapic cr8_legacy&nbsp;abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt npt&nbsp;lbrv&nbsp;svm_lock nrip_save<br>bogomips&nbsp;: 4623.17<br>TLB size&nbsp;: 1024 4K pages<br>clflush size&nbsp;: 64<br>cache_alignment&nbsp;: 64<br>address sizes&nbsp;: 48 bits&nbsp;physical, 48 bits virtual<br>power management: ts ttp tm stc 100mhzsteps hwpstate<br><br>processor&nbsp;: 7<br>vendor_id&nbsp;: AuthenticAMD<br>cpu family&nbsp;: 16<br>model&nbsp;: 4<br>model name&nbsp;: Quad-Core AMD&nbsp;Opteron(tm) Processor 2376<br>stepping&nbsp;: 2<br>cpu MHz&nbsp;: 2311.694<br>cache size&nbsp;: 512 KB<br>physical id&nbsp;: 1<br>siblings&nbsp;: 4<br>core id&nbsp;: 3<br>cpu cores&nbsp;: 4<br>apicid&nbsp;: 7<br>initial apicid&nbsp;: 7<br>fpu&nbsp;: yes<br>fpu_exception&nbsp;: yes<br>cpuid level&nbsp;: 5<br>wp&nbsp;: yes<br>flags&nbsp;: fpu vme de pse&nbsp;tsc msr pae mce cx8 apic mtrr pge mca cmov pat pse36 clflush mmx&nbsp;fxsr sse sse2 ht syscall&nbsp;nx mmxext fxsr_opt pdpe1gb rdtscp lm&nbsp;3dnowext 3dnow constant_tsc rep_good nonstop_tsc extd_apicid pni&nbsp;monitor&nbsp;cx16 popcnt lahf_lm cmp_legacy svm extapic cr8_legacy&nbsp;abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt npt&nbsp;lbrv&nbsp;svm_lock nrip_save<br>bogomips&nbsp;: 4623.18<br>TLB size&nbsp;: 1024 4K pages<br>clflush size&nbsp;: 64<br>cache_alignment&nbsp;: 64<br>address sizes&nbsp;: 48 bits&nbsp;physical, 48 bits virtual<br>power management: ts ttp tm stc 100mhzsteps hwpstate<br><br><br>On Jul 16, 2012, at 4:09 PM, Elken, Tom wrote:<br><br><blockquote type="cite">Anne,<br><br>output from "cat /proc/cpuinfo" on your node "hostname" &nbsp;may&nbsp;help those trying to answer.<br><br>-Tom<br><br><blockquote type="cite">-----Original Message-----<br></blockquote><blockquote type="cite">From:&nbsp;<a href="mailto:users-bounces@open-mpi.org">users-bounces@open-mpi.org</a>&nbsp;[mailto:users-bounces@open-mpi.org] On<br></blockquote><blockquote type="cite">Behalf Of Ralph Castain<br></blockquote><blockquote type="cite">Sent: Monday, July 16, 2012 2:47 PM<br></blockquote><blockquote type="cite">To: Open MPI Users<br></blockquote><blockquote type="cite">Subject: Re: [OMPI users] openmpi&nbsp;tar.gz for 1.6.1 or 1.6.2<br></blockquote><blockquote type="cite"><br></blockquote><blockquote type="cite">I gather there are two sockets on&nbsp;this node? So the second cmd line is equivalent<br></blockquote><blockquote type="cite">to leaving "num-sockets" off of the&nbsp;cmd line?<br></blockquote><blockquote type="cite"><br></blockquote><blockquote type="cite">I haven't tried what you are doing,&nbsp;so it is quite possible this is a bug.<br></blockquote><blockquote type="cite"><br></blockquote><blockquote type="cite"><br></blockquote><blockquote type="cite">On Jul 16, 2012, at 1:49 PM, Anne M.&nbsp;Hammond wrote:<br></blockquote><blockquote type="cite"><br></blockquote><blockquote type="cite"><blockquote type="cite">Thanks!<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">Built the latest snapshot. &nbsp;Still&nbsp;getting an error when trying to run<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">on only one socket (see below):&nbsp;&nbsp;Is there a workaround?<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">[hammond@node65 bin]$ ./mpirun -np&nbsp;4 --num-sockets 1 --npersocket 4<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">hostname<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">----------------------------------------------------------------------<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">---- An invalid physical processor&nbsp;ID was returned when attempting to<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">bind an MPI process to a unique&nbsp;processor.<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">This usually means that you&nbsp;requested binding to more processors than<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">exist (e.g., trying to bind N MPI&nbsp;processes to M processors, where N &gt;<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">M). &nbsp;Double check that you have&nbsp;enough unique processors for all the<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">MPI processes that you are&nbsp;launching on this host.<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">You job will now abort.<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">----------------------------------------------------------------------<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">----<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">----------------------------------------------------------------------<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">---- mpirun was unable to start&nbsp;the specified application as it<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">encountered an error:<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">Error name: Fatal<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">Node:&nbsp;<a href="http://node65.cl.corp.com">node65.cl.corp.com</a><br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">when attempting to start process&nbsp;rank 0.<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">----------------------------------------------------------------------<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">----<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">4 total processes failed to start<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">[hammond@node65 bin]$ ./mpirun -np&nbsp;4 --num-sockets 2 --npersocket 4<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">hostname&nbsp;<a href="http://node65.cl.corp.com">node65.cl.corp.com</a>&nbsp;node65.cl.corp.com&nbsp;node65.cl.corp.com<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><a href="http://node65.cl.corp.com">node65.cl.corp.com</a><br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">[hammond@node65 bin]$<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">On Jul 16, 2012, at 12:56 PM,&nbsp;Ralph Castain wrote:<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">Jeff is at the MPI Forum this&nbsp;week, so his answers will be delayed. Last I<br></blockquote></blockquote></blockquote><blockquote type="cite">heard, it was close, but no specific&nbsp;date has been set.<br></blockquote><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">On Jul 16, 2012, at 11:49 AM,&nbsp;Michael E. Thomadakis wrote:<br></blockquote></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">When is the expected date for&nbsp;the official 1.6.1 (or 1.6.2 ?) to be available ?<br></blockquote></blockquote></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">mike<br></blockquote></blockquote></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">On 07/16/2012 01:44 PM, Ralph&nbsp;Castain wrote:<br></blockquote></blockquote></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">You can get it here:<br></blockquote></blockquote></blockquote></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote></blockquote></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><a href="http://www.open-mpi.org/nightly/v1.6/">http://www.open-mpi.org/nightly/v1.6/</a><br></blockquote></blockquote></blockquote></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote></blockquote></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">On Jul 16, 2012, at 10:22&nbsp;AM, Anne M. Hammond wrote:<br></blockquote></blockquote></blockquote></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote></blockquote></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">Hi,<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">For benchmarking, we would&nbsp;like to use openmpi with<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">--num-sockets 1<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">This fails in 1.6, but Bug&nbsp;Report #3119 indicates it is changed in<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">1.6.1.<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">Is 1.6.1 or 1.6.2&nbsp;available in tar.gz form?<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">Thanks!<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">Anne<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">_______________________________________________<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">users mailing list<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">_______________________________________________<br></blockquote></blockquote></blockquote></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">users mailing list<br></blockquote></blockquote></blockquote></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br></blockquote></blockquote></blockquote></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></blockquote></blockquote></blockquote></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">_______________________________________________<br></blockquote></blockquote></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">users mailing list<br></blockquote></blockquote></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br></blockquote></blockquote></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></blockquote></blockquote></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">_______________________________________________<br></blockquote></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">users mailing list<br></blockquote></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br></blockquote></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></blockquote></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">Anne M. Hammond - Systems /&nbsp;Network Administration - Tech-X Corp<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;hammond_at_txcorp.com 720-974-1840<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">_______________________________________________<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">users mailing list<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></blockquote></blockquote><blockquote type="cite"><br></blockquote><blockquote type="cite"><br></blockquote><blockquote type="cite">_______________________________________________<br></blockquote><blockquote type="cite">users mailing list<br></blockquote><blockquote type="cite"><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br></blockquote><blockquote type="cite"><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></blockquote><br>_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>http://www.open-mpi.org/mailman/listinfo.cgi/users<br><br></blockquote><br>Anne M. Hammond - Systems / Network Administration - Tech-X&nbsp;Corp<br>&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;hammond_at_txcorp.com 720-974-1840<br><br><br><br><br><br><br><div>_______________________________________________</div><div>users mailing list</div><br class="Apple-interchange-newline"><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>http://www.open-mpi.org/mailman/listinfo.cgi/users<br></blockquote><br><br><div>--&nbsp;</div><div>Jun.-Prof. Dr. Dominik Göddeke</div><div>Hardware-orientierte Numerik für große Systeme</div><div>Institut für Angewandte Mathematik (LS III)</div><div>Fakultät für Mathematik, Technische Universität Dortmund</div><br class="Apple-interchange-newline"><a href="http://www.mathematik.tu-dortmund.de/~goeddeke">http://www.mathematik.tu-dortmund.de/~goeddeke</a><div><br></div><div>Tel. +49-(0)231-755-7218 &nbsp;Fax +49-(0)231-755-5933</div><div>--</div><div>Sent from my old-fashioned computer and not from a mobile device.</div><div>I proudly boycott 24/7 availability.</div><div><br></div><br class="Apple-interchange-newline">_______________________________________________<br>users mailing list<br>users@open-mpi.org<br>http://www.open-mpi.org/mailman/listinfo.cgi/users<br></blockquote><br>_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>http://www.open-mpi.org/mailman/listinfo.cgi/users<br></blockquote><br><div>Anne M. Hammond - Systems / Network Administration - Tech-X Corp<br>&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;hammond_at_txcorp.com 720-974-1840<br><br><br><br></div><br></div></body></html>
