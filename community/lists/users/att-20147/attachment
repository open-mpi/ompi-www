<html><body><div style="color:#000; background-color:#fff; font-family:times new roman, new york, times, serif;font-size:12pt"><div>Yevgeny,</div><div>The ibstat results:<br>
CA 'mthca0'<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; CA type: MT25208 (MT23108 compat mode)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Number of ports: 2<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Firmware version: 4.7.600<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Hardware version: a0<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Node GUID: 0x0005ad00000c21e0<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; System image GUID: 0x0005ad000100d050<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Port 1:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; State: Active<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Physical state: LinkUp<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Rate: 10<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Base lid: 4<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; LMC: 0<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; SM lid: 2<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Capability mask: 0x02510a68<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Port GUID: 0x0005ad00000c21e1<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Link layer: IB<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Port 2:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; State: Down<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Physical state: Polling<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Rate: 10<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Base lid: 0<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; LMC: 0<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; SM lid: 0<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Capability mask: 0x02510a68<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Port GUID: 0x0005ad00000c21e2<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Link layer: IB<br>
<br>
And more interestingly, ib_write_bw: <br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; RDMA_Write BW Test<br>
&nbsp;Number of qps&nbsp;&nbsp; : 1<br>
&nbsp;Connection type : RC<br>
&nbsp;TX depth&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; : 300<br>
&nbsp;CQ Moderation&nbsp;&nbsp; : 50<br>
&nbsp;Link type&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; : IB<br>
&nbsp;Mtu&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; : 2048<br>
&nbsp;Inline data is used up to 0 bytes message<br>
&nbsp;local address: LID 0x04 QPN 0x1c0407 PSN 0x48ad9e RKey 0xd86a0051 VAddr 0x002ae362870000<br>
&nbsp;remote address: LID 0x03 QPN 0x2e0407 PSN 0xf57209 RKey 0x8d98003b VAddr 0x002b533d366000<br>
------------------------------------------------------------------<br>
&nbsp;#bytes&nbsp;&nbsp;&nbsp;&nbsp; #iterations&nbsp;&nbsp;&nbsp; BW peak[MB/sec]&nbsp;&nbsp;&nbsp; BW average[MB/sec]<br>
Conflicting CPU frequency values detected: 1600.000000 != 3301.000000<br>
&nbsp;65536&nbsp;&nbsp;&nbsp;&nbsp; 5000&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0.00&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0.00&nbsp;&nbsp; <br>
------------------------------------------------------------------<br>
<br>
What does Conflicting CPU frequency values mean?<br>
<br>
Examining the /proc/cpuinfo file however shows:<br>
processor&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; : 0<br>
cpu MHz&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; : 3301.000<br>
processor&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; : 1<br>
cpu MHz&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; : 3301.000<br>
processor&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; : 2<br>
</div><div><span>cpu MHz&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; : 1600.000<br>
processor&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; : 3<br>
cpu MHz&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; : 1600.000<br>
</span></div>
<div style="color: rgb(0, 0, 0); font-size: 16px; font-family: times new roman,new york,times,serif; background-color: transparent; font-style: normal;"><span><br>
  </span></div>
<div style="color: rgb(0, 0, 0); font-size: 16px; font-family: times new roman,new york,times,serif; background-color: transparent; font-style: normal;"><span>Which seems oddly wierd to me...</span></div>
<div style="color: rgb(0, 0, 0); font-size: 16px; font-family: times new roman,new york,times,serif; background-color: transparent; font-style: normal;"><span><br></span></div>
<div><br></div>  <div style="font-family: times new roman,new york,times,serif; font-size: 12pt;"> <div style="font-family: times new roman,new york,times,serif; font-size: 12pt;"> <div dir="ltr"> <font face="Arial" size="2"> <hr size="1">  <b><span style="font-weight: bold;">From:</span></b> Yevgeny Kliteynik &lt;kliteyn@dev.mellanox.co.il&gt;<br> <b><span style="font-weight: bold;">To:</span></b> Randolph Pullen &lt;randolph_pullen@yahoo.com.au&gt;; OpenMPI Users &lt;users@open-mpi.org&gt; <br> <b><span style="font-weight: bold;">Sent:</span></b> Thursday, 6 September 2012 6:03 PM<br> <b><span style="font-weight: bold;">Subject:</span></b> Re: [OMPI users] Infiniband performance Problem and stalling<br> </font> </div> <br>On 9/3/2012 4:14 AM, Randolph Pullen wrote:<br>&gt; No RoCE, Just native IB with TCP over the top.<br><br>Sorry, I'm confused - still not clear what is "Melanox III HCA 10G card".<br>Could you run "ibstat" and post the
 results?<br><br>What is the expected BW on your cards?<br>Could you run "ib_write_bw" between two machines?<br><br>Also, please see below.<br><br>&gt; No I haven't used 1.6 I was trying to stick with the standards on the mellanox disk.<br>&gt; Is there a known problem with 1.4.3 ?<br>&gt; <br>&gt;
 ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
------------------------<br>&gt; *From:* Yevgeny Kliteynik &lt;<a ymailto="mailto:kliteyn@dev.mellanox.co.il" href="mailto:kliteyn@dev.mellanox.co.il">kliteyn@dev.mellanox.co.il</a>&gt;<br>&gt; *To:* Randolph Pullen &lt;<a ymailto="mailto:randolph_pullen@yahoo.com.au" href="mailto:randolph_pullen@yahoo.com.au">randolph_pullen@yahoo.com.au</a>&gt;; Open MPI Users &lt;<a ymailto="mailto:users@open-mpi.org" href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;<br>&gt; *Sent:* Sunday, 2 September 2012 10:54 PM<br>&gt; *Subject:* Re: [OMPI users] Infiniband performance Problem and stalling<br>&gt; <br>&gt; Randolph,<br>&gt; <br>&gt; Some clarification on the setup:<br>&gt; <br>&gt; "Melanox III HCA 10G cards" - are those ConnectX 3 cards configured to Ethernet?<br>&gt; That is, when you're using openib BTL, you mean RoCE, right?<br>&gt; <br>&gt; Also, have you had a chance to try some newer OMPI release?<br>&gt; Any 1.6.x would do.<br>&gt; <br>&gt;
 <br>&gt; -- YK<br>&gt; <br>&gt; On 8/31/2012 10:53 AM, Randolph Pullen wrote:<br>&gt;&nbsp; &gt; (reposted with consolidatedinformation)<br>&gt;&nbsp; &gt; I have a test rig comprising 2 i7 systems 8GB RAM with Melanox III HCA 10G cards<br>&gt;&nbsp; &gt; running Centos 5.7 Kernel 2.6.18-274<br>&gt;&nbsp; &gt; Open MPI 1.4.3<br>&gt;&nbsp; &gt; MLNX_OFED_LINUX-1.5.3-1.0.0.2 (OFED-1.5.3-1.0.0.2):<br>&gt;&nbsp; &gt; On a Cisco 24 pt switch<br>&gt;&nbsp; &gt; Normal performance is:<br>&gt;&nbsp; &gt; $ mpirun --mca btl openib,self -n 2 -hostfile mpi.hosts PingPong<br>&gt;&nbsp; &gt; results in:<br>&gt;&nbsp; &gt; Max rate = 958.388867 MB/sec Min latency = 4.529953 usec<br>&gt;&nbsp; &gt; and:<br>&gt;&nbsp; &gt; $ mpirun --mca btl tcp,self -n 2 -hostfile mpi.hosts PingPong<br>&gt;&nbsp; &gt; Max rate = 653.547293 MB/sec Min latency = 19.550323 usec<br>&gt;&nbsp; &gt; NetPipeMPI results show a max of 7.4 Gb/s at 8388605 bytes which seems fine.<br>&gt;&nbsp;
 &gt; log_num_mtt =20 and log_mtts_per_seg params =2<br>&gt;&nbsp; &gt; My application exchanges about a gig of data between the processes with 2 sender and 2 consumer processes on each node with 1 additional controller process on the starting node.<br>&gt;&nbsp; &gt; The program splits the data into 64K blocks and uses non blocking sends and receives with busy/sleep loops to monitor progress until completion.<br>&gt;&nbsp; &gt; Each process owns a single buffer for these 64K blocks.<br>&gt;&nbsp; &gt; My problem is I see better performance under IPoIB then I do on native IB (RDMA_CM).<br>&gt;&nbsp; &gt; My understanding is that IPoIB is limited to about 1G/s so I am at a loss to know why it is faster.<br>&gt;&nbsp; &gt; These 2 configurations are equivelant (about 8-10 seconds per cycle)<br>&gt;&nbsp; &gt; mpirun --mca btl_openib_flags 2 --mca mpi_leave_pinned 1 --mca btl tcp,self -H vh2,vh1 -np 9 --bycore prog<br>&gt;&nbsp; &gt; mpirun --mca
 btl_openib_flags 3 --mca mpi_leave_pinned 1 --mca btl tcp,self -H vh2,vh1 -np 9 --bycore prog<br><br>When you say "--mca btl tcp,self", it means that openib btl is not enabled.<br>Hence "--mca btl_openib_flags" is irrelevant.<br><br>&gt;&nbsp; &gt; And this one produces similar run times but seems to degrade with repeated cycles:<br>&gt;&nbsp; &gt; mpirun --mca btl_openib_eager_limit 64 --mca mpi_leave_pinned 1 --mca btl openib,self -H vh2,vh1 -np 9 --bycore prog<br><br>You're running 9 ranks on two machines, but you're using IB for intra-node communication.<br>Is it intentional? If not, you can add "sm" btl and have performance improved.<br><br>-- YK<br><br>&gt;&nbsp; &gt; Other btl_openib_flags settings result in much lower performance.<br>&gt;&nbsp; &gt; Changing the first of the above configs to use openIB results in a 21 second run time at best. Sometimes it takes up to 5 minutes.<br>&gt;&nbsp; &gt; In all cases, OpenIB runs in twice the time it
 takes TCP,except if I push the small message max to 64K and force short messages. Then the openib times are the same as TCP and no faster.<br>&gt;&nbsp; &gt; With openib:<br>&gt;&nbsp; &gt; - Repeated cycles during a single run seem to slow down with each cycle<br>&gt;&nbsp; &gt; (usually by about 10 seconds).<br>&gt;&nbsp; &gt; - On occasions it seems to stall indefinitely, waiting on a single receive.<br>&gt;&nbsp; &gt; I'm still at a loss as to why. I canâ€™t find any errors logged during the runs.<br>&gt;&nbsp; &gt; Any ideas appreciated.<br>&gt;&nbsp; &gt; Thanks in advance,<br>&gt;&nbsp; &gt; Randolph<br>&gt;&nbsp; &gt;<br>&gt;&nbsp; &gt;<br>&gt;&nbsp; &gt; _______________________________________________<br>&gt;&nbsp; &gt; users mailing list<br>&gt;&nbsp; &gt; <a ymailto="mailto:users@open-mpi.org" href="mailto:users@open-mpi.org">users@open-mpi.org</a> &lt;mailto:<a ymailto="mailto:users@open-mpi.org"
 href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;<br>&gt;&nbsp; &gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>&gt; <br>&gt; <br>&gt; <br><br><br><br> </div> </div>  </div></body></html>
