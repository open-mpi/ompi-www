<html>
  <head>
    <meta content="text/html; charset=windows-1252"
      http-equiv="Content-Type">
  </head>
  <body bgcolor="#FFFFFF" text="#000000">
    <p>Siegmar,</p>
    <p><br>
    </p>
    <p>here is the error :</p>
    <p>configure:17969: cc -o conftest -m64 -D_REENTRANT -g  -g
      -I/export2/src/openmpi-2.0.0/openmpi-v2.x-dev-1290-gbd0e4e1
-I/export2/src/openmpi-2.0.0/openmpi-v2.x-dev-1290-gbd0e4e1-SunOS.sparc.64_cc
-I/export2/src/openmpi-2.0.0/openmpi-v2.x-dev-1290-gbd0e4e1/opal/include
-I/export2/src/openmpi-2.0.0/openmpi-v2.x-dev-1290-gbd0e4e1-SunOS.sparc.64_cc/opal/include 
      -D_REENTRANT 
-I/export2/src/openmpi-2.0.0/openmpi-v2.x-dev-1290-gbd0e4e1/opal/mca/hwloc/hwloc1112/hwloc/include
-I/export2/src/openmpi-2.0.0/openmpi-v2.x-dev-1290-gbd0e4e1-SunOS.sparc.64_cc/opal/mca/hwloc/hwloc1112/hwloc/include
-I/export2/src/openmpi-2.0.0/openmpi-v2.x-dev-1290-gbd0e4e1/opal/mca/event/libevent2022/libevent
-I/export2/src/openmpi-2.0.0/openmpi-v2.x-dev-1290-gbd0e4e1/opal/mca/event/libevent2022/libevent/include
-I/export2/src/openmpi-2.0.0/openmpi-v2.x-dev-1290-gbd0e4e1-SunOS.sparc.64_cc/opal/mca/event/libevent2022/libevent/include
      -m64 conftest.c  &gt;&amp;5<br>
      "/usr/include/stdbool.h", line 42: #error: "Use of
      &lt;stdbool.h&gt; is valid only in a c99 compilation environment."<br>
    </p>
    <p><br>
    </p>
    <p>i cannot reproduce this on solaris 11 with oracle studio 5.3
      compiler, and i do not have solaris 10 yet.<br>
    </p>
    <p>could you please re-configure with '-std=c99' appended to your
      CFLAGS and see if it helps ?</p>
    <p><br>
    </p>
    <p>Cheers,</p>
    <p><br>
    </p>
    <p>Gilles<br>
    </p>
    <br>
    <div class="moz-cite-prefix">On 4/26/2016 7:57 PM, Siegmar Gross
      wrote:<br>
    </div>
    <blockquote
      cite="mid:ab089872-b470-2574-f580-d2df4a80348a@informatik.hs-fulda.de"
      type="cite">Hi Gilles and Ralph,
      <br>
      <br>
      I was able to sort out my mess. In my last email I compared the
      <br>
      files from "SunOS_sparc/openmpi-2.0.0_64_gcc/lib64/openmpi" from
      <br>
      the attachment of my email to Ralph with the files from
      <br>
      "SunOS_sparc/openmpi-2.0.0_64_cc/lib64/openmpi" from my current
      <br>
      file system. That's the reason while I have had different
      <br>
      timestamps. The other problem was that Ralph didn't recognize
      <br>
      that "mca_pmix_pmix112.so" wasn't built on Solaris with the
      <br>
      Sun C compiler. I've removed most of the files from the attachment
      <br>
      of my email so that it is easier to see the relevant files. Below
      <br>
      I try to give you more information that may be relevant to track
      <br>
      down the problem. I still get an error running one of my small
      <br>
      test programs, when I use my gcc-version of Open MPI.
      <br>
      "mca_pmix_pmix112.so" is a 64 bits library.
      <br>
      <br>
      Linux_x86_64/openmpi-2.0.0_64_cc/lib64/openmpi:
      <br>
      ...
      <br>
      -rwxr-xr-x 1 root root  261327 Apr 19 16:46 mca_plm_slurm.so
      <br>
      -rwxr-xr-x 1 root root    1002 Apr 19 16:45 mca_pmix_pmix112.la
      <br>
      -rwxr-xr-x 1 root root 3906526 Apr 19 16:45 mca_pmix_pmix112.so
      <br>
      -rwxr-xr-x 1 root root     966 Apr 19 16:51 mca_pml_cm.la
      <br>
      -rwxr-xr-x 1 root root 1574265 Apr 19 16:51 mca_pml_cm.so
      <br>
      ...
      <br>
      <br>
      Linux_x86_64/openmpi-2.0.0_64_gcc/lib64/openmpi:
      <br>
      ...
      <br>
      -rwxr-xr-x 1 root root   70371 Apr 19 16:43 mca_plm_slurm.so
      <br>
      -rwxr-xr-x 1 root root    1008 Apr 19 16:42 mca_pmix_pmix112.la
      <br>
      -rwxr-xr-x 1 root root 1029005 Apr 19 16:42 mca_pmix_pmix112.so
      <br>
      -rwxr-xr-x 1 root root     972 Apr 19 16:46 mca_pml_cm.la
      <br>
      -rwxr-xr-x 1 root root  284858 Apr 19 16:46 mca_pml_cm.so
      <br>
      ...
      <br>
      <br>
      SunOS_sparc/openmpi-2.0.0_64_cc/lib64/openmpi:
      <br>
      ...
      <br>
      -rwxr-xr-x 1 root root  319816 Apr 19 19:58 mca_plm_rsh.so
      <br>
      -rwxr-xr-x 1 root root     970 Apr 19 20:00 mca_pml_cm.la
      <br>
      -rwxr-xr-x 1 root root 1507440 Apr 19 20:00 mca_pml_cm.so
      <br>
      ...
      <br>
      <br>
      SunOS_sparc/openmpi-2.0.0_64_gcc/lib64/openmpi:
      <br>
      ...
      <br>
      -rwxr-xr-x 1 root root  153280 Apr 19 19:49 mca_plm_rsh.so
      <br>
      -rwxr-xr-x 1 root root    1007 Apr 19 19:47 mca_pmix_pmix112.la
      <br>
      -rwxr-xr-x 1 root root 1400512 Apr 19 19:47 mca_pmix_pmix112.so
      <br>
      -rwxr-xr-x 1 root root     971 Apr 19 19:52 mca_pml_cm.la
      <br>
      -rwxr-xr-x 1 root root  342440 Apr 19 19:52 mca_pml_cm.so
      <br>
      ...
      <br>
      <br>
      SunOS_x86_64/openmpi-2.0.0_64_cc/lib64/openmpi:
      <br>
      ...
      <br>
      -rwxr-xr-x 1 root root  300096 Apr 19 17:18 mca_plm_rsh.so
      <br>
      -rwxr-xr-x 1 root root     970 Apr 19 17:23 mca_pml_cm.la
      <br>
      -rwxr-xr-x 1 root root 1458816 Apr 19 17:23 mca_pml_cm.so
      <br>
      ...
      <br>
      <br>
      SunOS_x86_64/openmpi-2.0.0_64_gcc/lib64/openmpi:
      <br>
      ...
      <br>
      -rwxr-xr-x 1 root root  133096 Apr 19 17:42 mca_plm_rsh.so
      <br>
      -rwxr-xr-x 1 root root    1007 Apr 19 17:41 mca_pmix_pmix112.la
      <br>
      -rwxr-xr-x 1 root root 1320240 Apr 19 17:41 mca_pmix_pmix112.so
      <br>
      -rwxr-xr-x 1 root root     971 Apr 19 17:46 mca_pml_cm.la
      <br>
      -rwxr-xr-x 1 root root  419848 Apr 19 17:46 mca_pml_cm.so
      <br>
      ...
      <br>
      <br>
      <br>
      Yesterday I've installed openmpi-v2.x-dev-1290-gbd0e4e1 so that we
      <br>
      have a current version for the investigation of the problem. Once
      <br>
      more mca_pmix_pmix112.so isn't available on Solaris if I use the
      <br>
      Sun C compiler.
      <br>
      <br>
      "config.log" for gcc-5.1.0 shows the following.
      <br>
      <br>
      ...
      <br>
      configure:127799: /bin/bash
      '../../../../../../openmpi-v2.x-dev-1290-gbd0e4e1/opal/mca/pmix/pmix112/
      <br>
      pmix/configure' succeeded for opal/mca/pmix/pmix112/pmix
      <br>
      configure:127916: checking if MCA component pmix:pmix112 can
      compile
      <br>
      configure:127918: result: yes
      <br>
      configure:5637: --- MCA component pmix:external (m4 configuration
      macro)
      <br>
      configure:128523: checking for MCA component pmix:external compile
      mode
      <br>
      configure:128529: result: dso
      <br>
      configure:129054: checking if MCA component pmix:external can
      compile
      <br>
      configure:129056: result: no
      <br>
      ...
      <br>
      config.status:3897: creating opal/mca/pmix/Makefile
      <br>
      config.status:3897: creating opal/mca/pmix/s1/Makefile
      <br>
      config.status:3897: creating opal/mca/pmix/cray/Makefile
      <br>
      config.status:3897: creating opal/mca/pmix/s2/Makefile
      <br>
      config.status:3897: creating opal/mca/pmix/pmix112/Makefile
      <br>
      config.status:3897: creating opal/mca/pmix/external/Makefile
      <br>
      ...
      <br>
      MCA_BUILD_opal_pmix_cray_DSO_FALSE='#'
      <br>
      MCA_BUILD_opal_pmix_cray_DSO_TRUE=''
      <br>
      MCA_BUILD_opal_pmix_external_DSO_FALSE='#'
      <br>
      MCA_BUILD_opal_pmix_external_DSO_TRUE=''
      <br>
      MCA_BUILD_opal_pmix_pmix112_DSO_FALSE='#'
      <br>
      MCA_BUILD_opal_pmix_pmix112_DSO_TRUE=''
      <br>
      MCA_BUILD_opal_pmix_s1_DSO_FALSE='#'
      <br>
      MCA_BUILD_opal_pmix_s1_DSO_TRUE=''
      <br>
      MCA_BUILD_opal_pmix_s2_DSO_FALSE='#'
      <br>
      MCA_BUILD_opal_pmix_s2_DSO_TRUE=''
      <br>
      ...
      <br>
      MCA_opal_FRAMEWORKS='common  allocator backtrace btl dl event
      hwloc if installdirs memchecker memcpy memory mpool pmix pstat
      rcache sec shmem timer'
      <br>
      MCA_opal_FRAMEWORKS_SUBDIRS='mca/common  mca/allocator
      mca/backtrace mca/btl mca/dl mca/event mca/hwloc mca/if
      mca/installdirs mca/memchecker mca/memcpy mca/memory mca/mpool
      mca/pmix mca/pstat mca/rcache mca/sec mca/shmem mca/timer'
      <br>
MCA_opal_FRAMEWORK_COMPONENT_ALL_SUBDIRS='$(MCA_opal_common_ALL_SUBDIRS)
      $(MCA_opal_allocator_ALL_SUBDIRS)
      $(MCA_opal_backtrace_ALL_SUBDIRS) $(MCA_opal_btl_ALL_SUBDIRS)
      $(MCA_opal_dl_ALL_SUBDIRS) $(MCA_opal_event_ALL_SUBDIRS)
      $(MCA_opal_hwloc_ALL_SUBDIRS) $(MCA_opal_if_ALL_SUBDIRS)
      $(MCA_opal_installdirs_ALL_SUBDIRS)
      $(MCA_opal_memchecker_ALL_SUBDIRS) $(MCA_opal_memcpy_ALL_SUBDIRS)
      $(MCA_opal_memory_ALL_SUBDIRS) $(MCA_opal_mpool_ALL_SUBDIRS)
      $(MCA_opal_pmix_ALL_SUBDIRS) $(MCA_opal_pstat_ALL_SUBDIRS)
      $(MCA_opal_rcache_ALL_SUBDIRS) $(MCA_opal_sec_ALL_SUBDIRS)
      $(MCA_opal_shmem_ALL_SUBDIRS) $(MCA_opal_timer_ALL_SUBDIRS)'
      <br>
MCA_opal_FRAMEWORK_COMPONENT_DSO_SUBDIRS='$(MCA_opal_common_DSO_SUBDIRS)
      $(MCA_opal_allocator_DSO_SUBDIRS)
      $(MCA_opal_backtrace_DSO_SUBDIRS) $(MCA_opal_btl_DSO_SUBDIRS)
      $(MCA_opal_dl_DSO_SUBDIRS) $(MCA_opal_event_DSO_SUBDIRS)
      $(MCA_opal_hwloc_DSO_SUBDIRS) $(MCA_opal_if_DSO_SUBDIRS)
      $(MCA_opal_installdirs_DSO_SUBDIRS)
      $(MCA_opal_memchecker_DSO_SUBDIRS) $(MCA_opal_memcpy_DSO_SUBDIRS)
      $(MCA_opal_memory_DSO_SUBDIRS) $(MCA_opal_mpool_DSO_SUBDIRS)
      $(MCA_opal_pmix_DSO_SUBDIRS) $(MCA_opal_pstat_DSO_SUBDIRS)
      $(MCA_opal_rcache_DSO_SUBDIRS) $(MCA_opal_sec_DSO_SUBDIRS)
      $(MCA_opal_shmem_DSO_SUBDIRS) $(MCA_opal_timer_DSO_SUBDIRS)'
      <br>
MCA_opal_FRAMEWORK_COMPONENT_STATIC_SUBDIRS='$(MCA_opal_common_STATIC_SUBDIRS)
       $(MCA_opal_allocator_STATIC_SUBDIRS)
      $(MCA_opal_backtrace_STATIC_SUBDIRS)
      $(MCA_opal_btl_STATIC_SUBDIRS) $(MCA_opal_dl_STATIC_SUBDIRS)
      $(MCA_opal_event_STATIC_SUBDIRS) $(MCA_opal_hwloc_STATIC_SUBDIRS)
      $(MCA_opal_if_STATIC_SUBDIRS)
      $(MCA_opal_installdirs_STATIC_SUBDIRS)
      $(MCA_opal_memchecker_STATIC_SUBDIRS)
      $(MCA_opal_memcpy_STATIC_SUBDIRS)
      $(MCA_opal_memory_STATIC_SUBDIRS) $(MCA_opal_mpool_STATIC_SUBDIRS)
      $(MCA_opal_pmix_STATIC_SUBDIRS) $(MCA_opal_pstat_STATIC_SUBDIRS)
      $(MCA_opal_rcache_STATIC_SUBDIRS) $(MCA_opal_sec_STATIC_SUBDIRS)
      $(MCA_opal_shmem_STATIC_SUBDIRS) $(MCA_opal_timer_STATIC_SUBDIRS)'
      <br>
      MCA_opal_FRAMEWORK_LIBS=' $(MCA_opal_common_STATIC_LTLIBS)
      mca/allocator/libmca_allocator.la
      $(MCA_opal_allocator_STATIC_LTLIBS)
      mca/backtrace/libmca_backtrace.la
      $(MCA_opal_backtrace_STATIC_LTLIBS) mca/btl/libmca_btl.la
      $(MCA_opal_btl_STATIC_LTLIBS) mca/dl/libmca_dl.la
      $(MCA_opal_dl_STATIC_LTLIBS) mca/event/libmca_event.la
      $(MCA_opal_event_STATIC_LTLIBS) mca/hwloc/libmca_hwloc.la
      $(MCA_opal_hwloc_STATIC_LTLIBS) mca/if/libmca_if.la
      $(MCA_opal_if_STATIC_LTLIBS) mca/installdirs/libmca_installdirs.la
      $(MCA_opal_installdirs_STATIC_LTLIBS)
      mca/memchecker/libmca_memchecker.la
      $(MCA_opal_memchecker_STATIC_LTLIBS) mca/memcpy/libmca_memcpy.la
      $(MCA_opal_memcpy_STATIC_LTLIBS) mca/memory/libmca_memory.la
      $(MCA_opal_memory_STATIC_LTLIBS) mca/mpool/libmca_mpool.la
      $(MCA_opal_mpool_STATIC_LTLIBS) mca/pmix/libmca_pmix.la
      $(MCA_opal_pmix_STATIC_LTLIBS) mca/pstat/libmca_pstat.la
      $(MCA_opal_pstat_STATIC_LTLIBS) mca/rcache/libmca_rcache.la
      $(MCA_opal_rcache_STATIC_LTLIBS) mca/sec/libmca_sec.la
      $(MCA_opal_sec_STATIC_LTLIBS) mca/shmem/libmca_shmem.la
      $(MCA_opal_shmem_STATIC_LTLIBS) mca/timer/libmca_timer.la
      $(MCA_opal_timer_STATIC_LTLIBS)'
      <br>
      ...
      <br>
      MCA_opal_pmix_ALL_COMPONENTS=' s1 cray s2 pmix112 external'
      <br>
      MCA_opal_pmix_ALL_SUBDIRS=' mca/pmix/s1 mca/pmix/cray mca/pmix/s2
      mca/pmix/pmix112 mca/pmix/external'
      <br>
      MCA_opal_pmix_DSO_COMPONENTS=' pmix112'
      <br>
      MCA_opal_pmix_DSO_SUBDIRS=' mca/pmix/pmix112'
      <br>
      MCA_opal_pmix_STATIC_COMPONENTS=''
      <br>
      MCA_opal_pmix_STATIC_LTLIBS=''
      <br>
      MCA_opal_pmix_STATIC_SUBDIRS=''
      <br>
      ...
      <br>
      opal_pmix_ext_CPPFLAGS=''
      <br>
      opal_pmix_ext_LDFLAGS=''
      <br>
      opal_pmix_ext_LIBS=''
      <br>
opal_pmix_pmix112_CPPFLAGS='-I$(OPAL_TOP_BUILDDIR)/opal/mca/pmix/pmix112/pmix/include/pmix
      -I$(OPAL_TOP_BUILDDIR)/opal/mca/pmix/pmix112/pmix/include
      -I$(OPAL_TOP_BUILDDIR)/opal/mca/pmix/pmix112/pmix
      -I$(OPAL_TOP_SRCDIR)/opal/mca/pmix/pmix112/pmix'
      <br>
opal_pmix_pmix112_LIBS='$(OPAL_TOP_BUILDDIR)/opal/mca/pmix/pmix112/pmix/libpmix.la'
      <br>
      ...
      <br>
      <br>
      <br>
      <br>
      "config.log" for Sun C 5.13 shows the following.
      <br>
      <br>
      ...
      <br>
      configure:127803: /bin/bash
      '../../../../../../openmpi-v2.x-dev-1290-gbd0e4e1/opal/mca/pmix/pmix112/
      <br>
      pmix/configure' *failed* for opal/mca/pmix/pmix112/pmix
      <br>
      configure:128379: checking if MCA component pmix:pmix112 can
      compile
      <br>
      configure:128381: result: no
      <br>
      configure:5637: --- MCA component pmix:external (m4 configuration
      macro)
      <br>
      configure:128523: checking for MCA component pmix:external compile
      mode
      <br>
      configure:128529: result: dso
      <br>
      configure:129054: checking if MCA component pmix:external can
      compile
      <br>
      configure:129056: result: no
      <br>
      ...
      <br>
      config.status:3887: creating opal/mca/pmix/Makefile
      <br>
      config.status:3887: creating opal/mca/pmix/s1/Makefile
      <br>
      config.status:3887: creating opal/mca/pmix/cray/Makefile
      <br>
      config.status:3887: creating opal/mca/pmix/s2/Makefile
      <br>
      config.status:3887: creating opal/mca/pmix/pmix112/Makefile
      <br>
      config.status:3887: creating opal/mca/pmix/external/Makefile
      <br>
      ...
      <br>
      MCA_BUILD_opal_pmix_cray_DSO_FALSE='#'
      <br>
      MCA_BUILD_opal_pmix_cray_DSO_TRUE=''
      <br>
      MCA_BUILD_opal_pmix_external_DSO_FALSE='#'
      <br>
      MCA_BUILD_opal_pmix_external_DSO_TRUE=''
      <br>
      MCA_BUILD_opal_pmix_pmix112_DSO_FALSE='#'
      <br>
      MCA_BUILD_opal_pmix_pmix112_DSO_TRUE=''
      <br>
      MCA_BUILD_opal_pmix_s1_DSO_FALSE='#'
      <br>
      MCA_BUILD_opal_pmix_s1_DSO_TRUE=''
      <br>
      MCA_BUILD_opal_pmix_s2_DSO_FALSE='#'
      <br>
      MCA_BUILD_opal_pmix_s2_DSO_TRUE=''
      <br>
      ...
      <br>
      MCA_opal_FRAMEWORKS='common  allocator backtrace btl dl event
      hwloc if installdirs memchecker memcpy memory mpool pmix pstat
      rcache sec shmem timer'
      <br>
      MCA_opal_FRAMEWORKS_SUBDIRS='mca/common  mca/allocator
      mca/backtrace mca/btl mca/dl mca/event mca/hwloc mca/if
      mca/installdirs mca/memchecker mca/memcpy mca/memory mca/mpool
      mca/pmix mca/pstat mca/rcache mca/sec mca/shmem mca/timer'
      <br>
MCA_opal_FRAMEWORK_COMPONENT_ALL_SUBDIRS='$(MCA_opal_common_ALL_SUBDIRS)
      $(MCA_opal_allocator_ALL_SUBDIRS)
      $(MCA_opal_backtrace_ALL_SUBDIRS) $(MCA_opal_btl_ALL_SUBDIRS)
      $(MCA_opal_dl_ALL_SUBDIRS) $(MCA_opal_event_ALL_SUBDIRS)
      $(MCA_opal_hwloc_ALL_SUBDIRS) $(MCA_opal_if_ALL_SUBDIRS)
      $(MCA_opal_installdirs_ALL_SUBDIRS)
      $(MCA_opal_memchecker_ALL_SUBDIRS) $(MCA_opal_memcpy_ALL_SUBDIRS)
      $(MCA_opal_memory_ALL_SUBDIRS) $(MCA_opal_mpool_ALL_SUBDIRS)
      $(MCA_opal_pmix_ALL_SUBDIRS) $(MCA_opal_pstat_ALL_SUBDIRS)
      $(MCA_opal_rcache_ALL_SUBDIRS) $(MCA_opal_sec_ALL_SUBDIRS)
      $(MCA_opal_shmem_ALL_SUBDIRS) $(MCA_opal_timer_ALL_SUBDIRS)'
      <br>
MCA_opal_FRAMEWORK_COMPONENT_DSO_SUBDIRS='$(MCA_opal_common_DSO_SUBDIRS)
      $(MCA_opal_allocator_DSO_SUBDIRS)
      $(MCA_opal_backtrace_DSO_SUBDIRS) $(MCA_opal_btl_DSO_SUBDIRS)
      $(MCA_opal_dl_DSO_SUBDIRS) $(MCA_opal_event_DSO_SUBDIRS)
      $(MCA_opal_hwloc_DSO_SUBDIRS) $(MCA_opal_if_DSO_SUBDIRS)
      $(MCA_opal_installdirs_DSO_SUBDIRS)
      $(MCA_opal_memchecker_DSO_SUBDIRS) $(MCA_opal_memcpy_DSO_SUBDIRS)
      $(MCA_opal_memory_DSO_SUBDIRS) $(MCA_opal_mpool_DSO_SUBDIRS)
      $(MCA_opal_pmix_DSO_SUBDIRS) $(MCA_opal_pstat_DSO_SUBDIRS)
      $(MCA_opal_rcache_DSO_SUBDIRS) $(MCA_opal_sec_DSO_SUBDIRS)
      $(MCA_opal_shmem_DSO_SUBDIRS) $(MCA_opal_timer_DSO_SUBDIRS)'
      <br>
MCA_opal_FRAMEWORK_COMPONENT_STATIC_SUBDIRS='$(MCA_opal_common_STATIC_SUBDIRS)
       $(MCA_opal_allocator_STATIC_SUBDIRS)
      $(MCA_opal_backtrace_STATIC_SUBDIRS)
      $(MCA_opal_btl_STATIC_SUBDIRS) $(MCA_opal_dl_STATIC_SUBDIRS)
      $(MCA_opal_event_STATIC_SUBDIRS) $(MCA_opal_hwloc_STATIC_SUBDIRS)
      $(MCA_opal_if_STATIC_SUBDIRS)
      $(MCA_opal_installdirs_STATIC_SUBDIRS)
      $(MCA_opal_memchecker_STATIC_SUBDIRS)
      $(MCA_opal_memcpy_STATIC_SUBDIRS)
      $(MCA_opal_memory_STATIC_SUBDIRS) $(MCA_opal_mpool_STATIC_SUBDIRS)
      $(MCA_opal_pmix_STATIC_SUBDIRS) $(MCA_opal_pstat_STATIC_SUBDIRS)
      $(MCA_opal_rcache_STATIC_SUBDIRS) $(MCA_opal_sec_STATIC_SUBDIRS)
      $(MCA_opal_shmem_STATIC_SUBDIRS) $(MCA_opal_timer_STATIC_SUBDIRS)'
      <br>
      MCA_opal_FRAMEWORK_LIBS=' $(MCA_opal_common_STATIC_LTLIBS)
      mca/allocator/libmca_allocator.la
      $(MCA_opal_allocator_STATIC_LTLIBS)
      mca/backtrace/libmca_backtrace.la
      $(MCA_opal_backtrace_STATIC_LTLIBS) mca/btl/libmca_btl.la
      $(MCA_opal_btl_STATIC_LTLIBS) mca/dl/libmca_dl.la
      $(MCA_opal_dl_STATIC_LTLIBS) mca/event/libmca_event.la
      $(MCA_opal_event_STATIC_LTLIBS) mca/hwloc/libmca_hwloc.la
      $(MCA_opal_hwloc_STATIC_LTLIBS) mca/if/libmca_if.la
      $(MCA_opal_if_STATIC_LTLIBS) mca/installdirs/libmca_installdirs.la
      $(MCA_opal_installdirs_STATIC_LTLIBS)
      mca/memchecker/libmca_memchecker.la
      $(MCA_opal_memchecker_STATIC_LTLIBS) mca/memcpy/libmca_memcpy.la
      $(MCA_opal_memcpy_STATIC_LTLIBS) mca/memory/libmca_memory.la
      $(MCA_opal_memory_STATIC_LTLIBS) mca/mpool/libmca_mpool.la
      $(MCA_opal_mpool_STATIC_LTLIBS) mca/pmix/libmca_pmix.la
      $(MCA_opal_pmix_STATIC_LTLIBS) mca/pstat/libmca_pstat.la
      $(MCA_opal_pstat_STATIC_LTLIBS) mca/rcache/libmca_rcache.la
      $(MCA_opal_rcache_STATIC_LTLIBS) mca/sec/libmca_sec.la
      $(MCA_opal_sec_STATIC_LTLIBS) mca/shmem/libmca_shmem.la
      $(MCA_opal_shmem_STATIC_LTLIBS) mca/timer/libmca_timer.la
      $(MCA_opal_timer_STATIC_LTLIBS)'
      <br>
      ...
      <br>
      MCA_opal_pmix_ALL_COMPONENTS=' s1 cray s2 pmix112 external'
      <br>
      MCA_opal_pmix_ALL_SUBDIRS=' mca/pmix/s1 mca/pmix/cray mca/pmix/s2
      mca/pmix/pmix112 mca/pmix/external'
      <br>
      MCA_opal_pmix_DSO_COMPONENTS=''
      <br>
      MCA_opal_pmix_DSO_SUBDIRS=''
      <br>
      MCA_opal_pmix_STATIC_COMPONENTS=''
      <br>
      MCA_opal_pmix_STATIC_LTLIBS=''
      <br>
      MCA_opal_pmix_STATIC_SUBDIRS=''
      <br>
      ...
      <br>
      opal_pmix_ext_CPPFLAGS=''
      <br>
      opal_pmix_ext_LDFLAGS=''
      <br>
      opal_pmix_ext_LIBS=''
      <br>
      opal_pmix_pmix112_CPPFLAGS=''
      <br>
      opal_pmix_pmix112_LIBS=''
      <br>
      ...
      <br>
      <br>
      <br>
      <br>
      <br>
      I've attached the config.log files for pmix.
      <br>
      <br>
      tyr openmpi-2.0.0 142 tar zvft pmix_config.log.tar.gz
      <br>
      -rw-r--r-- root/root    136291 2016-04-25 08:05:34
openmpi-v2.x-dev-1290-gbd0e4e1-SunOS.sparc.64_cc/opal/mca/pmix/pmix112/pmix/config.log<br>
      -rw-r--r-- root/root    528808 2016-04-25 08:07:54
openmpi-v2.x-dev-1290-gbd0e4e1-SunOS.sparc.64_gcc/opal/mca/pmix/pmix112/pmix/config.log<br>
      tyr openmpi-2.0.0 143
      <br>
      <br>
      <br>
      <br>
      I've also attached the output for the broken execution of
      <br>
      "spawn_multiple_master" for my gcc-version of Open MPI.
      <br>
      "spawn_master" works as expected with my gcc-version of Open MPI.
      <br>
      <br>
      Hopefully you can fix the problem.
      <br>
      <br>
      <br>
      Kind regards and thank you very much for your help
      <br>
      <br>
      Siegmar
      <br>
      <br>
      <br>
      <br>
      Am 23.04.2016 um 21:34 schrieb Siegmar Gross:
      <br>
      <blockquote type="cite">Hi Gilles,
        <br>
        <br>
        I don't know what happened, but the files are not available now
        <br>
        and they were definitely available when I answered the email
        from
        <br>
        Ralph. The files also have a different timestamp now. This is an
        <br>
        extract from my email to Ralph for Solaris Sparc.
        <br>
        <br>
        -rwxr-xr-x 1 root root     977 Apr 19 19:49 mca_plm_rsh.la
        <br>
        -rwxr-xr-x 1 root root  153280 Apr 19 19:49 mca_plm_rsh.so
        <br>
        -rwxr-xr-x 1 root root    1007 Apr 19 19:47 mca_pmix_pmix112.la
        <br>
        -rwxr-xr-x 1 root root 1400512 Apr 19 19:47 mca_pmix_pmix112.so
        <br>
        -rwxr-xr-x 1 root root     971 Apr 19 19:52 mca_pml_cm.la
        <br>
        -rwxr-xr-x 1 root root  342440 Apr 19 19:52 mca_pml_cm.so
        <br>
        <br>
        Now I have the following output for these files.
        <br>
        <br>
        -rwxr-xr-x 1 root root     976 Apr 19 19:58 mca_plm_rsh.la
        <br>
        -rwxr-xr-x 1 root root  319816 Apr 19 19:58 mca_plm_rsh.so
        <br>
        -rwxr-xr-x 1 root root     970 Apr 19 20:00 mca_pml_cm.la
        <br>
        -rwxr-xr-x 1 root root 1507440 Apr 19 20:00 mca_pml_cm.so
        <br>
        <br>
        I'll try to find out what happened next week when I'm back in
        <br>
        my office.
        <br>
        <br>
        <br>
        Kind regards
        <br>
        <br>
        Siegmar
        <br>
        <br>
        <br>
        <br>
        <br>
        <br>
        Am 23.04.16 um 02:12 schrieb Gilles Gouaillardet:
        <br>
        <blockquote type="cite">Siegmar,
          <br>
          <br>
          I will try to reproduce this on my solaris11 x86_64 vm
          <br>
          <br>
          In the mean time, can you please double check
          mca_pmix_pmix_pmix112.so
          <br>
          is a 64 bits library ?
          <br>
          (E.g, confirm "-m64" was correctly passed to pmix)
          <br>
          <br>
          Cheers,
          <br>
          <br>
          Gilles
          <br>
          <br>
          On Friday, April 22, 2016, Siegmar Gross
          <br>
          &lt;<a class="moz-txt-link-abbreviated" href="mailto:siegmar.gross@informatik.hs-fulda.de">siegmar.gross@informatik.hs-fulda.de</a>
          <br>
          <a class="moz-txt-link-rfc2396E" href="mailto:siegmar.gross@informatik.hs-fulda.de">&lt;mailto:siegmar.gross@informatik.hs-fulda.de&gt;</a>&gt; wrote:
          <br>
          <br>
              Hi Ralph,
          <br>
          <br>
              I've already used "-enable-debug". "SYSTEM_ENV" is "SunOS"
          or
          <br>
              "Linux" and "MACHINE_ENV" is "sparc" or "x86_84".
          <br>
          <br>
              mkdir
          openmpi-v2.x-dev-1280-gc110ae8-${SYSTEM_ENV}.${MACHINE_ENV}.64_gcc
          <br>
              cd
          openmpi-v2.x-dev-1280-gc110ae8-${SYSTEM_ENV}.${MACHINE_ENV}.64_gcc
          <br>
          <br>
              ../openmpi-v2.x-dev-1280-gc110ae8/configure \
          <br>
                --prefix=/usr/local/openmpi-2.0.0_64_gcc \
          <br>
                --libdir=/usr/local/openmpi-2.0.0_64_gcc/lib64 \
          <br>
                --with-jdk-bindir=/usr/local/jdk1.8.0/bin \
          <br>
                --with-jdk-headers=/usr/local/jdk1.8.0/include \
          <br>
                JAVA_HOME=/usr/local/jdk1.8.0 \
          <br>
                LDFLAGS="-m64" CC="gcc" CXX="g++" FC="gfortran" \
          <br>
                CFLAGS="-m64" CXXFLAGS="-m64" FCFLAGS="-m64" \
          <br>
                CPP="cpp" CXXCPP="cpp" \
          <br>
                --enable-mpi-cxx \
          <br>
                --enable-cxx-exceptions \
          <br>
                --enable-mpi-java \
          <br>
                --enable-heterogeneous \
          <br>
                --enable-mpi-thread-multiple \
          <br>
                --with-hwloc=internal \
          <br>
                --without-verbs \
          <br>
                --with-wrapper-cflags="-std=c11 -m64" \
          <br>
                --with-wrapper-cxxflags="-m64" \
          <br>
                --with-wrapper-fcflags="-m64" \
          <br>
                --enable-debug \
          <br>
                |&amp; tee log.configure.$SYSTEM_ENV.$MACHINE_ENV.64_gcc
          <br>
          <br>
          <br>
              mkdir
          openmpi-v2.x-dev-1280-gc110ae8-${SYSTEM_ENV}.${MACHINE_ENV}.64_cc
          <br>
              cd
          openmpi-v2.x-dev-1280-gc110ae8-${SYSTEM_ENV}.${MACHINE_ENV}.64_cc
          <br>
          <br>
              ../openmpi-v2.x-dev-1280-gc110ae8/configure \
          <br>
                --prefix=/usr/local/openmpi-2.0.0_64_cc \
          <br>
                --libdir=/usr/local/openmpi-2.0.0_64_cc/lib64 \
          <br>
                --with-jdk-bindir=/usr/local/jdk1.8.0/bin \
          <br>
                --with-jdk-headers=/usr/local/jdk1.8.0/include \
          <br>
                JAVA_HOME=/usr/local/jdk1.8.0 \
          <br>
                LDFLAGS="-m64" CC="cc" CXX="CC" FC="f95" \
          <br>
                CFLAGS="-m64" CXXFLAGS="-m64 -library=stlport4"
          FCFLAGS="-m64" \
          <br>
                CPP="cpp" CXXCPP="cpp" \
          <br>
                --enable-mpi-cxx \
          <br>
                --enable-cxx-exceptions \
          <br>
                --enable-mpi-java \
          <br>
                --enable-heterogeneous \
          <br>
                --enable-mpi-thread-multiple \
          <br>
                --with-hwloc=internal \
          <br>
                --without-verbs \
          <br>
                --with-wrapper-cflags="-m64" \
          <br>
                --with-wrapper-cxxflags="-m64 -library=stlport4" \
          <br>
                --with-wrapper-fcflags="-m64" \
          <br>
                --with-wrapper-ldflags="" \
          <br>
                --enable-debug \
          <br>
                |&amp; tee log.configure.$SYSTEM_ENV.$MACHINE_ENV.64_cc
          <br>
          <br>
          <br>
              Kind regards
          <br>
          <br>
              Siegmar
          <br>
          <br>
              Am 21.04.2016 um 18:18 schrieb Ralph Castain:
          <br>
          <br>
                  Can you please rebuild OMPI with -enable-debug in the
          configure
          <br>
                  cmd? It will let us see more error output
          <br>
          <br>
          <br>
                      On Apr 21, 2016, at 8:52 AM, Siegmar Gross
          <br>
                      <a class="moz-txt-link-rfc2396E" href="mailto:siegmar.gross@informatik.hs-fulda.de">&lt;siegmar.gross@informatik.hs-fulda.de&gt;</a>
          wrote:
          <br>
          <br>
                      Hi Ralph,
          <br>
          <br>
                      I don't see any additional information.
          <br>
          <br>
                      tyr hello_1 108 mpiexec -np 4 --host
          <br>
                      tyr,sunpc1,linpc1,ruester -mca
          <br>
                      mca_base_component_show_load_errors 1 hello_1_mpi
          <br>
                      [tyr.informatik.hs-fulda.de:06211
          <br>
                      <a class="moz-txt-link-rfc2396E" href="http://tyr.informatik.hs-fulda.de:06211">&lt;http://tyr.informatik.hs-fulda.de:06211&gt;</a>]
          [[48741,0],0]
          <br>
                      ORTE_ERROR_LOG: Not found in file
          <br>
          <br>
../../../../../openmpi-v2.x-dev-1280-gc110ae8/orte/mca/ess/hnp/ess_hnp_module.c
          <br>
                      at line 638
          <br>
          <br>
--------------------------------------------------------------------------
          <br>
                      It looks like orte_init failed for some reason;
          your
          <br>
                      parallel process is
          <br>
                      likely to abort.  There are many reasons that a
          parallel
          <br>
                      process can
          <br>
                      fail during orte_init; some of which are due to
          configuration or
          <br>
                      environment problems.  This failure appears to be
          an
          <br>
                      internal failure;
          <br>
                      here's some additional information (which may only
          be
          <br>
                      relevant to an
          <br>
                      Open MPI developer):
          <br>
          <br>
                       opal_pmix_base_select failed
          <br>
                       --&gt; Returned value Not found (-13) instead of
          ORTE_SUCCESS
          <br>
          <br>
--------------------------------------------------------------------------
          <br>
          <br>
          <br>
                      tyr hello_1 109 mpiexec -np 4 --host
          <br>
                      tyr,sunpc1,linpc1,ruester -mca
          <br>
                      mca_base_component_show_load_errors 1 -mca
          pmix_base_verbose
          <br>
                      10 -mca pmix_server_verbose 5 hello_1_mpi
          <br>
                      [tyr.informatik.hs-fulda.de:06212
          <br>
                      <a class="moz-txt-link-rfc2396E" href="http://tyr.informatik.hs-fulda.de:06212">&lt;http://tyr.informatik.hs-fulda.de:06212&gt;</a>]
          mca: base:
          <br>
                      components_register: registering framework pmix
          components
          <br>
                      [tyr.informatik.hs-fulda.de:06212
          <br>
                      <a class="moz-txt-link-rfc2396E" href="http://tyr.informatik.hs-fulda.de:06212">&lt;http://tyr.informatik.hs-fulda.de:06212&gt;</a>]
          mca: base:
          <br>
                      components_open: opening pmix components
          <br>
                      [tyr.informatik.hs-fulda.de:06212
          <br>
                      <a class="moz-txt-link-rfc2396E" href="http://tyr.informatik.hs-fulda.de:06212">&lt;http://tyr.informatik.hs-fulda.de:06212&gt;</a>]
          mca:base:select:
          <br>
                      Auto-selecting pmix components
          <br>
                      [tyr.informatik.hs-fulda.de:06212
          <br>
                      <a class="moz-txt-link-rfc2396E" href="http://tyr.informatik.hs-fulda.de:06212">&lt;http://tyr.informatik.hs-fulda.de:06212&gt;</a>]
          mca:base:select:(
          <br>
                      pmix) No component selected!
          <br>
                      [tyr.informatik.hs-fulda.de:06212
          <br>
                      <a class="moz-txt-link-rfc2396E" href="http://tyr.informatik.hs-fulda.de:06212">&lt;http://tyr.informatik.hs-fulda.de:06212&gt;</a>]
          [[48738,0],0]
          <br>
                      ORTE_ERROR_LOG: Not found in file
          <br>
          <br>
../../../../../openmpi-v2.x-dev-1280-gc110ae8/orte/mca/ess/hnp/ess_hnp_module.c
          <br>
                      at line 638
          <br>
          <br>
--------------------------------------------------------------------------
          <br>
                      It looks like orte_init failed for some reason;
          your
          <br>
                      parallel process is
          <br>
                      likely to abort.  There are many reasons that a
          parallel
          <br>
                      process can
          <br>
                      fail during orte_init; some of which are due to
          configuration or
          <br>
                      environment problems.  This failure appears to be
          an
          <br>
                      internal failure;
          <br>
                      here's some additional information (which may only
          be
          <br>
                      relevant to an
          <br>
                      Open MPI developer):
          <br>
          <br>
                       opal_pmix_base_select failed
          <br>
                       --&gt; Returned value Not found (-13) instead of
          ORTE_SUCCESS
          <br>
          <br>
--------------------------------------------------------------------------
          <br>
                      tyr hello_1 110
          <br>
          <br>
          <br>
                      Kind regards
          <br>
          <br>
                      Siegmar
          <br>
          <br>
          <br>
                      Am 21.04.2016 um 17:24 schrieb Ralph Castain:
          <br>
          <br>
                          Hmmm…it looks like you built the right
          components, but
          <br>
                          they are not being picked up. Can you run your
          mpiexec
          <br>
                          command again, adding “-mca
          <br>
                          mca_base_component_show_load_errors 1” to the
          cmd line?
          <br>
          <br>
          <br>
                              On Apr 21, 2016, at 8:16 AM, Siegmar Gross
          <br>
                             
          <a class="moz-txt-link-rfc2396E" href="mailto:siegmar.gross@informatik.hs-fulda.de">&lt;siegmar.gross@informatik.hs-fulda.de&gt;</a> wrote:
          <br>
          <br>
                              Hi Ralph,
          <br>
          <br>
                              I have attached ompi_info output for both
          compilers
          <br>
                              from my
          <br>
                              sparc machine and the listings for both
          compilers
          <br>
                              from the
          <br>
                              &lt;prefix&gt;/lib/openmpi directories.
          Hopefully that
          <br>
                              helps to
          <br>
                              find the problem.
          <br>
          <br>
                              hermes tmp 3 tar zvft
          openmpi-2.x_info.tar.gz
          <br>
                              -rw-r--r-- root/root     10969 2016-04-21
          17:06
          <br>
                              ompi_info_SunOS_sparc_cc.txt
          <br>
                              -rw-r--r-- root/root     11044 2016-04-21
          17:06
          <br>
                              ompi_info_SunOS_sparc_gcc.txt
          <br>
                              -rw-r--r-- root/root     71252 2016-04-21
          17:02
          <br>
                              lib64_openmpi.txt
          <br>
                              hermes tmp 4
          <br>
          <br>
          <br>
                              Kind regards and thank you very much once
          more for
          <br>
                              your help
          <br>
          <br>
                              Siegmar
          <br>
          <br>
          <br>
                              Am 21.04.2016 um 15:54 schrieb Ralph
          Castain:
          <br>
          <br>
                                  Odd - it would appear that none of the
          pmix
          <br>
                                  components built? Can you send
          <br>
                                  along the output from ompi_info? Or
          just send a
          <br>
                                  listing of the files in the
          <br>
                                  &lt;prefix&gt;/lib/openmpi directory?
          <br>
          <br>
          <br>
                                      On Apr 21, 2016, at 1:27 AM,
          Siegmar Gross
          <br>
                                     
          &lt;<a class="moz-txt-link-abbreviated" href="mailto:Siegmar.Gross@informatik.hs-fulda.de">Siegmar.Gross@informatik.hs-fulda.de</a>
          <br>
                                     
          <a class="moz-txt-link-rfc2396E" href="mailto:Siegmar.Gross@informatik.hs-fulda.de">&lt;mailto:Siegmar.Gross@informatik.hs-fulda.de&gt;</a>&gt;
          <br>
                                      wrote:
          <br>
          <br>
                                      Hi Ralph,
          <br>
          <br>
                                      Am 21.04.2016 um 00:18 schrieb
          Ralph Castain:
          <br>
          <br>
                                          Could you please rerun these
          test and
          <br>
                                          add “-mca pmix_base_verbose 10
          <br>
                                          -mca pmix_server_verbose 5” to
          your cmd
          <br>
                                          line? I need to see why the
          <br>
                                          pmix components failed.
          <br>
          <br>
          <br>
          <br>
                                      tyr spawn 111 mpiexec -np 1 --host
          <br>
                                      tyr,sunpc1,linpc1,ruester -mca
          <br>
                                      pmix_base_verbose 10 -mca
          <br>
                                      pmix_server_verbose 5
          spawn_multiple_master
          <br>
                                      [tyr.informatik.hs-fulda.de
          <br>
                                     
          <a class="moz-txt-link-rfc2396E" href="http://tyr.informatik.hs-fulda.de">&lt;http://tyr.informatik.hs-fulda.de&gt;</a>
          <br>
                                     
          <a class="moz-txt-link-rfc2396E" href="http://tyr.informatik.hs-fulda.de/">&lt;http://tyr.informatik.hs-fulda.de/&gt;</a>:26652] mca:
          <br>
                                      base: components_register:
          registering
          <br>
                                      framework pmix components
          <br>
                                      [tyr.informatik.hs-fulda.de
          <br>
                                     
          <a class="moz-txt-link-rfc2396E" href="http://tyr.informatik.hs-fulda.de">&lt;http://tyr.informatik.hs-fulda.de&gt;</a>
          <br>
                                     
          <a class="moz-txt-link-rfc2396E" href="http://tyr.informatik.hs-fulda.de/">&lt;http://tyr.informatik.hs-fulda.de/&gt;</a>:26652] mca:
          <br>
                                      base: components_open: opening
          pmix components
          <br>
                                      [tyr.informatik.hs-fulda.de
          <br>
                                     
          <a class="moz-txt-link-rfc2396E" href="http://tyr.informatik.hs-fulda.de">&lt;http://tyr.informatik.hs-fulda.de&gt;</a>
          <br>
                                     
          <a class="moz-txt-link-rfc2396E" href="http://tyr.informatik.hs-fulda.de/">&lt;http://tyr.informatik.hs-fulda.de/&gt;</a>:26652]
          <br>
                                      mca:base:select: Auto-selecting
          pmix components
          <br>
                                      [tyr.informatik.hs-fulda.de
          <br>
                                     
          <a class="moz-txt-link-rfc2396E" href="http://tyr.informatik.hs-fulda.de">&lt;http://tyr.informatik.hs-fulda.de&gt;</a>
          <br>
                                     
          <a class="moz-txt-link-rfc2396E" href="http://tyr.informatik.hs-fulda.de/">&lt;http://tyr.informatik.hs-fulda.de/&gt;</a>:26652]
          <br>
                                      mca:base:select:( pmix) No
          component selected!
          <br>
                                      [tyr.informatik.hs-fulda.de
          <br>
                                     
          <a class="moz-txt-link-rfc2396E" href="http://tyr.informatik.hs-fulda.de">&lt;http://tyr.informatik.hs-fulda.de&gt;</a>
          <br>
                                     
          <a class="moz-txt-link-rfc2396E" href="http://tyr.informatik.hs-fulda.de/">&lt;http://tyr.informatik.hs-fulda.de/&gt;</a>:26652]
          <br>
                                      [[52794,0],0] ORTE_ERROR_LOG: Not
          found in file
          <br>
          <br>
../../../../../openmpi-v2.x-dev-1280-gc110ae8/orte/mca/ess/hnp/ess_hnp_module.c
          <br>
                                      at line 638
          <br>
          <br>
--------------------------------------------------------------------------
          <br>
                                      It looks like orte_init failed for
          some
          <br>
                                      reason; your parallel process is
          <br>
                                      likely to abort.  There are many
          reasons
          <br>
                                      that a parallel process can
          <br>
                                      fail during orte_init; some of
          which are due
          <br>
                                      to configuration or
          <br>
                                      environment problems.  This
          failure appears
          <br>
                                      to be an internal failure;
          <br>
                                      here's some additional information
          (which
          <br>
                                      may only be relevant to an
          <br>
                                      Open MPI developer):
          <br>
          <br>
                                      opal_pmix_base_select failed
          <br>
                                      --&gt; Returned value Not found
          (-13) instead
          <br>
                                      of ORTE_SUCCESS
          <br>
          <br>
--------------------------------------------------------------------------
          <br>
                                      tyr spawn 112
          <br>
          <br>
          <br>
          <br>
          <br>
                                      tyr hello_1 116 mpiexec -np 1
          --host
          <br>
                                      tyr,sunpc1,linpc1,ruester -mca
          <br>
                                      pmix_base_verbose 10 -mca
          <br>
                                      pmix_server_verbose 5 hello_1_mpi
          <br>
                                      [tyr.informatik.hs-fulda.de
          <br>
                                     
          <a class="moz-txt-link-rfc2396E" href="http://tyr.informatik.hs-fulda.de">&lt;http://tyr.informatik.hs-fulda.de&gt;</a>
          <br>
                                     
          <a class="moz-txt-link-rfc2396E" href="http://tyr.informatik.hs-fulda.de/">&lt;http://tyr.informatik.hs-fulda.de/&gt;</a>:27261] mca:
          <br>
                                      base: components_register:
          registering
          <br>
                                      framework pmix components
          <br>
                                      [tyr.informatik.hs-fulda.de
          <br>
                                     
          <a class="moz-txt-link-rfc2396E" href="http://tyr.informatik.hs-fulda.de">&lt;http://tyr.informatik.hs-fulda.de&gt;</a>
          <br>
                                     
          <a class="moz-txt-link-rfc2396E" href="http://tyr.informatik.hs-fulda.de/">&lt;http://tyr.informatik.hs-fulda.de/&gt;</a>:27261] mca:
          <br>
                                      base: components_open: opening
          pmix components
          <br>
                                      [tyr.informatik.hs-fulda.de
          <br>
                                     
          <a class="moz-txt-link-rfc2396E" href="http://tyr.informatik.hs-fulda.de">&lt;http://tyr.informatik.hs-fulda.de&gt;</a>
          <br>
                                     
          <a class="moz-txt-link-rfc2396E" href="http://tyr.informatik.hs-fulda.de/">&lt;http://tyr.informatik.hs-fulda.de/&gt;</a>:27261]
          <br>
                                      mca:base:select: Auto-selecting
          pmix components
          <br>
                                      [tyr.informatik.hs-fulda.de
          <br>
                                     
          <a class="moz-txt-link-rfc2396E" href="http://tyr.informatik.hs-fulda.de">&lt;http://tyr.informatik.hs-fulda.de&gt;</a>
          <br>
                                     
          <a class="moz-txt-link-rfc2396E" href="http://tyr.informatik.hs-fulda.de/">&lt;http://tyr.informatik.hs-fulda.de/&gt;</a>:27261]
          <br>
                                      mca:base:select:( pmix) No
          component selected!
          <br>
                                      [tyr.informatik.hs-fulda.de
          <br>
                                     
          <a class="moz-txt-link-rfc2396E" href="http://tyr.informatik.hs-fulda.de">&lt;http://tyr.informatik.hs-fulda.de&gt;</a>
          <br>
                                     
          <a class="moz-txt-link-rfc2396E" href="http://tyr.informatik.hs-fulda.de/">&lt;http://tyr.informatik.hs-fulda.de/&gt;</a>:27261]
          <br>
                                      [[52315,0],0] ORTE_ERROR_LOG: Not
          found in file
          <br>
          <br>
../../../../../openmpi-v2.x-dev-1280-gc110ae8/orte/mca/ess/hnp/ess_hnp_module.c
          <br>
                                      at line 638
          <br>
          <br>
--------------------------------------------------------------------------
          <br>
                                      It looks like orte_init failed for
          some
          <br>
                                      reason; your parallel process is
          <br>
                                      likely to abort.  There are many
          reasons
          <br>
                                      that a parallel process can
          <br>
                                      fail during orte_init; some of
          which are due
          <br>
                                      to configuration or
          <br>
                                      environment problems.  This
          failure appears
          <br>
                                      to be an internal failure;
          <br>
                                      here's some additional information
          (which
          <br>
                                      may only be relevant to an
          <br>
                                      Open MPI developer):
          <br>
          <br>
                                      opal_pmix_base_select failed
          <br>
                                      --&gt; Returned value Not found
          (-13) instead
          <br>
                                      of ORTE_SUCCESS
          <br>
          <br>
--------------------------------------------------------------------------
          <br>
                                      tyr hello_1 117
          <br>
          <br>
          <br>
          <br>
                                      Thank you very much for your help.
          <br>
          <br>
          <br>
                                      Kind regards
          <br>
          <br>
                                      Siegmar
          <br>
          <br>
          <br>
          <br>
          <br>
                                          Thanks
          <br>
                                          Ralph
          <br>
          <br>
                                              On Apr 20, 2016, at 10:12
          AM,
          <br>
                                              Siegmar Gross
          <br>
                                             
          &lt;<a class="moz-txt-link-abbreviated" href="mailto:Siegmar.Gross@informatik.hs-fulda.de">Siegmar.Gross@informatik.hs-fulda.de</a>
          <br>
          <br>
          <a class="moz-txt-link-rfc2396E" href="mailto:Siegmar.Gross@informatik.hs-fulda.de">&lt;mailto:Siegmar.Gross@informatik.hs-fulda.de&gt;</a>&gt;
          <br>
                                              wrote:
          <br>
          <br>
                                              Hi,
          <br>
          <br>
                                              I have built
          <br>
                                             
          openmpi-v2.x-dev-1280-gc110ae8 on my
          <br>
                                              machines
          <br>
                                              (Solaris 10 Sparc, Solaris
          10
          <br>
                                              x86_64, and openSUSE Linux
          <br>
                                              12.1 x86_64) with
          gcc-5.1.0 and Sun
          <br>
                                              C 5.13. Unfortunately I
          get
          <br>
                                              runtime errors for some
          programs.
          <br>
          <br>
          <br>
                                              Sun C 5.13:
          <br>
                                              ===========
          <br>
          <br>
                                              For all my test programs I
          get the
          <br>
                                              same error on Solaris
          Sparc and
          <br>
                                              Solaris x86_64, while the
          programs
          <br>
                                              work fine on Linux.
          <br>
          <br>
                                              tyr hello_1 115 mpiexec
          -np 2
          <br>
                                              hello_1_mpi
          <br>
                                             
          [tyr.informatik.hs-fulda.de
          <br>
                                             
          <a class="moz-txt-link-rfc2396E" href="http://tyr.informatik.hs-fulda.de">&lt;http://tyr.informatik.hs-fulda.de&gt;</a>
          <br>
                                             
          <a class="moz-txt-link-rfc2396E" href="http://tyr.informatik.hs-fulda.de">&lt;http://tyr.informatik.hs-fulda.de&gt;</a>:22373]
          <br>
                                              [[61763,0],0]
          ORTE_ERROR_LOG: Not
          <br>
                                              found in file
          <br>
          <br>
../../../../../openmpi-v2.x-dev-1280-gc110ae8/orte/mca/ess/hnp/ess_hnp_module.c
          <br>
                                              at line 638
          <br>
          <br>
--------------------------------------------------------------------------
          <br>
                                              It looks like orte_init
          failed for
          <br>
                                              some reason; your parallel
          process is
          <br>
                                              likely to abort.  There
          are many
          <br>
                                              reasons that a parallel
          process can
          <br>
                                              fail during orte_init;
          some of which
          <br>
                                              are due to configuration
          or
          <br>
                                              environment problems. 
          This failure
          <br>
                                              appears to be an internal
          failure;
          <br>
                                              here's some additional
          information
          <br>
                                              (which may only be
          relevant to an
          <br>
                                              Open MPI developer):
          <br>
          <br>
                                              opal_pmix_base_select
          failed
          <br>
                                              --&gt; Returned value Not
          found (-13)
          <br>
                                              instead of ORTE_SUCCESS
          <br>
          <br>
--------------------------------------------------------------------------
          <br>
                                              tyr hello_1 116
          <br>
          <br>
          <br>
          <br>
          <br>
                                              GCC-5.1.0:
          <br>
                                              ==========
          <br>
          <br>
                                              tyr spawn 121 mpiexec -np
          1 --host
          <br>
                                              tyr,sunpc1,linpc1,ruester
          <br>
                                              spawn_multiple_master
          <br>
          <br>
                                              Parent process 0 running
          on
          <br>
                                              tyr.informatik.hs-fulda.de
          <br>
                                             
          <a class="moz-txt-link-rfc2396E" href="http://tyr.informatik.hs-fulda.de">&lt;http://tyr.informatik.hs-fulda.de&gt;</a>
          <br>
                                             
          <a class="moz-txt-link-rfc2396E" href="http://tyr.informatik.hs-fulda.de">&lt;http://tyr.informatik.hs-fulda.de&gt;</a>
          <br>
                                              I create 3 slave
          processes.
          <br>
          <br>
                                             
          [tyr.informatik.hs-fulda.de
          <br>
                                             
          <a class="moz-txt-link-rfc2396E" href="http://tyr.informatik.hs-fulda.de">&lt;http://tyr.informatik.hs-fulda.de&gt;</a>
          <br>
                                             
          <a class="moz-txt-link-rfc2396E" href="http://tyr.informatik.hs-fulda.de">&lt;http://tyr.informatik.hs-fulda.de&gt;</a>:25366]
          <br>
                                              PMIX ERROR:
          UNPACK-PAST-END in file
          <br>
          <br>
../../../../../../openmpi-v2.x-dev-1280-gc110ae8/opal/mca/pmix/pmix112/pmix/src/server/pmix_server_ops.c
          <br>
          <br>
                                              at line 829
          <br>
                                             
          [tyr.informatik.hs-fulda.de
          <br>
                                             
          <a class="moz-txt-link-rfc2396E" href="http://tyr.informatik.hs-fulda.de">&lt;http://tyr.informatik.hs-fulda.de&gt;</a>
          <br>
                                             
          <a class="moz-txt-link-rfc2396E" href="http://tyr.informatik.hs-fulda.de">&lt;http://tyr.informatik.hs-fulda.de&gt;</a>:25366]
          <br>
                                              PMIX ERROR:
          UNPACK-PAST-END in file
          <br>
          <br>
../../../../../../openmpi-v2.x-dev-1280-gc110ae8/opal/mca/pmix/pmix112/pmix/src/server/pmix_server.c
          <br>
          <br>
                                              at line 2176
          <br>
                                              [tyr:25377] *** An error
          occurred in
          <br>
                                              MPI_Comm_spawn_multiple
          <br>
                                              [tyr:25377] *** reported
          by process
          <br>
                                              [3308257281,0]
          <br>
                                              [tyr:25377] *** on
          communicator
          <br>
                                              MPI_COMM_WORLD
          <br>
                                              [tyr:25377] ***
          MPI_ERR_SPAWN: could
          <br>
                                              not spawn processes
          <br>
                                              [tyr:25377] ***
          MPI_ERRORS_ARE_FATAL
          <br>
                                              (processes in this
          communicator will
          <br>
                                              now abort,
          <br>
                                              [tyr:25377] ***    and
          potentially
          <br>
                                              your MPI job)
          <br>
                                              tyr spawn 122
          <br>
          <br>
          <br>
                                              I would be grateful if
          somebody can
          <br>
                                              fix the problems. Thank
          you very
          <br>
                                              much for any help in
          advance.
          <br>
          <br>
          <br>
                                              Kind regards
          <br>
          <br>
                                              Siegmar
          <br>
          <br>
&lt;hello_1_mpi.c&gt;&lt;spawn_multiple_master.c&gt;_______________________________________________
          <br>
          <br>
                                              users mailing list
          <br>
                                              <a class="moz-txt-link-abbreviated" href="mailto:users@open-mpi.org">users@open-mpi.org</a>
          <br>
          <a class="moz-txt-link-rfc2396E" href="mailto:users@open-mpi.org">&lt;mailto:users@open-mpi.org&gt;</a>
          <br>
                                              Subscription:
          <br>
          <br>
          <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>
          <br>
                                              Link to this post:
          <br>
          <br>
<a class="moz-txt-link-freetext" href="http://www.open-mpi.org/community/lists/users/2016/04/28983.php">http://www.open-mpi.org/community/lists/users/2016/04/28983.php</a>
          <br>
          <br>
          <br>
                                         
          _______________________________________________
          <br>
                                          users mailing list
          <br>
                                          <a class="moz-txt-link-abbreviated" href="mailto:users@open-mpi.org">users@open-mpi.org</a>
          <a class="moz-txt-link-rfc2396E" href="mailto:users@open-mpi.org">&lt;mailto:users@open-mpi.org&gt;</a>
          <br>
                                          Subscription:
          <br>
          <br>
          <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>
          <br>
                                          Link to this
          <br>
                                          post:
          <br>
          <br>
<a class="moz-txt-link-freetext" href="http://www.open-mpi.org/community/lists/users/2016/04/28986.php">http://www.open-mpi.org/community/lists/users/2016/04/28986.php</a>
          <br>
          <br>
          <br>
                                     
          _______________________________________________
          <br>
                                      users mailing list
          <br>
                                      <a class="moz-txt-link-abbreviated" href="mailto:users@open-mpi.org">users@open-mpi.org</a>
          <a class="moz-txt-link-rfc2396E" href="mailto:users@open-mpi.org">&lt;mailto:users@open-mpi.org&gt;</a>
          <br>
                                      Subscription:
          <br>
                                     
          <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>
          <br>
                                      Link to this
          <br>
                                      post:
          <br>
          <br>
<a class="moz-txt-link-freetext" href="http://www.open-mpi.org/community/lists/users/2016/04/28987.php">http://www.open-mpi.org/community/lists/users/2016/04/28987.php</a>
          <br>
          <br>
          <br>
          <br>
          <br>
                                 
          _______________________________________________
          <br>
                                  users mailing list
          <br>
                                  <a class="moz-txt-link-abbreviated" href="mailto:users@open-mpi.org">users@open-mpi.org</a>
          <br>
                                  Subscription:
          <br>
                                 
          <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>
          <br>
                                  Link to this post:
          <br>
          <br>
<a class="moz-txt-link-freetext" href="http://www.open-mpi.org/community/lists/users/2016/04/28988.php">http://www.open-mpi.org/community/lists/users/2016/04/28988.php</a>
          <br>
          <br>
          <br>
&lt;openmpi-2.x_info.tar.gz&gt;_______________________________________________
          <br>
                              users mailing list
          <br>
                              <a class="moz-txt-link-abbreviated" href="mailto:users@open-mpi.org">users@open-mpi.org</a>
          <br>
                              Subscription:
          <br>
                             
          <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>
          <br>
                              Link to this post:
          <br>
          <br>
<a class="moz-txt-link-freetext" href="http://www.open-mpi.org/community/lists/users/2016/04/28989.php">http://www.open-mpi.org/community/lists/users/2016/04/28989.php</a>
          <br>
          <br>
          <br>
                         
          _______________________________________________
          <br>
                          users mailing list
          <br>
                          <a class="moz-txt-link-abbreviated" href="mailto:users@open-mpi.org">users@open-mpi.org</a>
          <br>
                          Subscription:
          <br>
                         
          <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>
          <br>
                          Link to this post:
          <br>
                         
          <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/community/lists/users/2016/04/28990.php">http://www.open-mpi.org/community/lists/users/2016/04/28990.php</a>
          <br>
          <br>
          <br>
                      _______________________________________________
          <br>
                      users mailing list
          <br>
                      <a class="moz-txt-link-abbreviated" href="mailto:users@open-mpi.org">users@open-mpi.org</a>
          <br>
                      Subscription:
          <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>
          <br>
                      Link to this post:
          <br>
                     
          <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/community/lists/users/2016/04/28991.php">http://www.open-mpi.org/community/lists/users/2016/04/28991.php</a>
          <br>
          <br>
          <br>
                  _______________________________________________
          <br>
                  users mailing list
          <br>
                  <a class="moz-txt-link-abbreviated" href="mailto:users@open-mpi.org">users@open-mpi.org</a>
          <br>
                  Subscription:
          <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>
          <br>
                  Link to this post:
          <br>
                 
          <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/community/lists/users/2016/04/28992.php">http://www.open-mpi.org/community/lists/users/2016/04/28992.php</a>
          <br>
          <br>
              _______________________________________________
          <br>
              users mailing list
          <br>
              <a class="moz-txt-link-abbreviated" href="mailto:users@open-mpi.org">users@open-mpi.org</a>
          <br>
              Subscription:
          <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>
          <br>
              Link to this post:
          <br>
             
          <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/community/lists/users/2016/04/28993.php">http://www.open-mpi.org/community/lists/users/2016/04/28993.php</a>
          <br>
          <br>
          <br>
          <br>
          _______________________________________________
          <br>
          users mailing list
          <br>
          <a class="moz-txt-link-abbreviated" href="mailto:users@open-mpi.org">users@open-mpi.org</a>
          <br>
          Subscription:
          <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>
          <br>
          Link to this post:
          <br>
<a class="moz-txt-link-freetext" href="http://www.open-mpi.org/community/lists/users/2016/04/28999.php">http://www.open-mpi.org/community/lists/users/2016/04/28999.php</a>
          <br>
          <br>
        </blockquote>
        _______________________________________________
        <br>
        users mailing list
        <br>
        <a class="moz-txt-link-abbreviated" href="mailto:users@open-mpi.org">users@open-mpi.org</a>
        <br>
        Subscription: <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>
        <br>
        Link to this post:
        <br>
        <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/community/lists/users/2016/04/29009.php">http://www.open-mpi.org/community/lists/users/2016/04/29009.php</a>
        <br>
      </blockquote>
      <br>
      <fieldset class="mimeAttachmentHeader"></fieldset>
      <br>
      <pre wrap="">_______________________________________________
users mailing list
<a class="moz-txt-link-abbreviated" href="mailto:users@open-mpi.org">users@open-mpi.org</a>
Subscription: <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>
Link to this post: <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/community/lists/users/2016/04/29033.php">http://www.open-mpi.org/community/lists/users/2016/04/29033.php</a></pre>
    </blockquote>
    <br>
  </body>
</html>

