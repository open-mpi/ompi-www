<html>
  <head>
    <meta content="text/html; charset=windows-1252"
      http-equiv="Content-Type">
  </head>
  <body bgcolor="#FFFFFF" text="#000000">
    Diego,<br>
    <br>
    There aren't many linear solvers that are bit-consistent, where the
    answer is the same no matter how many cores or processes you use. 
    Intel's version of Pardiso is bit-consistent and I think MUMPS 5.0
    might be, but that's all.  You should assume your answer will not be
    exactly the same as you change the number of cores or processes,
    although you should reach the same overall error tolerance in
    approximately the same number of iterations.<br>
    <br>
    Damien<br>
    <br>
    On 2015-10-28 3:51 PM, Diego Avesani wrote:<br>
    <blockquote
cite="mid:CAG8o1y6nvcbxKxNmHyFJ8rQOUeRxaYBNZuPSSUmJPC_J_26C_g@mail.gmail.com"
      type="cite">
      <div dir="ltr">dear Andreas, dear all,
        <div>The code is quite long. It is a conjugate gradient
          algorithm to solve a complex system.</div>
        <div><br>
        </div>
        <div>I have noticed that when a do cycle is small, let's say</div>
        <div>do i=1,3</div>
        <div><br>
        </div>
        <div>enddo</div>
        <div><br>
        </div>
        <div>the results are identical. If the cycle is big, let's say
          do i=1,20, the results are different and the difference
          increase with the number of iterations.</div>
        <div><br>
        </div>
        <div>What do you think?</div>
        <div><br>
        </div>
        <div><br>
        </div>
      </div>
      <div class="gmail_extra"><br clear="all">
        <div>
          <div class="gmail_signature">Diego<br>
            <br>
          </div>
        </div>
        <br>
        <div class="gmail_quote">On 28 October 2015 at 22:32, Andreas
          Schäfer <span dir="ltr">&lt;<a moz-do-not-send="true"
              href="mailto:gentryx@gmx.de" target="_blank">gentryx@gmx.de</a>&gt;</span>
          wrote:<br>
          <blockquote class="gmail_quote" style="margin:0 0 0
            .8ex;border-left:1px #ccc solid;padding-left:1ex"><span
              class="">On 22:03 Wed 28 Oct     , Diego Avesani wrote:<br>
              &gt; When I use a single CPU a get a results, when I use 4
              CPU I get another<br>
              &gt; one. I do not think that very is a bug.<br>
              <br>
            </span>Sounds like a bug to me, most likely in your code.<br>
            <span class=""><br>
              &gt; Do you think that these small differences are normal?<br>
              <br>
            </span>It depends on what small means. Floating point
            operations in a<br>
            computer are generally not commutative, so parallelization
            may in deed<br>
            lead to different results.<br>
            <span class=""><br>
              &gt; Is there any way to get the same results? is some
              align problem?<br>
              <br>
            </span>Impossible to say without knowing your code.<br>
            <br>
            Cheers<br>
            -Andreas<br>
            <br>
            <br>
            --<br>
            ==========================================================<br>
            Andreas Schäfer<br>
            HPC and Grid Computing<br>
            Department of Computer Science 3<br>
            Friedrich-Alexander-Universität Erlangen-Nürnberg, Germany<br>
            <a moz-do-not-send="true" href="tel:%2B49%209131%2085-27910"
              value="+4991318527910">+49 9131 85-27910</a><br>
            PGP/GPG key via keyserver<br>
            <a moz-do-not-send="true" href="http://www.libgeodecomp.org"
              rel="noreferrer" target="_blank">http://www.libgeodecomp.org</a><br>
            ==========================================================<br>
            <br>
            (\___/)<br>
            (+'.'+)<br>
            (")_(")<br>
            This is Bunny. Copy and paste Bunny into your<br>
            signature to help him gain world domination!<br>
            <br>
            _______________________________________________<br>
            users mailing list<br>
            <a moz-do-not-send="true" href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
            Subscription: <a moz-do-not-send="true"
              href="http://www.open-mpi.org/mailman/listinfo.cgi/users"
              rel="noreferrer" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
            Link to this post: <a moz-do-not-send="true"
              href="http://www.open-mpi.org/community/lists/users/2015/10/27933.php"
              rel="noreferrer" target="_blank">http://www.open-mpi.org/community/lists/users/2015/10/27933.php</a><br>
          </blockquote>
        </div>
        <br>
      </div>
      <br>
      <fieldset class="mimeAttachmentHeader"></fieldset>
      <br>
      <pre wrap="">_______________________________________________
users mailing list
<a class="moz-txt-link-abbreviated" href="mailto:users@open-mpi.org">users@open-mpi.org</a>
Subscription: <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>
Link to this post: <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/community/lists/users/2015/10/27934.php">http://www.open-mpi.org/community/lists/users/2015/10/27934.php</a></pre>
    </blockquote>
    <br>
  </body>
</html>

