<html><head><meta http-equiv="Content-Type" content="text/html charset=iso-2022-jp"></head><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space;"><br><div><div>On Aug 20, 2014, at 9:04 AM, Reuti &lt;<a href="mailto:reuti@staff.uni-marburg.de">reuti@staff.uni-marburg.de</a>&gt; wrote:</div><br class="Apple-interchange-newline"><blockquote type="cite"><div style="font-size: 12px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px;">Am 20.08.2014 um 16:26 schrieb Ralph Castain:<br><br><blockquote type="cite">On Aug 20, 2014, at 6:58 AM, Reuti &lt;<a href="mailto:reuti@staff.uni-marburg.de">reuti@staff.uni-marburg.de</a>&gt; wrote:<br><br><blockquote type="cite">Hi,<br><br>Am 20.08.2014 um 13:26 schrieb <a href="mailto:tmishima@jcity.maeda.co.jp">tmishima@jcity.maeda.co.jp</a>:<br><br><blockquote type="cite">Reuti,<br><br>If you want to allocate 10 procs with N threads, the Torque<br>script below should work for you:<br><br>qsub -l nodes=10:ppn=N<br>mpirun -map-by slot:pe=N -np 10 -x OMP_NUM_THREADS=N ./inverse.exe<br></blockquote><br>I played around with giving -np 10 in addition to a Tight Integration. The slot count is not really divided I think, but only 10 out of the granted maximum is used (while on each of the listed machines an `orted` is started). Due to the fixed allocation this is of course the result we want to achieve as it subtracts bunches of 8 from the given list of machines resp. slots. In SGE it's sufficient to use and AFAICS it works (without touching the $PE_HOSTFILE any longer):<br><br>===<br>export OMP_NUM_THREADS=8<br>mpirun -map-by slot:pe=$OMP_NUM_THREADS -np $(bc &lt;&lt;&lt;"$NSLOTS / $OMP_NUM_THREADS") ./inverse.exe<br>===<br><br>and submit with:<br><br>$ qsub -pe orte 80 job.sh<br><br>as the variables are distributed to the slave nodes by SGE already.<br><br>Nevertheless, using -np in addition to the Tight Integration gives a taste of a kind of half-tight integration in some way. And would not work for us because "--bind-to none" can't be used in such a command (see below) and throws an error.<br><br><br><blockquote type="cite">Then, the openmpi automatically reduces the logical slot count to 10<br>by dividing real slot count 10N by binding width of N.<br><br>I don't know why you want to use pe=N without binding, but unfortunately<br>the openmpi allocates successive cores to each process so far when you<br>use pe option - it forcibly bind_to core.<br></blockquote><br>In a shared cluster with many users and different MPI libraries in use, only the queuingsystem could know which job got which cores granted. This avoids any oversubscription of cores, while others are idle.<br></blockquote><br>FWIW: we detect the exterior binding constraint and work within it<br></blockquote><br>Aha, this is quite interesting - how do you do this: scanning the /proc/&lt;pid&gt;/status or alike? What happens if you don't find enough free cores as they are used up by other applications already?<br><br></div></blockquote><div><br></div>Remember, when you use mpirun to launch, we launch our own daemons using the native launcher (e.g., qsub). So the external RM will bind our daemons to the specified cores on each node. We use hwloc to determine what cores our daemons are bound to, and then bind our own child processes to cores within that range.</div><div><br></div><div>If the cores we are bound to are the same on each node, then we will do this with no further instruction. However, if the cores are different on the individual nodes, then you need to add --hetero-nodes to your command line (as the nodes appear to be heterogeneous to us).</div><div><br></div><div>So it is up to the RM to set the constraint - we just live within it.</div><div><br></div><div><br><blockquote type="cite"><div style="font-size: 12px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px;">-- Reuti<br><br><br><blockquote type="cite"><blockquote type="cite">-- Reuti<br><br><br><blockquote type="cite">Tetsuya<br><br><br><blockquote type="cite">Hi,<br><br>Am 20.08.2014 um 06:26 schrieb Tetsuya Mishima:<br><br><blockquote type="cite">Reuti and Oscar,<br><br>I'm a Torque user and I myself have never used SGE, so I hesitated to<br></blockquote></blockquote>join<br><blockquote type="cite"><blockquote type="cite">the discussion.<br><br>From my experience with the Torque, the openmpi 1.8 series has already<br>resolved the issue you pointed out in combining MPI with OpenMP.<br><br>Please try to add --map-by slot:pe=8 option, if you want to use 8<br></blockquote></blockquote>threads.<br><blockquote type="cite"><blockquote type="cite">Then, then openmpi 1.8 should allocate processes properly without any<br></blockquote></blockquote>modification<br><blockquote type="cite"><blockquote type="cite">of the hostfile provided by the Torque.<br><br>In your case(8 threads and 10 procs):<br><br># you have to request 80 slots using SGE command before mpirun<br>mpirun --map-by slot:pe=8 -np 10 ./inverse.exe<br></blockquote><br>Thx for pointing me to this option, for now I can't get it working though<br></blockquote>(in fact, I want to use it without binding essentially). This allows to<br>tell Open MPI to bind more cores to each of the MPI<br><blockquote type="cite">processes - ok, but does it lower the slot count granted by Torque too? I<br></blockquote>mean, was your submission command like:<br><blockquote type="cite"><br>$ qsub -l nodes=10:ppn=8 ...<br><br>so that Torque knows, that it should grant and remember this slot count<br></blockquote>of a total of 80 for the correct accounting?<br><blockquote type="cite"><br>-- Reuti<br><br><br><blockquote type="cite">where you can omit --bind-to option because --bind-to core is assumed<br>as default when pe=N is provided by the user.<br>Regards,<br>Tetsuya<br><br><blockquote type="cite">Hi,<br><br>Am 19.08.2014 um 19:06 schrieb Oscar Mojica:<br><br><blockquote type="cite">I discovered what was the error. I forgot include the '-fopenmp' when<br></blockquote></blockquote></blockquote></blockquote>I compiled the objects in the Makefile, so the program worked but it didn't<br>divide the job<br><blockquote type="cite"><blockquote type="cite">in threads. Now the program is working and I can use until 15 cores for<br></blockquote></blockquote>machine in the queue one.q.<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><br>Anyway i would like to try implement your advice. Well I'm not alone<br></blockquote></blockquote></blockquote></blockquote>in the cluster so i must implement your second suggestion. The steps are<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><br>a) Use '$ qconf -mp orte' to change the allocation rule to 8<br></blockquote><br>The number of slots defined in your used one.q was also increased to 8<br></blockquote></blockquote></blockquote>(`qconf -sq one.q`)?<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><br><br><blockquote type="cite">b) Set '#$ -pe orte 80' in the script<br></blockquote><br>Fine.<br><br><br><blockquote type="cite">c) I'm not sure how to do this step. I'd appreciate your help here. I<br></blockquote></blockquote></blockquote></blockquote>can add some lines to the script to determine the PE_HOSTFILE path and<br>contents, but i<br><blockquote type="cite"><blockquote type="cite">don't know how alter it<br><blockquote type="cite"><br>For now you can put in your jobscript (just after OMP_NUM_THREAD is<br></blockquote></blockquote></blockquote>exported):<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><br>awk -v omp_num_threads=$OMP_NUM_THREADS '{ $2/=omp_num_threads;<br></blockquote></blockquote></blockquote>print }' $PE_HOSTFILE &gt; $TMPDIR/machines<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">export PE_HOSTFILE=$TMPDIR/machines<br><br>=============<br><br>Unfortunately noone stepped into this discussion, as in my opinion<br></blockquote></blockquote></blockquote>it's a much broader issue which targets all users who want to combine MPI<br>with OpenMP. The<br><blockquote type="cite"><blockquote type="cite">queuingsystem should get a proper request for the overall amount of<br></blockquote></blockquote>slots the user needs. For now this will be forwarded to Open MPI and it<br>will use this<br><blockquote type="cite"><blockquote type="cite">information to start the appropriate number of processes (which was an<br></blockquote></blockquote>achievement for the Tight Integration out-of-the-box of course) and ignores<br>any setting of<br><blockquote type="cite"><blockquote type="cite">OMP_NUM_THREADS. So, where should the generated list of machines be<br></blockquote></blockquote>adjusted; there are several options:<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><br>a) The PE of the queuingsystem should do it:<br><br>+ a one time setup for the admin<br>+ in SGE the "start_proc_args" of the PE could alter the $PE_HOSTFILE<br>- the "start_proc_args" would need to know the number of threads, i.e.<br></blockquote></blockquote></blockquote>OMP_NUM_THREADS must be defined by "qsub -v ..." outside of the jobscript<br>(tricky scanning<br><blockquote type="cite"><blockquote type="cite">of the submitted jobscript for OMP_NUM_THREADS would be too nasty)<br><blockquote type="cite">- limits to use inside the jobscript calls to libraries behaving in<br></blockquote></blockquote></blockquote>the same way as Open MPI only<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><br><br>b) The particular queue should do it in a queue prolog:<br><br>same as a) I think<br><br><br>c) The user should do it<br><br>+ no change in the SGE installation<br>- each and every user must include it in all the jobscripts to adjust<br></blockquote></blockquote></blockquote>the list and export the pointer to the $PE_HOSTFILE, but he could change it<br>forth and back<br><blockquote type="cite"><blockquote type="cite">for different steps of the jobscript though<br><blockquote type="cite"><br><br>d) Open MPI should do it<br><br>+ no change in the SGE installation<br>+ no change to the jobscript<br>+ OMP_NUM_THREADS can be altered for different steps of the jobscript<br></blockquote></blockquote></blockquote>while staying inside the granted allocation automatically<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">o should MKL_NUM_THREADS be covered too (does it use OMP_NUM_THREADS<br></blockquote></blockquote></blockquote>already)?<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><br>-- Reuti<br><br><br><blockquote type="cite">echo "PE_HOSTFILE:"<br>echo $PE_HOSTFILE<br>echo<br>echo "cat PE_HOSTFILE:"<br>cat $PE_HOSTFILE<br><br>Thanks for take a time for answer this emails, your advices had been<br></blockquote></blockquote></blockquote></blockquote>very useful<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><br>PS: The version of SGE is &nbsp;&nbsp;OGS/GE 2011.11p1<br><br><br>Oscar Fabian Mojica Ladino<br>Geologist M.S. in &nbsp;Geophysics<br><br><br><blockquote type="cite">From: <a href="mailto:reuti@staff.uni-marburg.de">reuti@staff.uni-marburg.de</a><br>Date: Fri, 15 Aug 2014 20:38:12 +0200<br>To: <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>Subject: Re: [OMPI users] Running a hybrid MPI+openMP program<br><br>Hi,<br><br>Am 15.08.2014 um 19:56 schrieb Oscar Mojica:<br><br><blockquote type="cite">Yes, my installation of Open MPI is SGE-aware. I got the following<br><br>[oscar@compute-1-2 ~]$ ompi_info | grep grid<br>MCA ras: gridengine (MCA v2.0, API v2.0, Component v1.6.2)<br></blockquote><br>Fine.<br><br><br><blockquote type="cite">I'm a bit slow and I didn't understand the las part of your<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>message. So i made a test trying to solve my doubts.<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">This is the cluster configuration: There are some machines turned<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>off but that is no problem<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><br>[oscar@aguia free-noise]$ qhost<br>HOSTNAME ARCH NCPU LOAD MEMTOT MEMUSE SWAPTO SWAPUS<br><br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>-------------------------------------------------------------------------------<br><br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">global - - - - - - -<br>compute-1-10 linux-x64 16 0.97 23.6G 558.6M 996.2M 0.0<br>compute-1-11 linux-x64 16 - 23.6G - 996.2M -<br>compute-1-12 linux-x64 16 0.97 23.6G 561.1M 996.2M 0.0<br>compute-1-13 linux-x64 16 0.99 23.6G 558.7M 996.2M 0.0<br>compute-1-14 linux-x64 16 1.00 23.6G 555.1M 996.2M 0.0<br>compute-1-15 linux-x64 16 0.97 23.6G 555.5M 996.2M 0.0<br>compute-1-16 linux-x64 8 0.00 15.7G 296.9M 1000.0M 0.0<br>compute-1-17 linux-x64 8 0.00 15.7G 299.4M 1000.0M 0.0<br>compute-1-18 linux-x64 8 - 15.7G - 1000.0M -<br>compute-1-19 linux-x64 8 - 15.7G - 996.2M -<br>compute-1-2 linux-x64 16 1.19 23.6G 468.1M 1000.0M 0.0<br>compute-1-20 linux-x64 8 0.04 15.7G 297.2M 1000.0M 0.0<br>compute-1-21 linux-x64 8 - 15.7G - 1000.0M -<br>compute-1-22 linux-x64 8 0.00 15.7G 297.2M 1000.0M 0.0<br>compute-1-23 linux-x64 8 0.16 15.7G 299.6M 1000.0M 0.0<br>compute-1-24 linux-x64 8 0.00 15.7G 291.5M 996.2M 0.0<br>compute-1-25 linux-x64 8 0.04 15.7G 293.4M 996.2M 0.0<br>compute-1-26 linux-x64 8 - 15.7G - 1000.0M -<br>compute-1-27 linux-x64 8 0.00 15.7G 297.0M 1000.0M 0.0<br>compute-1-29 linux-x64 8 - 15.7G - 1000.0M -<br>compute-1-3 linux-x64 16 - 23.6G - 996.2M -<br>compute-1-30 linux-x64 16 - 23.6G - 996.2M -<br>compute-1-4 linux-x64 16 0.97 23.6G 571.6M 996.2M 0.0<br>compute-1-5 linux-x64 16 1.00 23.6G 559.6M 996.2M 0.0<br>compute-1-6 linux-x64 16 0.66 23.6G 403.1M 996.2M 0.0<br>compute-1-7 linux-x64 16 0.95 23.6G 402.7M 996.2M 0.0<br>compute-1-8 linux-x64 16 0.97 23.6G 556.8M 996.2M 0.0<br>compute-1-9 linux-x64 16 1.02 23.6G 566.0M 1000.0M 0.0<br><br>I ran my program using only MPI with 10 processors of the queue<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>one.q which has 14 machines (compute-1-2 to compute-1-15). Whit 'qstat -t'<br>I got:<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><br>[oscar@aguia free-noise]$ qstat -t<br>job-ID prior name user state submit/start at queue master<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>ja-task-ID task-ID state cpu mem io stat failed<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote></blockquote></blockquote><br></blockquote></blockquote>-------------------------------------------------------------------------------------------------------------------------------------------------------------------<br><br><blockquote type="cite"><blockquote type="cite">----<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">2726 0.50500 job oscar r 08/15/2014 12:38:21<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote><a href="mailto:one.q@compute-1-2.local">one.q@compute-1-2.local</a> MASTER r 00:49:12 554.13753 0.09163<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><a href="mailto:one.q@compute-1-2.local">one.q@compute-1-2.local</a> SLAVE<br>2726 0.50500 job oscar r 08/15/2014 12:38:21<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote><a href="mailto:one.q@compute-1-5.local">one.q@compute-1-5.local</a> SLAVE 1.compute-1-5 r 00:48:53 551.49022 0.09410<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">2726 0.50500 job oscar r 08/15/2014 12:38:21<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote><a href="mailto:one.q@compute-1-9.local">one.q@compute-1-9.local</a> SLAVE 1.compute-1-9 r 00:50:00 564.22764 0.09409<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">2726 0.50500 job oscar r 08/15/2014 12:38:21<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote><a href="mailto:one.q@compute-1-12.local">one.q@compute-1-12.local</a> SLAVE 1.compute-1-12 r 00:47:30 535.30379 0.09379<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">2726 0.50500 job oscar r 08/15/2014 12:38:21<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote><a href="mailto:one.q@compute-1-13.local">one.q@compute-1-13.local</a> SLAVE 1.compute-1-13 r 00:49:51 561.69868 0.09379<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">2726 0.50500 job oscar r 08/15/2014 12:38:21<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote><a href="mailto:one.q@compute-1-14.local">one.q@compute-1-14.local</a> SLAVE 1.compute-1-14 r 00:49:14 554.60818 0.09379<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">2726 0.50500 job oscar r 08/15/2014 12:38:21<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote><a href="mailto:one.q@compute-1-10.local">one.q@compute-1-10.local</a> SLAVE 1.compute-1-10 r 00:49:59 562.95487 0.09349<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">2726 0.50500 job oscar r 08/15/2014 12:38:21<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote><a href="mailto:one.q@compute-1-15.local">one.q@compute-1-15.local</a> SLAVE 1.compute-1-15 r 00:50:01 563.27221 0.09361<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">2726 0.50500 job oscar r 08/15/2014 12:38:21<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote><a href="mailto:one.q@compute-1-8.local">one.q@compute-1-8.local</a> SLAVE 1.compute-1-8 r 00:49:26 556.68431 0.09349<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">2726 0.50500 job oscar r 08/15/2014 12:38:21<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote><a href="mailto:one.q@compute-1-4.local">one.q@compute-1-4.local</a> SLAVE 1.compute-1-4 r 00:49:27 556.87510 0.04967<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><br>Yes, here you got 10 slots (= cores) granted by SGE. So there is no<br></blockquote></blockquote></blockquote></blockquote></blockquote>free core left inside the allocation of SGE to allow the use of additional<br>cores for your<br><blockquote type="cite"><blockquote type="cite">threads. If you use more cores than granted by SGE, it will<br></blockquote></blockquote>oversubscribe the machines.<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><br>The issue is now:<br><br>a) If you want 8 threads per MPI process, your job will use 80 cores<br></blockquote></blockquote></blockquote></blockquote></blockquote>in total - for now SGE isn't aware of it.<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><br>b) Although you specified $fill_up as allocation rule, it looks like<br></blockquote></blockquote></blockquote></blockquote></blockquote>$round_robin. Is there more than one slot defined in the queue definition<br>of one.q to get<br><blockquote type="cite"><blockquote type="cite">exclusive access?<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><br>c) What version of SGE are you using? Certain ones use cgroups or<br></blockquote></blockquote></blockquote></blockquote></blockquote>bind processes directly to cores (although it usually needs to be requested<br>by the job:<br><blockquote type="cite"><blockquote type="cite">first line of `qconf -help`).<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><br><br>In case you are alone in the cluster, you could bypass the<br></blockquote></blockquote></blockquote></blockquote></blockquote>allocation with b) (unless you are hit by c)). But having a mixture of<br>users and jobs a different<br><blockquote type="cite"><blockquote type="cite">handling would be necessary to handle this in a proper way IMO:<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><br>a) having a PE with a fixed allocation rule of 8<br><br>b) requesting this PE with an overall slot count of 80<br><br>c) copy and alter the $PE_HOSTFILE to show only (granted core count<br></blockquote></blockquote></blockquote></blockquote></blockquote>per machine) divided by (OMP_NUM_THREADS) per entry, change $PE_HOSTFILE so<br>that it points<br><blockquote type="cite"><blockquote type="cite">to the altered file<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><br>d) Open MPI with a Tight Integration will now start only N process<br></blockquote></blockquote></blockquote></blockquote></blockquote>per machine according to the altered hostfile, in your case one<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><br>e) Your application can start the desired threads and you stay<br></blockquote></blockquote></blockquote></blockquote></blockquote>inside the granted allocation<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><br>-- Reuti<br><br><br><blockquote type="cite">I accessed to the MASTER processor with 'ssh compute-1-2.local' ,<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>and with $ ps -e f and got this, I'm showing only the last lines<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><br>2506 ? Ss 0:00 /usr/sbin/atd<br>2548 tty1 Ss+ 0:00 /sbin/mingetty /dev/tty1<br>2550 tty2 Ss+ 0:00 /sbin/mingetty /dev/tty2<br>2552 tty3 Ss+ 0:00 /sbin/mingetty /dev/tty3<br>2554 tty4 Ss+ 0:00 /sbin/mingetty /dev/tty4<br>2556 tty5 Ss+ 0:00 /sbin/mingetty /dev/tty5<br>2558 tty6 Ss+ 0:00 /sbin/mingetty /dev/tty6<br>3325 ? Sl 0:04 /opt/gridengine/bin/linux-x64/sge_execd<br>17688 ? S 0:00 \_ sge_shepherd-2726 -bg<br>17695 ? Ss 0:00 \_<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>-bash /opt/gridengine/default/spool/compute-1-2/job_scripts/2726<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">17797 ? S 0:00 \_ /usr/bin/time -f %E /opt/openmpi/bin/mpirun -v<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>-np 10 ./inverse.exe<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">17798 ? S 0:01 \_ /opt/openmpi/bin/mpirun -v -np 10 ./inverse.exe<br>17799 ? Sl 0:00 \_ /opt/gridengine/bin/linux-x64/qrsh -inherit<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>-nostdin -V compute-1-5.local PATH=/opt/openmpi/bin:$PATH ; expo<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">17800 ? Sl 0:00 \_ /opt/gridengine/bin/linux-x64/qrsh -inherit<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>-nostdin -V compute-1-9.local PATH=/opt/openmpi/bin:$PATH ; expo<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">17801 ? Sl 0:00 \_ /opt/gridengine/bin/linux-x64/qrsh -inherit<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>-nostdin -V compute-1-12.local PATH=/opt/openmpi/bin:$PATH ; exp<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">17802 ? Sl 0:00 \_ /opt/gridengine/bin/linux-x64/qrsh -inherit<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>-nostdin -V compute-1-13.local PATH=/opt/openmpi/bin:$PATH ; exp<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">17803 ? Sl 0:00 \_ /opt/gridengine/bin/linux-x64/qrsh -inherit<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>-nostdin -V compute-1-14.local PATH=/opt/openmpi/bin:$PATH ; exp<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">17804 ? Sl 0:00 \_ /opt/gridengine/bin/linux-x64/qrsh -inherit<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>-nostdin -V compute-1-10.local PATH=/opt/openmpi/bin:$PATH ; exp<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">17805 ? Sl 0:00 \_ /opt/gridengine/bin/linux-x64/qrsh -inherit<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>-nostdin -V compute-1-15.local PATH=/opt/openmpi/bin:$PATH ; exp<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">17806 ? Sl 0:00 \_ /opt/gridengine/bin/linux-x64/qrsh -inherit<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>-nostdin -V compute-1-8.local PATH=/opt/openmpi/bin:$PATH ; expo<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">17807 ? Sl 0:00 \_ /opt/gridengine/bin/linux-x64/qrsh -inherit<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>-nostdin -V compute-1-4.local PATH=/opt/openmpi/bin:$PATH ; expo<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">17826 ? R 31:36 \_ ./inverse.exe<br>3429 ? Ssl 0:00 automount --pid-file /var/run/autofs.pid<br><br>So the job is using the 10 machines, Until here is all right OK. Do<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>you think that changing the "allocation_rule " to a number instead $fill_up<br>the MPI<br><blockquote type="cite"><blockquote type="cite">processes would divide the work in that number of threads?<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><br>Thanks a lot<br><br>Oscar Fabian Mojica Ladino<br>Geologist M.S. in Geophysics<br><br><br>PS: I have another doubt, what is a slot? is a physical core?<br><br><br><blockquote type="cite">From: <a href="mailto:reuti@staff.uni-marburg.de">reuti@staff.uni-marburg.de</a><br>Date: Thu, 14 Aug 2014 23:54:22 +0200<br>To: <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>Subject: Re: [OMPI users] Running a hybrid MPI+openMP program<br><br>Hi,<br><br>I think this is a broader issue in case an MPI library is used in<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>conjunction with threads while running inside a queuing system. First:<br>whether your<br><blockquote type="cite"><blockquote type="cite">actual installation of Open MPI is SGE-aware you can check with:<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><br>$ ompi_info | grep grid<br>MCA ras: gridengine (MCA v2.0, API v2.0, Component v1.6.5)<br><br>Then we can look at the definition of your PE: "allocation_rule<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>$fill_up". This means that SGE will grant you 14 slots in total in any<br>combination on the<br><blockquote type="cite"><blockquote type="cite">available machines, means 8+4+2 slots allocation is an allowed<br></blockquote></blockquote>combination like 4+4+3+3 and so on. Depending on the SGE-awareness it's a<br>question: will your<br><blockquote type="cite"><blockquote type="cite">application just start processes on all nodes and completely disregard<br></blockquote></blockquote>the granted allocation, or as the other extreme does it stays on one and<br>the same machine<br><blockquote type="cite"><blockquote type="cite">for all started processes? On the master node of the parallel job you<br></blockquote></blockquote>can issue:<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><br>$ ps -e f<br><br>(f w/o -) to have a look whether `ssh` or `qrsh -inhert ...` is<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>used to reach other machines and their requested process count.<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><br><br>Now to the common problem in such a set up:<br><br>AFAICS: for now there is no way in the Open MPI + SGE combination<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>to specify the number of MPI processes and intended number of threads which<br>are<br><blockquote type="cite"><blockquote type="cite">automatically read by Open MPI while staying inside the granted slot<br></blockquote></blockquote>count and allocation. So it seems to be necessary to have the intended<br>number of threads being<br><blockquote type="cite"><blockquote type="cite">honored by Open MPI too.<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><br>Hence specifying e.g. "allocation_rule 8" in such a setup while<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>requesting 32 processes, would for now start 32 processes by MPI already,<br>as Open MP reads &gt; the $PE_HOSTFILE and acts accordingly.<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><br>Open MPI would have to read the generated machine file in a<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>slightly different way regarding threads: a) read the $PE_HOSTFILE, b)<br>divide the granted<br><blockquote type="cite"><blockquote type="cite">slots per machine by OMP_NUM_THREADS, c) throw an error in case it's<br></blockquote></blockquote>not divisible by OMP_NUM_THREADS. Then start one process per quotient.<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><br>Would this work for you?<br><br>-- Reuti<br><br>PS: This would also mean to have a couple of PEs in SGE having a<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>fixed "allocation_rule". While this works right now, an extension in SGE<br>could be<br><blockquote type="cite"><blockquote type="cite">"$fill_up_omp"/"$round_robin_omp" and using OMP_NUM_THREADS there too,<br></blockquote></blockquote>hence it must not be specified as an `export` in the job script but either<br>on the command<br><blockquote type="cite"><blockquote type="cite">line or inside the job script in #$ lines as job requests. This would<br></blockquote></blockquote>mean to collect slots in bunches of OMP_NUM_THREADS on each machine to<br>reach the overall<br><blockquote type="cite"><blockquote type="cite">specified slot count. Whether OMP_NUM_THREADS or n times<br></blockquote></blockquote>OMP_NUM_THREADS is allowed per machine needs to be discussed.<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><br>PS2: As Univa SGE can also supply a list of granted cores in the<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>$PE_HOSTFILE, it would be an extension to feed this to Open MPI to allow<br>any UGE aware<br><blockquote type="cite"><blockquote type="cite">binding.<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><br><br>Am 14.08.2014 um 21:52 schrieb Oscar Mojica:<br><br><blockquote type="cite">Guys<br><br>I changed the line to run the program in the script with both<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>options<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">/usr/bin/time -f "%E" /opt/openmpi/bin/mpirun -v --bind-to-none<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>-np $NSLOTS ./inverse.exe<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">/usr/bin/time -f "%E" /opt/openmpi/bin/mpirun -v --bind-to-socket<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>-np $NSLOTS ./inverse.exe<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><br>but I got the same results. When I use man mpirun appears:<br><br>-bind-to-none, --bind-to-none<br>Do not bind processes. (Default.)<br><br>and the output of 'qconf -sp orte' is<br><br>pe_name orte<br>slots 9999<br>user_lists NONE<br>xuser_lists NONE<br>start_proc_args /bin/true<br>stop_proc_args /bin/true<br>allocation_rule $fill_up<br>control_slaves TRUE<br>job_is_first_task FALSE<br>urgency_slots min<br>accounting_summary TRUE<br><br>I don't know if the installed Open MPI was compiled with<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>'--with-sge'. How can i know that?<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">before to think in an hybrid application i was using only MPI and<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>the program used few processors (14). The cluster possesses 28 machines, 15<br>with 16<br><blockquote type="cite"><blockquote type="cite">cores and 13 with 8 cores totalizing 344 units of processing. When I<br></blockquote></blockquote>submitted the job (only MPI), the MPI processes were spread to the cores<br>directly, for that<br><blockquote type="cite"><blockquote type="cite">reason I created a new queue with 14 machines trying to gain more time.<br></blockquote></blockquote>the results were the same in both cases. In the last case i could prove<br>that the processes<br><blockquote type="cite"><blockquote type="cite">were distributed to all machines correctly.<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><br>What I must to do?<br>Thanks<br><br>Oscar Fabian Mojica Ladino<br>Geologist M.S. in Geophysics<br><br><br><blockquote type="cite">Date: Thu, 14 Aug 2014 10:10:17 -0400<br>From: <a href="mailto:maxime.boissonneault@calculquebec.ca">maxime.boissonneault@calculquebec.ca</a><br>To: <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>Subject: Re: [OMPI users] Running a hybrid MPI+openMP program<br><br>Hi,<br>You DEFINITELY need to disable OpenMPI's new default binding.<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>Otherwise,<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">your N threads will run on a single core. --bind-to socket would<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>be my<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">recommendation for hybrid jobs.<br><br>Maxime<br><br><br>Le 2014-08-14 10:04, Jeff Squyres (jsquyres) a 馗rit :<br><blockquote type="cite">I don't know much about OpenMP, but do you need to disable Open<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>MPI's default bind-to-core functionality (I'm assuming you're using Open<br>MPI 1.8.x)?<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><br>You can try "mpirun --bind-to none ...", which will have Open<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>MPI not bind MPI processes to cores, which might allow OpenMP to think that<br>it can use<br><blockquote type="cite"><blockquote type="cite">all the cores, and therefore it will spawn num_cores threads...?<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><br><br>On Aug 14, 2014, at 9:50 AM, Oscar Mojica<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>&lt;<a href="mailto:o_mojical@hotmail.com">o_mojical@hotmail.com</a>&gt; wrote:<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><br><blockquote type="cite">Hello everybody<br><br>I am trying to run a hybrid mpi + openmp program in a cluster.<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>I created a queue with 14 machines, each one with 16 cores. The program<br>divides the<br><blockquote type="cite"><blockquote type="cite">work among the 14 processors with MPI and within each processor a loop<br></blockquote></blockquote>is also divided into 8 threads for example, using openmp. The problem is<br>that when I submit<br><blockquote type="cite"><blockquote type="cite">the job to the queue the MPI processes don't divide the work into<br></blockquote></blockquote>threads and the program prints the number of threads that are working<br>within each process as one.<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><br>I made a simple test program that uses openmp and I logged in<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>one machine of the fourteen. I compiled it using gfortran -fopenmp<br>program.f -o exe,<br><blockquote type="cite"><blockquote type="cite">set the OMP_NUM_THREADS environment variable equal to 8 and when I ran<br></blockquote></blockquote>directly in the terminal the loop was effectively divided among the cores<br>and for example in<br><blockquote type="cite"><blockquote type="cite">this case the program printed the number of threads equal to 8<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><br>This is my Makefile<br><br># Start of the makefile<br># Defining variables<br>objects = inv_grav3d.o funcpdf.o gr3dprm.o fdjac.o dsvd.o<br>#f90comp = /opt/openmpi/bin/mpif90<br>f90comp = /usr/bin/mpif90<br>#switch = -O3<br>executable = inverse.exe<br># Makefile<br>all : $(executable)<br>$(executable) : $(objects)<br>$(f90comp) -fopenmp -g -O -o $(executable) $(objects)<br>rm $(objects)<br>%.o: %.f<br>$(f90comp) -c $&lt;<br># Cleaning everything<br>clean:<br>rm $(executable)<br>#<span class="Apple-tab-span" style="white-space: pre;">	</span>rm $(objects)<br># End of the makefile<br><br>and the script that i am using is<br><br>#!/bin/bash<br>#$ -cwd<br>#$ -j y<br>#$ -S /bin/bash<br>#$ -pe orte 14<br>#$ -N job<br>#$ -q new.q<br><br>export OMP_NUM_THREADS=8<br>/usr/bin/time -f "%E" /opt/openmpi/bin/mpirun -v -np<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>$NSLOTS ./inverse.exe<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><br>am I forgetting something?<br><br>Thanks,<br><br>Oscar Fabian Mojica Ladino<br>Geologist M.S. in Geophysics<br>_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>Subscription:<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">Link to this post:<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>http://www.open-mpi.org/community/lists/users/2014/08/25016.php<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><br></blockquote><br><br>--<br>---------------------------------<br>Maxime Boissonneault<br>Analyste de calcul - Calcul Qu饕ec, Universit・Laval<br>Ph. D. en physique<br><br>_______________________________________________<br>users mailing list<br>users@open-mpi.org<br>Subscription: http://www.open-mpi.org/mailman/listinfo.cgi/users<br>Link to this post:<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>http://www.open-mpi.org/community/lists/users/2014/08/25020.php<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">_______________________________________________<br>users mailing list<br>users@open-mpi.org<br>Subscription: http://www.open-mpi.org/mailman/listinfo.cgi/users<br>Link to this post:<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>http://www.open-mpi.org/community/lists/users/2014/08/25032.php<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><br>_______________________________________________<br>users mailing list<br>users@open-mpi.org<br>Subscription: http://www.open-mpi.org/mailman/listinfo.cgi/users<br>Link to this post:<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>http://www.open-mpi.org/community/lists/users/2014/08/25034.php<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">_______________________________________________<br>users mailing list<br>users@open-mpi.org<br>Subscription: http://www.open-mpi.org/mailman/listinfo.cgi/users<br>Link to this post:<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>http://www.open-mpi.org/community/lists/users/2014/08/25037.php<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><br>_______________________________________________<br>users mailing list<br>users@open-mpi.org<br>Subscription: http://www.open-mpi.org/mailman/listinfo.cgi/users<br>Link to this post:<br></blockquote></blockquote></blockquote></blockquote></blockquote>http://www.open-mpi.org/community/lists/users/2014/08/25038.php<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">_______________________________________________<br>users mailing list<br>users@open-mpi.org<br>Subscription: http://www.open-mpi.org/mailman/listinfo.cgi/users<br>Link to this post:<br></blockquote></blockquote></blockquote></blockquote>http://www.open-mpi.org/community/lists/users/2014/08/25079.php<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><br>_______________________________________________<br>users mailing list<br>users@open-mpi.org<br>Subscription: http://www.open-mpi.org/mailman/listinfo.cgi/users<br>Link to this post:<br></blockquote></blockquote></blockquote>http://www.open-mpi.org/community/lists/users/2014/08/25080.php<br><blockquote type="cite"><blockquote type="cite"><br>----<br>Tetsuya Mishima &nbsp;tmishima@jcity.maeda.co.jp<br>_______________________________________________<br>users mailing list<br>users@open-mpi.org<br>Subscription: http://www.open-mpi.org/mailman/listinfo.cgi/users<br>Link to this post:<br></blockquote></blockquote>http://www.open-mpi.org/community/lists/users/2014/08/25081.php<br><blockquote type="cite"><br>_______________________________________________<br>users mailing list<br>users@open-mpi.org<br>Subscription: http://www.open-mpi.org/mailman/listinfo.cgi/users<br>Link to this post:<br></blockquote>http://www.open-mpi.org/community/lists/users/2014/08/25083.php<br><br>_______________________________________________<br>users mailing list<br>users@open-mpi.org<br>Subscription: http://www.open-mpi.org/mailman/listinfo.cgi/users<br>Link to this post: http://www.open-mpi.org/community/lists/users/2014/08/25084.php<br></blockquote><br>_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>Subscription: http://www.open-mpi.org/mailman/listinfo.cgi/users<br>Link to this post: http://www.open-mpi.org/community/lists/users/2014/08/25087.php<br></blockquote><br>_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>Subscription:<span class="Apple-converted-space">&nbsp;</span><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>Link to this post:<span class="Apple-converted-space">&nbsp;</span><a href="http://www.open-mpi.org/community/lists/users/2014/08/25088.php">http://www.open-mpi.org/community/lists/users/2014/08/25088.php</a><br></blockquote><br>_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>Subscription:<span class="Apple-converted-space">&nbsp;</span><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>Link to this post:<span class="Apple-converted-space">&nbsp;</span><a href="http://www.open-mpi.org/community/lists/users/2014/08/25089.php">http://www.open-mpi.org/community/lists/users/2014/08/25089.php</a></div></blockquote></div><br></body></html>
