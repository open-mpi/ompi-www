<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=gb2312">
</head>
<body dir="auto">
<div>Normally we use rdma-cm to build rdma connection ,then create Qpairs to do rdma data transmit ion, so what is the consideration for separating rdma-cm connection built and data transmit ion at design stage?&nbsp;</div>
<div><br>
</div>
<div>Maybe this question is not reasonable, and hope any response?</div>
<div><br>
</div>
<div>Thanks</div>
<div>Yanfei<br>
<br>
Sent from my iPad</div>
<div><br>
On 2014年3月27日, at 下午10:10, &quot;Ralph Castain&quot; &lt;<a href="mailto:rhc@open-mpi.org">rhc@open-mpi.org</a>&gt; wrote:<br>
<br>
</div>
<blockquote type="cite">
<div>
<div dir="ltr">
<div>Just one other point to clarify - there is an apparent misunderstanding regarding the following MCA param:<br>
<br>
<span style="font-size:10.5pt;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;;color:rgb(31,73,125)" lang="EN-US">-mca btl_openib_cpc_include rdmacm<br>
<br>
</span></div>
<span style="font-size:10.5pt;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;;color:rgb(31,73,125)" lang="EN-US">This param has nothing to do with telling openib to use RDMA for communication. What it does is tell the openib BTL to use RDMA to establish the point-to-point
 connection between the two processes. The actual messaging may or may not use RDMA to move the bytes - that's a totally separate code path.<br>
<br>
</span></div>
<div class="gmail_extra"><br>
<br>
<div class="gmail_quote">On Thu, Mar 27, 2014 at 6:21 AM, Wang,Yanfei(SYS) <span dir="ltr">
&lt;<a href="mailto:wangyanfei01@baidu.com" target="_blank">wangyanfei01@baidu.com</a>&gt;</span> wrote:<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
it seem that all confusions have already been shot, thanks Jeff!<br>
<br>
Thanks!<br>
Yanfei<br>
<br>
Sent from my iPad<br>
<div class="HOEnZb">
<div class="h5"><br>
&gt; On 2014年3月27日, at 下午8:38, &quot;Jeff Squyres (jsquyres)&quot; &lt;<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a>&gt; wrote:<br>
&gt;<br>
&gt; Here's a few key facts that might help:<br>
&gt;<br>
&gt; 1. The hostfile has nothing to do with what network interfaces are used for MPI traffic. &nbsp;It is only used to specify what servers you launch on, regardless of what IP interface on that server you specify.<br>
&gt; 2. What network interfaces are used are a combination of the BTL selected and then any optional additional parameters given that that BTL.<br>
&gt; 3. If you do not specify any BTL, then Open MPI will choose the &quot;best&quot; ones, and use that.<br>
&gt; 4. As of somewhere in the v1.7.x series, the ompi_info command only shows a few MCA parameters by default. &nbsp;To see all MCA parameters, add &quot;--level 9&quot; to the command line.<br>
&gt;<br>
&gt; In your case, if you didn't specify a BTL, Open MPI would see your RoCE interfaces and therefore choose the openib BTL for off-node communication (and exclude the TCP BTL, because it is &quot;worse&quot; than the openib BTL), sm for on-node communication, and self
 for loopback communication.<br>
&gt;<br>
&gt; If you specify --mca btl tcp,sm,self, then you are restricting OMPI's pool of BTLs that it can choose from -- meaning that the openib BTL won't even be considered. &nbsp;So OMPI will therefore use the TCP BTL for off-node communication.<br>
&gt;<br>
&gt; Also, remember that you can &quot;mpirun ... hostname&quot; (i.e., the Linux &quot;hostname&quot; command) to verify what servers you are actually running on.<br>
&gt;<br>
&gt; I see that the ompi_info(1) man page is not super-detailed about the --level option; I'll go fix that right now (and ensure it's in the v1.8 release).<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt;&gt; On Mar 27, 2014, at 6:44 AM, &quot;Wang,Yanfei(SYS)&quot; &lt;<a href="mailto:wangyanfei01@baidu.com">wangyanfei01@baidu.com</a>&gt; wrote:<br>
&gt;&gt;<br>
&gt;&gt; Hi,<br>
&gt;&gt;<br>
&gt;&gt; Update:<br>
&gt;&gt; If explicitly assign --mca btl tcp,sm,self and the traffic will go 10G TCP/IP link instead of 40G RDMA link, and the tcp/ip latency is 22us at average, which is reasonable.<br>
&gt;&gt; [root@bb-nsi-ib04 pt2pt]# mpirun --hostfile hosts -np 2 --map-by node --mca btl tcp,sm,self osu_latency<br>
&gt;&gt; # OSU MPI Latency Test v4.3<br>
&gt;&gt; # Size &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Latency (us)<br>
&gt;&gt; 0 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;22.07<br>
&gt;&gt; 1 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;22.48<br>
&gt;&gt; 2 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;22.38<br>
&gt;&gt; 4 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;22.39<br>
&gt;&gt; 8 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;22.52<br>
&gt;&gt; 16 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 22.52<br>
&gt;&gt; 32 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 22.59<br>
&gt;&gt; 64 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 22.73<br>
&gt;&gt; 128 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;23.01<br>
&gt;&gt; 256 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;24.32<br>
&gt;&gt; 512 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;28.50<br>
&gt;&gt; 1024 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 31.06<br>
&gt;&gt; 2048 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 56.06<br>
&gt;&gt; 4096 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 68.53<br>
&gt;&gt; 8192 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 77.09<br>
&gt;&gt; 16384 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 105.23<br>
&gt;&gt; 32768 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 143.51<br>
&gt;&gt; 65536 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 229.79<br>
&gt;&gt; 131072 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;285.28<br>
&gt;&gt; 262144 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;423.26<br>
&gt;&gt; 524288 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;693.82<br>
&gt;&gt; 1048576 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;1634.03<br>
&gt;&gt; 2097152 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;3311.69<br>
&gt;&gt; 4194304 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;7055.16<br>
&gt;&gt;<br>
&gt;&gt; The conclusion is that the “ Chostfile with 10G IP address” does enable traffic select 10G TCP/IP link, and mpirun select RDMA link by default even if you did not enable &nbsp;“--mca btl openib,sm,self”!<br>
&gt;&gt; So， how to understand that “Chostfile” does not work fine and how to control the multi-HCA(NIC) traffic for MPI library?<br>
&gt;&gt;<br>
&gt;&gt; Besides, the following command does not reflect any information about rdma transport parameter control except tcp parameter.<br>
&gt;&gt;<br>
&gt;&gt; [root@bb-nsi-ib04 pt2pt]# ompi_info --param btl all<br>
&gt;&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; MCA btl: parameter &quot;btl_tcp_if_include&quot; (current value: &quot;&quot;,<br>
&gt;&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;data source: default, level: 1 user/basic, type:<br>
&gt;&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;string)<br>
&gt;&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Comma-delimited list of devices and/or CIDR<br>
&gt;&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;notation of networks to use for MPI communication<br>
&gt;&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;(e.g., &quot;eth0,<a href="http://192.168.0.0/16" target="_blank">192.168.0.0/16</a>&quot;). &nbsp;Mutually exclusive<br>
&gt;&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;with btl_tcp_if_exclude.<br>
&gt;&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; MCA btl: parameter &quot;btl_tcp_if_exclude&quot; (current value:<br>
&gt;&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&quot;<a href="http://127.0.0.1/8,sppp" target="_blank">127.0.0.1/8,sppp</a>&quot;, data source: default, level: 1<br>
&gt;&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;user/basic, type: string)<br>
&gt;&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Comma-delimited list of devices and/or CIDR<br>
&gt;&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;notation of networks to NOT use for MPI<br>
&gt;&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;communication -- all devices not matching these<br>
&gt;&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;specifications will be used (e.g.,<br>
&gt;&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&quot;eth0,<a href="http://192.168.0.0/16" target="_blank">192.168.0.0/16</a>&quot;). &nbsp;If set to a non-default<br>
&gt;&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;value, it is mutually exclusive with<br>
&gt;&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;btl_tcp_if_include.<br>
&gt;&gt; [root@bb-nsi-ib04 pt2pt]#<br>
&gt;&gt;<br>
&gt;&gt; Hope to have a deep understand on it~<br>
&gt;&gt;<br>
&gt;&gt; Thanks<br>
&gt;&gt; --Yanfei<br>
&gt;&gt;<br>
&gt;&gt; 发件人: devel [mailto:<a href="mailto:devel-bounces@open-mpi.org">devel-bounces@open-mpi.org</a>] 代表 Wang,Yanfei(SYS)<br>
&gt;&gt; 发送时间: 2014年3月27日 18:17<br>
&gt;&gt; 收件人: Open MPI Developers<br>
&gt;&gt; 主题: [OMPI devel] 答复: doubt on latency result with OpenMPI library<br>
&gt;&gt;<br>
&gt;&gt; HI，<br>
&gt;&gt;<br>
&gt;&gt; “--map-by node” does remove this trouble.<br>
&gt;&gt; ---<br>
&gt;&gt; Configuration:<br>
&gt;&gt; Even if using mpi --hostfile to control traffic to go 10G TCP/IP network, and the latency still is 5us in both situation!<br>
&gt;&gt; [root@bb-nsi-ib04 pt2pt]# cat /etc/hosts<br>
&gt;&gt; 192.168.71.3 ib03<br>
&gt;&gt; 192.168.71.4 ib04<br>
&gt;&gt; [root@bb-nsi-ib04 pt2pt]# ifconfig<br>
&gt;&gt; eth0 &nbsp; &nbsp; &nbsp;Link encap:Ethernet &nbsp;HWaddr 20:0B:C7:26:3F:C3<br>
&gt;&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;inet addr:192.168.71.4 &nbsp;Bcast:192.168.71.255 &nbsp;Mask:255.255.255.0<br>
&gt;&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;inet6 addr: fe80::220b:c7ff:fe26:3fc3/64 Scope:Link<br>
&gt;&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;UP BROADCAST RUNNING MULTICAST &nbsp;MTU:1500 &nbsp;Metric:1<br>
&gt;&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;RX packets:834635 errors:0 dropped:0 overruns:0 frame:0<br>
&gt;&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;TX packets:339853 errors:0 dropped:0 overruns:0 carrier:0<br>
&gt;&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;collisions:0 txqueuelen:1000<br>
&gt;&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;RX bytes:681908607 (650.3 MiB) &nbsp;TX bytes:103031295 (98.2 MiB)<br>
&gt;&gt; 10G eth0 is not rdma-enabled nic~<br>
&gt;&gt;<br>
&gt;&gt; a. &nbsp; &nbsp; &nbsp; using openib<br>
&gt;&gt; [root@bb-nsi-ib04 pt2pt]# mpirun --hostfile hosts -np 2 --map-by node --mca btl openib,self,sm --mca btl_openib_cpc_include rdmacm osu_latency<br>
&gt;&gt; # OSU MPI Latency Test v4.3<br>
&gt;&gt; # Size &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Latency (us)<br>
&gt;&gt; 0 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 5.20<br>
&gt;&gt; 1 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 5.36<br>
&gt;&gt; 2 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 5.31<br>
&gt;&gt; 4 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 5.34<br>
&gt;&gt; 8 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 5.46<br>
&gt;&gt; 16 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;5.35<br>
&gt;&gt; 32 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;5.44<br>
&gt;&gt; 64 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;5.48<br>
&gt;&gt; 128 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 6.74<br>
&gt;&gt; 256 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 6.87<br>
&gt;&gt; 512 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 7.05<br>
&gt;&gt; 1024 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;7.52<br>
&gt;&gt; 2048 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;8.38<br>
&gt;&gt; 4096 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 10.36<br>
&gt;&gt; 8192 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 14.18<br>
&gt;&gt; 16384 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;23.69<br>
&gt;&gt; 32768 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;31.91<br>
&gt;&gt; 65536 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;38.89<br>
&gt;&gt; 131072 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 47.76<br>
&gt;&gt; 262144 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 80.42<br>
&gt;&gt; 524288 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;137.52<br>
&gt;&gt; 1048576 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 251.81<br>
&gt;&gt; 2097152 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 485.23<br>
&gt;&gt; 4194304 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 948.08<br>
&gt;&gt; b. &nbsp; &nbsp; &nbsp; have no explicit rdma setting.<br>
&gt;&gt; [root@bb-nsi-ib04 pt2pt]# mpirun --hostfile hosts -np 2 --map-by node osu_latency<br>
&gt;&gt; # OSU MPI Latency Test v4.3<br>
&gt;&gt; # Size &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Latency (us)<br>
&gt;&gt; 0 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 5.23<br>
&gt;&gt; 1 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 5.28<br>
&gt;&gt; 2 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 5.21<br>
&gt;&gt; 4 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 5.33<br>
&gt;&gt; 8 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 5.33<br>
&gt;&gt; 16 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;5.36<br>
&gt;&gt; 32 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;5.33<br>
&gt;&gt; 64 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;5.41<br>
&gt;&gt; 128 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 6.74<br>
&gt;&gt; 256 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 6.98<br>
&gt;&gt; 512 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 7.11<br>
&gt;&gt; 1024 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;7.47<br>
&gt;&gt; 2048 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;8.46<br>
&gt;&gt; 4096 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 10.38<br>
&gt;&gt; 8192 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 14.30<br>
&gt;&gt; 16384 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;21.20<br>
&gt;&gt; 32768 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;31.21<br>
&gt;&gt; 65536 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;39.85<br>
&gt;&gt; 131072 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 47.70<br>
&gt;&gt; 262144 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 80.24<br>
&gt;&gt; 524288 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;137.59<br>
&gt;&gt; 1048576 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 251.62<br>
&gt;&gt; 2097152 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 485.14<br>
&gt;&gt; 4194304 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 945.80<br>
&gt;&gt; [root@bb-nsi-ib04 pt2pt]#<br>
&gt;&gt;<br>
&gt;&gt; I found that the bandwidth got from osu_bw benchmark is equal to 40G RDMA HCA, so I doubt if the traffic always goes between 40G RDMA link, and the control for TCP/IP link does work.<br>
&gt;&gt;<br>
&gt;&gt; I will consult the FAQ for details, if further suggestion, welcome..<br>
&gt;&gt;<br>
&gt;&gt; Thanks<br>
&gt;&gt; --Yanfei<br>
&gt;&gt; 发件人: devel [mailto:<a href="mailto:devel-bounces@open-mpi.org">devel-bounces@open-mpi.org</a>] 代表 Ralph Castain<br>
&gt;&gt; 发送时间: 2014年3月27日 18:05<br>
&gt;&gt; 收件人: Open MPI Developers<br>
&gt;&gt; 主题: Re: [OMPI devel] doubt on latency result with OpenMPI library<br>
&gt;&gt;<br>
&gt;&gt; Try adding &quot;--map-by node&quot; to your command line to ensure the procs really are running on separate nodes.<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; On Thu, Mar 27, 2014 at 1:40 AM, Wang,Yanfei(SYS) &lt;<a href="mailto:wangyanfei01@baidu.com">wangyanfei01@baidu.com</a>&gt; wrote:<br>
&gt;&gt; Hi，<br>
&gt;&gt;<br>
&gt;&gt; HW Test Topology：<br>
&gt;&gt; Ip：<a href="http://192.168.72.4/24" target="_blank">192.168.72.4/24</a> C<a href="http://192.168.72.4/24" target="_blank">192.168.72.4/24</a>, enable vlan and RoCE<br>
&gt;&gt; IB03 server 40G port-- - 40G Ethernet switch ----IB04 server 40G port： configure it as RoCE link<br>
&gt;&gt; IP: <a href="http://192.168.71.3/24" target="_blank">192.168.71.3/24</a> ---<a href="http://192.168.71.4/24" target="_blank">192.168.71.4/24</a><br>
&gt;&gt; IB03 server 10G port C 10G Ethernet switch C IB04 server 10G port： configure it as normal TCP/IP Ethernet link：（server management interface）<br>
&gt;&gt;<br>
&gt;&gt; Mpi configuration：<br>
&gt;&gt; MPI Hosts file：<br>
&gt;&gt; [root@bb-nsi-ib04 pt2pt]# cat hosts<br>
&gt;&gt; ib03 slots=1<br>
&gt;&gt; ib04 slots=1<br>
&gt;&gt; DNS hosts<br>
&gt;&gt; [root@bb-nsi-ib04 pt2pt]# cat /etc/hosts<br>
&gt;&gt; 192.168.71.3 ib03<br>
&gt;&gt; 192.168.71.4 ib04<br>
&gt;&gt; [root@bb-nsi-ib04 pt2pt]#<br>
&gt;&gt; This configuration will create 2 nodes for MPI latency evaluation<br>
&gt;&gt;<br>
&gt;&gt; Benchmark:<br>
&gt;&gt; osu-micro-benchmarks-4.3<br>
&gt;&gt;<br>
&gt;&gt; result:<br>
&gt;&gt; a. &nbsp; &nbsp; &nbsp; Enable traffic go between 10G TCP/IP port using following /etc/hosts file<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; root@bb-nsi-ib04 pt2pt]# cat /etc/hosts<br>
&gt;&gt; 192.168.71.3 ib03<br>
&gt;&gt; 192.168.71.4 ib04<br>
&gt;&gt; The average latency is 4.5us of osu_latency, see log following:<br>
&gt;&gt; [root@bb-nsi-ib04 pt2pt]# mpirun --hostfile hosts -np 2 osu_latency<br>
&gt;&gt; # OSU MPI Latency Test v4.3<br>
&gt;&gt; # Size &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Latency (us)<br>
&gt;&gt; 0 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 4.56<br>
&gt;&gt; 1 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 4.90<br>
&gt;&gt; 2 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 4.90<br>
&gt;&gt; 4 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 4.60<br>
&gt;&gt; 8 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 4.71<br>
&gt;&gt; 16 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;4.72<br>
&gt;&gt; 32 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;5.40<br>
&gt;&gt; 64 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;4.77<br>
&gt;&gt; 128 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 6.74<br>
&gt;&gt; 256 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 7.01<br>
&gt;&gt; 512 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 7.14<br>
&gt;&gt; 1024 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;7.63<br>
&gt;&gt; 2048 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;8.22<br>
&gt;&gt; 4096 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 10.39<br>
&gt;&gt; 8192 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 14.26<br>
&gt;&gt; 16384 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;20.80<br>
&gt;&gt; 32768 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;31.97<br>
&gt;&gt; 65536 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;37.75<br>
&gt;&gt; 131072 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 47.28<br>
&gt;&gt; 262144 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 80.40<br>
&gt;&gt; 524288 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;137.65<br>
&gt;&gt; 1048576 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 250.17<br>
&gt;&gt; 2097152 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 484.71<br>
&gt;&gt; 4194304 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 946.01<br>
&gt;&gt;<br>
&gt;&gt; b. &nbsp; &nbsp; &nbsp; Enable traffic go between RoCE link using /etc/hosts as following and mpirun Cmca btl openib,self,sm …<br>
&gt;&gt;<br>
&gt;&gt; [root@bb-nsi-ib04 pt2pt]# cat /etc/hosts<br>
&gt;&gt; 192.168.72.3 ib03<br>
&gt;&gt; 192.168.72.4 ib04<br>
&gt;&gt; Result:<br>
&gt;&gt; [root@bb-nsi-ib04 pt2pt]# mpirun --hostfile hosts -np 2 --mca btl openib,self,sm --mca btl_openib_cpc_include rdmacm osu_latency<br>
&gt;&gt; # OSU MPI Latency Test v4.3<br>
&gt;&gt; # Size &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Latency (us)<br>
&gt;&gt; 0 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 4.83<br>
&gt;&gt; 1 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 5.17<br>
&gt;&gt; 2 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 5.12<br>
&gt;&gt; 4 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 5.25<br>
&gt;&gt; 8 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 5.38<br>
&gt;&gt; 16 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;5.40<br>
&gt;&gt; 32 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;5.19<br>
&gt;&gt; 64 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;5.04<br>
&gt;&gt; 128 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 6.74<br>
&gt;&gt; 256 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 7.04<br>
&gt;&gt; 512 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 7.34<br>
&gt;&gt; 1024 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;7.91<br>
&gt;&gt; 2048 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;8.17<br>
&gt;&gt; 4096 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 10.39<br>
&gt;&gt; 8192 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 14.22<br>
&gt;&gt; 16384 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;22.05<br>
&gt;&gt; 32768 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;31.68<br>
&gt;&gt; 65536 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;37.57<br>
&gt;&gt; 131072 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 48.25<br>
&gt;&gt; 262144 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 79.98<br>
&gt;&gt; 524288 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;137.66<br>
&gt;&gt; 1048576 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 251.38<br>
&gt;&gt; 2097152 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 485.66<br>
&gt;&gt; 4194304 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 947.81<br>
&gt;&gt; [root@bb-nsi-ib04 pt2pt]#<br>
&gt;&gt;<br>
&gt;&gt; Question:<br>
&gt;&gt; 1. &nbsp; &nbsp; &nbsp; Why do they have similar latency, 5us, which is too small to believe it! In our test environment, it will take more than 50 us to deal with tcp sync and return sync_ack, and also x86 server will take more thans 20us at average to do ip forwarding(test
 from professional HW tester), so does the latency is reasonable?<br>
&gt;&gt;<br>
&gt;&gt; 2. &nbsp; &nbsp; &nbsp; Normally, the switch will introduces more than 1.5us switch time! Using accelio, a mellanox released opensource rdma library, it will take at least 4 us rtt latency to do simpe ping-pong test. So 5 us MPI latency (TCP/IP and RoCE) above is rather
 unbelievable…<br>
&gt;&gt;<br>
&gt;&gt; 3. &nbsp; &nbsp; &nbsp; The fact that the tcp/ip transport and roce RDMA transport acquire same latency &nbsp;is so puzzling..<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; Before deeply understanding what happened inside the MPI benchmark, can show us some suggestion? Does the mpirun command works correctly here?<br>
&gt;&gt; It must has some mistakes about this test, pls correct me,.<br>
&gt;&gt;<br>
&gt;&gt; Eg: tcp syn&amp;sync ack latency:<br>
&gt;&gt; &lt;image001.png&gt;<br>
&gt;&gt;<br>
&gt;&gt; Thanks<br>
&gt;&gt; -Yanfei<br>
&gt;&gt;<br>
&gt;&gt; _______________________________________________<br>
&gt;&gt; devel mailing list<br>
&gt;&gt; <a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>
&gt;&gt; Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">
http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
&gt;&gt; Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2014/03/14400.php" target="_blank">
http://www.open-mpi.org/community/lists/devel/2014/03/14400.php</a><br>
&gt;&gt;<br>
&gt;&gt; _______________________________________________<br>
&gt;&gt; devel mailing list<br>
&gt;&gt; <a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>
&gt;&gt; Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">
http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
&gt;&gt; Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2014/03/14403.php" target="_blank">
http://www.open-mpi.org/community/lists/devel/2014/03/14403.php</a><br>
&gt;<br>
&gt;<br>
&gt; --<br>
&gt; Jeff Squyres<br>
&gt; <a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a><br>
&gt; For corporate legal information go to: <a href="http://www.cisco.com/web/about/doing_business/legal/cri/" target="_blank">
http://www.cisco.com/web/about/doing_business/legal/cri/</a><br>
&gt;<br>
&gt; _______________________________________________<br>
&gt; devel mailing list<br>
&gt; <a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>
&gt; Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">
http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
&gt; Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2014/03/14404.php" target="_blank">
http://www.open-mpi.org/community/lists/devel/2014/03/14404.php</a><br>
</div>
</div>
<br>
_______________________________________________<br>
devel mailing list<br>
<a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">
http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2014/03/14405.php" target="_blank">
http://www.open-mpi.org/community/lists/devel/2014/03/14405.php</a><br>
</blockquote>
</div>
<br>
</div>
</div>
</blockquote>
<blockquote type="cite">
<div><span>_______________________________________________</span><br>
<span>devel mailing list</span><br>
<span><a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a></span><br>
<span>Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel">
http://www.open-mpi.org/mailman/listinfo.cgi/devel</a></span><br>
<span>Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2014/03/14406.php">
http://www.open-mpi.org/community/lists/devel/2014/03/14406.php</a></span></div>
</blockquote>
</body>
</html>

