This looks fine. There is one point we may want to clarify/highlight in some manner. When we reach 1.3.4, the SLURM support changes so things won&#39;t work for LANL&#39;s interactive login method (a rather unique one) unless they upgrade to slurm xxx (forget number - can look it up when I get back).<br>
<br>I&#39;m not sure how to handle that point, though - outside of the tri-labs, who already know about the restriction, nobody else is likely to notice or care. So perhaps nothing need be said? I fear the language may get complex, and wind up confusing people who shouldn&#39;t care.<br>
<br>Thoughts?<br>Ralph<br><br><br><div class="gmail_quote">On Wed, Oct 28, 2009 at 2:02 PM, Jeff Squyres <span dir="ltr">&lt;<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">
Folks -- here&#39;s what I put in the NEWS section for 1.3.4rc2. Please send me any corrections / additions / etc. for 1.3.4 final ASAP:<br>
<br>
- Fix some issues in OMPI&#39;s SRPM with regard to shell_scripts_basename<br>
 마nd its use with mpi-selector. Thanks to Bill Johnstone for<br>
 맗ointing out the problem.<br>
- Added many new MPI job process affinity options to mpirun. See the<br>
 맕ewly-updated mpirun(1) man page for details.<br>
- Several updates to mpirun&#39;s XML output.<br>
- Update to fix a few Valgrind warnings with regards to the ptmalloc2<br>
 마llocator and Open MPI&#39;s use of PLPA.<br>
- Many updates and fixes to the (non-default) &quot;sm&quot; collective<br>
 맊omponent (i.e., native shared memory MPI collective operations).<br>
- Updates and fixes to some MPI_COMM_SPAWN_MULTIPLE corner cases.<br>
- Fix some internal copying functions in Open MPI&#39;s use of PLPA.<br>
- Correct some SLURM nodelist parsing logic that may have interfered<br>
 망ith large jobs. ㅁdditionally, per advice from the SLURM team,<br>
 맊hange the environment variable that we use for obtaining the job&#39;s<br>
 마llocation.<br>
- Revert to an older, safer (but slower) communicator ID allocation<br>
 마lgorithm.<br>
- Fixed minimum distance finding for OpenFabrics devices in the openib<br>
 BTL.<br>
- Relax the parameter checking MPI_CART_CREATE a bit.<br>
- Fix MPI_COMM_SPAWN[_MULTIPLE] to only error-check the info arguments<br>
 맖n the root process. Thanks to Federico Golfre Andreasi for<br>
 reporting the problem.<br>
- Fixed some BLCR configure issues.<br>
- Fixed a potential deadlock when the openib BTL was used with<br>
 MPI_THREAD_MULTIPLE.<br>
- Fixed dynamic rules selection for the &quot;tuned&quot; coll component.<br>
- Added a launch progress meter to mpirun (useful for large jobs; set<br>
 맚he orte_report_launch_progress MCA parameter to 1 to see it).<br>
- Reduced the number of file descriptors consumed by each MPI process.<br>
- Add new device IDs for Chelsio T3 RNICs to the openib BTL config file.<br>
- Fix some CRS self component issues.<br>
- Added some MCA parameters to the PSM MTL to tune its run-time<br>
 막ehavior.<br>
- Fix some VT issues with MPI_BOTTOM/MPI_IN_PLACE.<br>
- Man page updates from the Debain Open MPI package maintainers.<br>
- Add cycle counter support for the Alpha and Sparc platforms.<br>
- Pass visibility flags to libltdl&#39;s configure script, resulting in<br>
 맚hose symbols being hidden. This appears to mainly solve the<br>
 맗roblem of applications attempting to use different versions of<br>
 맓ibltdl from that used to build Open MPI.<br><font color="#888888">
<br>
<br>
-- <br>
Jeff Squyres<br>
<a href="mailto:jsquyres@cisco.com" target="_blank">jsquyres@cisco.com</a><br>
<br>
_______________________________________________<br>
devel mailing list<br>
<a href="mailto:devel@open-mpi.org" target="_blank">devel@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
</font></blockquote></div><br>

