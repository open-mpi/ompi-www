<html><head></head><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space; ">To clarify: what I'm trying to understand is what the heck a "BASIL_RESERVATION_ID" is - it isn't a standard slurm thing, nor can I find it defined in alps, so it appears to just be a local name you invented. True?<div><br></div><div>If so, I would rather see some standard name instead of something local to one organization.</div><div><br><div><div>On Jul 9, 2010, at 8:08 AM, Jerome Soumagne wrote:</div><br class="Apple-interchange-newline"><blockquote type="cite">
<div text="#000000" bgcolor="#ffffff">
Ok I may have not explained very clearly. In our case we only use SLURM
for the resource manager.<br>
The difference here is that the SLURM version that we use has support
for ALPS. Therefore when we run our job using the mpirun command, since
we have the alps environment loaded, it's the ALPS RAS which is
selected, and the ALPS PLM as well. I think I could even not compile
the OpenMPI slurm support.<br>
<br>
Here is what we do for example: here is my batch script (with the
patched version)<small><font face="Courier New, Courier, monospace"> <br>
#!/bin/bash<br>
#SBATCH --job-name=HelloOMPI <br>
#SBATCH --nodes=2<br>
#SBATCH --time=00:30:00<br>
<br>
set -ex<br>
cd /users/soumagne/gele/hello<br>
mpirun --mca ras_base_verbose 10 --mca plm_base_verbose 10 -np 2
--bynode `pwd`/hello</font></small><br>
<br>
And here is the output that I get:<br>
<big><font face="Courier New, Courier, monospace"><small><small>soumagne@gele1:~/gele/hello&gt;
more slurm-165.out <br>
+ cd /users/soumagne/gele/hello<br>
++ pwd<br>
+ mpirun --mca ras_base_verbose 10 --mca plm_base_verbose 10 --bynode
-np 2 /use<br>
rs/soumagne/gele/hello/hello<br>
[gele2:15844] mca: base: components_open: Looking for plm components<br>
[gele2:15844] mca: base: components_open: opening plm components<br>
[gele2:15844] mca: base: components_open: found loaded component alps<br>
[gele2:15844] mca: base: components_open: component alps has no
register functio<br>
n<br>
[gele2:15844] mca: base: components_open: component alps open function
successfu<br>
l<br>
[gele2:15844] mca: base: components_open: found loaded component slurm<br>
[gele2:15844] mca: base: components_open: component slurm has no
register functi<br>
on<br>
[gele2:15844] mca: base: components_open: component slurm open function
successf<br>
ul<br>
[gele2:15844] mca:base:select: Auto-selecting plm components<br>
[gele2:15844] mca:base:select:(&nbsp; plm) Querying component [alps]<br>
[gele2:15844] mca:base:select:(&nbsp; plm) Query of component [alps] set
priority to <br>
75<br>
[gele2:15844] mca:base:select:(&nbsp; plm) Querying component [slurm]<br>
[gele2:15844] mca:base:select:(&nbsp; plm) Query of component [slurm] set
priority to<br>
&nbsp;75<br>
[gele2:15844] mca:base:select:(&nbsp; plm) Selected component [alps]<br>
[gele2:15844] mca: base: close: component slurm closed<br>
[gele2:15844] mca: base: close: unloading component slurm<br>
[gele2:15844] mca: base: components_open: Looking for ras components<br>
[gele2:15844] mca: base: components_open: opening ras components<br>
[gele2:15844] mca: base: components_open: found loaded component cm<br>
[gele2:15844] mca: base: components_open: component cm has no register
function<br>
[gele2:15844] mca: base: components_open: component cm open function
successful<br>
[gele2:15844] mca: base: components_open: found loaded component alps<br>
[gele2:15844] mca: base: components_open: component alps has no
register functio<br>
n<br>
[gele2:15844] mca: base: components_open: component alps open function
successfu<br>
l<br>
[gele2:15844] mca: base: components_open: found loaded component slurm<br>
[gele2:15844] mca: base: components_open: component slurm has no
register functi<br>
on<br>
[gele2:15844] mca: base: components_open: component slurm open function
successf<br>
ul<br>
[gele2:15844] mca:base:select: Auto-selecting ras components<br>
[gele2:15844] mca:base:select:(&nbsp; ras) Querying component [cm]<br>
[gele2:15844] mca:base:select:(&nbsp; ras) Skipping component [cm]. Query
failed to r<br>
eturn a module<br>
[gele2:15844] mca:base:select:(&nbsp; ras) Querying component [alps]<br>
[gele2:15844] ras:alps: available for selection<br>
[gele2:15844] mca:base:select:(&nbsp; ras) Query of component [alps] set
priority to <br>
75<br>
[gele2:15844] mca:base:select:(&nbsp; ras) Querying component [slurm]<br>
[gele2:15844] mca:base:select:(&nbsp; ras) Query of component [slurm] set
priority to<br>
&nbsp;75<br>
[gele2:15844] mca:base:select:(&nbsp; ras) Selected component [alps]<br>
[gele2:15844] mca: base: close: unloading component cm<br>
[gele2:15844] mca: base: close: unloading component slurm<br>
[gele2:15844] ras:alps:allocate: Using ALPS configuration file:
"/etc/sysconfig/<br>
alps"<br>
[gele2:15844] ras:alps:allocate: Located ALPS scheduler file:
"/ufs/alps_shared/<br>
appinfo"<br>
[gele2:15844] ras:alps:orte_ras_alps_get_appinfo_attempts: 10<br>
[gele2:15844] ras:alps:read_appinfo: got NID 16<br>
[gele2:15844] ras:alps:read_appinfo: added NID 16 to list<br>
[gele2:15844] ras:alps:read_appinfo: got NID 16<br>
[gele2:15844] ras:alps:read_appinfo: got NID 16<br>
[gele2:15844] ras:alps:read_appinfo: got NID 16<br>
[gele2:15844] ras:alps:read_appinfo: got NID 16<br>
[gele2:15844] ras:alps:read_appinfo: got NID 16<br>
[gele2:15844] ras:alps:read_appinfo: got NID 16<br>
[gele2:15844] ras:alps:read_appinfo: got NID 16<br>
[gele2:15844] ras:alps:read_appinfo: got NID 16<br>
[gele2:15844] ras:alps:read_appinfo: got NID 16<br>
[gele2:15844] ras:alps:read_appinfo: got NID 16<br>
[gele2:15844] ras:alps:read_appinfo: got NID 16<br>
[gele2:15844] ras:alps:read_appinfo: got NID 20<br>
[gele2:15844] ras:alps:read_appinfo: added NID 20 to list<br>
[gele2:15844] ras:alps:read_appinfo: got NID 20<br>
[gele2:15844] ras:alps:read_appinfo: got NID 20<br>
[gele2:15844] ras:alps:read_appinfo: got NID 20<br>
[gele2:15844] ras:alps:read_appinfo: got NID 20<br>
[gele2:15844] ras:alps:read_appinfo: got NID 20<br>
[gele2:15844] ras:alps:read_appinfo: got NID 20<br>
[gele2:15844] ras:alps:read_appinfo: got NID 20<br>
[gele2:15844] ras:alps:read_appinfo: got NID 20<br>
[gele2:15844] ras:alps:read_appinfo: got NID 20<br>
[gele2:15844] ras:alps:read_appinfo: got NID 20<br>
[gele2:15844] ras:alps:read_appinfo: got NID 20<br>
[gele2:15844] ras:alps:allocate: success<br>
I am nid00020 process 2/2<br>
I am nid00016 process 1/2<br>
[gele2:15844] mca: base: close: unloading component alps<br>
[gele2:15844] mca: base: close: component alps closed<br>
[gele2:15844] mca: base: close: unloading component alps<br>
</small></small></font></big><br>
I think that in this case you would not break anything since it's
really a basic patch which enables you to directly do an mpirun,
without having to manually select any reservation id (assuming that the
user has the SLURM version with ALPS support which will be available
soon).<br>
<br>
Jerome<br>
<br>
On 07/09/2010 03:06 PM, Ralph Castain wrote:
<blockquote cite="mid:4C227A4D-DC14-4DA1-A135-F13A8B4F560F@open-mpi.org" type="cite">Afraid I'm now even more confused. You use SLURM to do the
allocation, and then use ALPS to launch the job?
  <div><br>
  </div>
  <div>I'm just trying to understand because I'm the person who
generally maintains this code area. We have two frameworks involved
here:</div>
  <div><br>
  </div>
  <div>1. RAS - determines what nodes were allocated to us. There are
both slurm and alps modules here.</div>
  <div><br>
  </div>
  <div>2. PLM - actually launches the job. There are both slurm and
alps modules here.</div>
  <div><br>
  </div>
  <div>Up until now, we have always seen people running with either
alps or slurm, but never both together, so the module selection of
these two frameworks is identical - if you select slurm for the RAS
module, you will definitely get slurm for the launcher. Ditto for
alps.&nbsp;Are you sure that mpirun is actually using the modules you think?
Have you run this with -mca ras_base_verbose 10 -mca plm_base_verbose
10 and seen what modules are being used?</div>
  <div><br>
  </div>
  <div>In any event, this seems like a very strange combination, but I
assume you have some reason for doing this?</div>
  <div><br>
  </div>
  <div>I'm always leery of fiddling with the SLURM modules as (a) there
aren't very many slurm users out there, (b) the primary users are the
DOE national labs themselves, using software provided by LLNL (who
controls slurm), and (c) there are major disconnects between the
various slurm releases, so we wind up breaking things for someone
rather easily.</div>
  <div><br>
  </div>
  <div>So the more I can understand what you are doing, the easier it
is to determine how to use your patch without breaking slurm support
for others.</div>
  <div><br>
  </div>
  <div>Thanks!</div>
  <div>Ralph</div>
  <div><br>
  </div>
  <div><br>
  <div>
  <div>On Jul 9, 2010, at 6:46 AM, Jerome Soumagne wrote:</div>
  <br class="Apple-interchange-newline">
  <blockquote type="cite">
    <div text="#000000" bgcolor="#ffffff">Well we actually use a
patched version of SLURM, 2.2.0-pre8. It is
planned to submit the modifications made internally at CSCS for the
next SLURM release in November. We implement ALPS support based on the
basic architecture of SLURM.<br>
SLURM is only used to do the ALPS ressource allocation. We then use
mpirun based on the portals and on the alps libaries.<br>
We don't use mca parameters to direct selection and the alps RAS is
automatically well selected.<br>
    <br>
On 07/09/2010 01:59 PM, Ralph Castain wrote:
    <blockquote cite="mid:4728DD74-13C8-437D-88B6-2D62CD5DF563@open-mpi.org" type="cite">Forgive my confusion, but could you please clarify
something? You are using ALPS as the resource manager doing the
allocation, and then using SLURM as the launcher (instead of ALPS)?
      <div><br>
      </div>
      <div>That's a combination we've never seen or heard about. I
suspect
our module selection logic would be confused by such a combination -
are you using mca params to direct selection?</div>
      <div><br>
      </div>
      <div><br>
      <div>
      <div>On Jul 9, 2010, at 4:19 AM, Jerome Soumagne wrote:</div>
      <br class="Apple-interchange-newline">
      <blockquote type="cite">
        <div text="#000000" bgcolor="#ffffff">Hi,<br>
        <br>
We've recently installed OpenMPI on one of our Cray XT5 machines, here
at CSCS. This machine uses SLURM for launching jobs.<br>
Doing an salloc defines this environment variable:<big><br>
        </big><small><font face="Courier New, Courier, monospace"><big>&nbsp;
&nbsp;
&nbsp; &nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; BASIL_RESERVATION_ID<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; The reservation ID on Cray systems running ALPS/BASIL
only.</big><br>
        </font></small><br>
Since the alps ras module tries to find a variable called
OMPI_ALPS_RESID which is set using a script, we thought that for SLURM
systems it would be a good idea to directly integrate this
BASIL_RESERVATION_ID variable in the code, rather than using a script.
The small patch is attached.<br>
        <br>
Regards,<br>
        <br>
Jerome<br>
        <pre class="moz-signature" cols="72">-- 
Jérôme Soumagne
Scientific Computing Research Group
CSCS, Swiss National Supercomputing Centre 
Galleria 2, Via Cantonale  | Tel: +41 (0)91 610 8258
CH-6928 Manno, Switzerland | Fax: +41 (0)91 610 8282

    </pre>
        </div>
        <span>&lt;patch_slurm_alps.txt&gt;</span>_______________________________________________<br>
devel mailing list<br>
        <a moz-do-not-send="true" href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>
        <a moz-do-not-send="true" class="moz-txt-link-freetext" href="http://www.open-mpi.org/mailman/listinfo.cgi/devel">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a></blockquote>
      </div>
      <br>
      </div>
      <pre wrap=""><fieldset class="mimeAttachmentHeader"></fieldset>
_______________________________________________
devel mailing list
<a moz-do-not-send="true" class="moz-txt-link-abbreviated" href="mailto:devel@open-mpi.org">devel@open-mpi.org</a>
<a moz-do-not-send="true" class="moz-txt-link-freetext" href="http://www.open-mpi.org/mailman/listinfo.cgi/devel">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a></pre>
    </blockquote>
    <br>
    <br>
    <pre class="moz-signature" cols="72">-- 
Jérôme Soumagne
Scientific Computing Research Group
CSCS, Swiss National Supercomputing Centre 
Galleria 2, Via Cantonale  | Tel: +41 (0)91 610 8258
CH-6928 Manno, Switzerland | Fax: +41 (0)91 610 8282

    </pre>
    </div>
_______________________________________________<br>
devel mailing list<br>
    <a moz-do-not-send="true" href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>
<a class="moz-txt-link-freetext" href="http://www.open-mpi.org/mailman/listinfo.cgi/devel">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a></blockquote>
  </div>
  <br>
  </div>
  <pre wrap=""><fieldset class="mimeAttachmentHeader"></fieldset>
_______________________________________________
devel mailing list
<a class="moz-txt-link-abbreviated" href="mailto:devel@open-mpi.org">devel@open-mpi.org</a>
<a class="moz-txt-link-freetext" href="http://www.open-mpi.org/mailman/listinfo.cgi/devel">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a></pre>
</blockquote>
<br>
</div>

_______________________________________________<br>devel mailing list<br><a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>http://www.open-mpi.org/mailman/listinfo.cgi/devel</blockquote></div><br></div></body></html>
